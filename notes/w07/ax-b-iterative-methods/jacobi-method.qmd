---
title: 'The Jacobi Method for Solving Linear Systems'
author: 'Nathan Lunceford'
format:
  html:
    self-contained: true
    page-layout: full
    toc: true
    toc-depth: 3
    toc-location: right
    number-sections: false
    html-math-method: katex
    embed-resources: true
    code-fold: true
    code-summary: 'Show Code'
    code-overflow: wrap
    code-copy: hover
    code-tools:
      source: false
      toggle: true
      caption: See code
engine: jupyter
preview:
  port: 3000
  browser: true
  watch-inputs: true
  navigate: true
---

## **Overview**

The **Jacobi Method** is an iterative algorithm used to solve linear systems of the form $A\mathbf{x} = \mathbf{b}$, where $A$ is a square matrix, $\mathbf{x}$ is the vector of unknowns, and $\mathbf{b}$ is the right-hand side vector. This method is particularly useful for large, sparse systems where direct methods like Gaussian elimination may be computationally expensive.

In this document, we will explore the mechanics of the Jacobi Method, its convergence criteria, and an example application.

## **The Algorithm**

The Jacobi Method iteratively refines an initial guess for $\mathbf{x}$ by solving for each component of $\mathbf{x}$ using the equations of the linear system. The algorithm assumes that the diagonal entries of $A$ are non-zero.

### **Step-by-Step Procedure**

1. Rewrite the system $A\mathbf{x} = \mathbf{b}$ in a form suitable for iteration.  
   For $A = [a_{ij}]$, $\mathbf{x} = [x_j]$, and $\mathbf{b} = [b_i]$, the system can be rewritten as:

   $$
   x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
   $$

   where $x_i^{(k+1)}$ is the updated value of $x_i$ in the $(k+1)$-th iteration.

2. Choose an initial guess $\mathbf{x}^{(0)}$.

3. Update each component of $\mathbf{x}$ iteratively using the formula above.

4. Stop when the solution converges, i.e., when $\|\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\|$ is smaller than a predefined tolerance.

## **Convergence Criteria**

The Jacobi Method converges if the matrix $A$ is **diagonally dominant** or **symmetric positive definite**.

### **Diagonal Dominance**

A matrix $A = [a_{ij}]$ is diagonally dominant if:

$$
|a_{ii}| > \sum_{j \neq i} |a_{ij}|
$$

for all $i$.

### **Symmetric Positive Definiteness**

A matrix $A$ is symmetric positive definite if it is symmetric ($A = A^\top$) and all its eigenvalues are positive.

## **Advantages and Disadvantages**

### **Advantages**

- Simple and easy to implement.
- Suitable for parallel computation since each $x_i^{(k+1)}$ depends only on values from the previous iteration.
- Works well for sparse systems.

### **Disadvantages**

- May converge slowly for some systems.
- Requires $A$ to satisfy certain properties (e.g., diagonal dominance) for guaranteed convergence.

## **A Practical Example**

Consider solving the system:

$$
\begin{bmatrix}
4 & 1 & 2 \\
3 & 5 & 1 \\
1 & 1 & 3
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
=
\begin{bmatrix}
4 \\
7 \\
3
\end{bmatrix}
$$

### **Step 1: Rewrite for Iteration**

From the formula:

$$
x_1^{(k+1)} = \frac{1}{4} \left( 4 - x_2^{(k)} - 2x_3^{(k)} \right)
$$

$$
x_2^{(k+1)} = \frac{1}{5} \left( 7 - 3x_1^{(k)} - x_3^{(k)} \right)
$$

$$
x_3^{(k+1)} = \frac{1}{3} \left( 3 - x_1^{(k)} - x_2^{(k)} \right)
$$

### **Step 2: Choose Initial Guess**

Let the initial guess be:

$$
\mathbf{x}^{(0)} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
$$

### **Step 3: Iterative Updates**

#### **Iteration 1:**

Using $\mathbf{x}^{(0)} = [0, 0, 0]$:

$$
x_1^{(1)} = \frac{1}{4} (4 - 0 - 0) = 1
$$

$$
x_2^{(1)} = \frac{1}{5} (7 - 0 - 0) = 1.4
$$

$$
x_3^{(1)} = \frac{1}{3} (3 - 0 - 0) = 1
$$

$$
\mathbf{x}^{(1)} = \begin{bmatrix} 1 \\ 1.4 \\ 1 \end{bmatrix}
$$

#### **Iteration 2:**

Using $\mathbf{x}^{(1)} = [1, 1.4, 1]$:

$$
x_1^{(2)} = \frac{1}{4} (4 - 1.4 - 2(1)) = 0.4
$$

$$
x_2^{(2)} = \frac{1}{5} (7 - 3(1) - 1) = 0.6
$$

$$
x_3^{(2)} = \frac{1}{3} (3 - 1 - 0.6) = 0.8
$$

$$
\mathbf{x}^{(2)} = \begin{bmatrix} 0.4 \\ 0.6 \\ 0.8 \end{bmatrix}
$$

### **Step 4: Convergence**

Continue iterations until $\|\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\| < \text{tolerance}$.

### **Final Solution**

The final solution is:

$$
\mathbf{x} = \begin{bmatrix} 0.5 \\ 1 \\ 0.75 \end{bmatrix}
$$

after sufficient iterations, to within the desired tolerance.

## **Code Implementation**

```python
import numpy as np

# Matrix A and vector b
A = np.array([[4, 1, 2],
              [3, 5, 1],
              [1, 1, 3]])
b = np.array([4, 7, 3])

# Initial guess
x = np.zeros_like(b, dtype=float)

# Iteration parameters
tolerance = 1e-5
max_iterations = 100

# Jacobi Method
for k in range(max_iterations):
    x_new = np.zeros_like(x)
    for i in range(A.shape[0]):
        s = sum(A[i, j] * x[j] for j in range(A.shape[1]) if j != i)
        x_new[i] = (b[i] - s) / A[i, i]
    if np.linalg.norm(x_new - x, ord=np.inf) < tolerance:
        break
    x = x_new

print(f"Solution: {x}")
```

## **Summary**

The Jacobi Method is a fundamental tool in numerical linear algebra. By understanding its workings and limitations, one can effectively apply it to solve linear systems, especially in scenarios where direct methods are infeasible. However, ensuring convergence through appropriate system properties is crucial for reliable results.
