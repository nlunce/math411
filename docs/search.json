[
  {
    "objectID": "reality-checks/rc01/rc01.html",
    "href": "reality-checks/rc01/rc01.html",
    "title": "REALITY CHECK 01",
    "section": "",
    "text": "PROBLEM 1\n\n\nWrite a python function for f(\\theta). The parameters L_1, L_2, L_3, \\gamma, x_1, x_2, y_2 are fixed constants, and the strut lengths p_1, p_2, p_3 will be known for a given pose. To test your code, set the parameters L_1 = 2, L_2 = L_3 = \\sqrt{2}, \\gamma = \\pi/2, and p_1 = p_2 = p_3 = \\sqrt{5}. Then, substituting \\theta = -\\pi/4 or \\theta = \\pi/4, should make f(\\theta) = 0.\nI implemented the Python function f(\\theta) by putting all fixed constants into a Constants object. I initialized the constants with the given values in order to verify that \\theta = -\\pi/4 and \\theta = \\pi/4 were roots.\n\n\nCreate function for f(\\theta)\n\n\nShow Code\n# Define the function f(θ) that calculates based on given constants and angle θ\ndef f(theta, constants):\n    \"\"\"\n    Calculates a value based on the given angle theta and constants object.\n\n    Parameters:\n    theta (float): The angle in radians.\n    constants (Constants): An object containing the necessary constants.\n\n    Returns:\n    float: The calculated result.\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n    p1, p2, p3 = constants.p1, constants.p2, constants.p3\n\n    a2 = l3 * np.cos(theta) - x1\n    b2 = l3 * np.sin(theta)\n    a3 = l2 * np.cos(theta + gamma) - x2\n    b3 = l2 * np.sin(theta + gamma) - y2\n    d = 2 * (a2 * b3 - b2 * a3)\n\n    n1 = b3 * (p2**2 - p1**2 - a2**2 - b2**2) - b2 * (p3**2 - p1**2 - a3**2 - b3**2)\n    n2 = -a3 * (p2**2 - p1**2 - a2**2 - b2**2) + a2 * (p3**2 - p1**2 - a3**2 - b3**2)\n\n    return n1**2 + n2**2 - p1**2 * d**2\n\n\n\n\nTest function f(\\theta)\n\n\nShow Code\n# Define constants and evaluate f(θ) at a specific angle θ = π/4 for testing purposes\nconstants = Constants(\n    l1=2,\n    l2=np.sqrt(2),\n    l3=np.sqrt(2),\n    gamma=np.pi / 2,\n    x1=4,\n    x2=0,\n    y2=4,\n    p1=np.sqrt(5),\n    p2=np.sqrt(5),\n    p3=np.sqrt(5)\n)\n\ntheta = np.pi / 4\n# Evaluate\nresult = f(theta, constants)\nprint(f'f(θ=π/4) = {result}')\n\n\nf(θ=π/4) = -4.547473508864641e-13\n\n\n\n\n\nPROBLEM 2\n\n\nPlot f(\\theta) on [-\\pi, \\pi]\nI plotted the function f(\\theta) over the interval [-π, π] by generating a range of \\theta values and computing f(\\theta) for each. The graph clearly shows the behavior of f(\\theta) and highlights that the the roots identified in Problem 1 are in fact roots.\n\n\nCreate Plot\n\n\nShow Code\n# Generate a range of theta values and compute f(θ) for each value to visualize the function\ntheta_values = np.linspace(-np.pi, np.pi, 400)\nresults = [f(theta, constants) for theta in theta_values]\n\n# Plot f(θ) over the range of theta values\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, results, label=r'$f(\\theta)$', linewidth=2)\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.axvline(-np.pi/4, color='red', linestyle=':', linewidth=2, label=r'$\\theta = -\\pi/4$')\nplt.axvline(np.pi/4, color='red', linestyle=':', linewidth=2, label=r'$\\theta = \\pi/4$')\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 3\n\n\nReproduce Figure 1.15. Plot a red triangle with vertices (u_1, v_1), (u_2, v_2), (u_3, v_3) and place small blue circles at the strut anchor points (0,0), (x_1, 0), (x_2, y_2):\nI utilized several helper functions to efficiently calculate and visualize the Stewart platform’s configuration. The get_x_y() function computes the x and y coordinates based on the given angle \\theta and the fixed constants, determining the position of one vertex of the triangle. The get_points() function then takes these coordinates, along with \\theta and the constants, to calculate the two other vertices of the red triangle. The get_anchor_points() function gets the fixed anchor points (0,0), (x_1, 0), and (x_2, y_2). The plot_triangle() function takes the calculated triangle vertices and anchor points to plot the red triangle and connect the anchor points with blue lines, while also marking the anchor points with blue circles.\n\n\nCreate Helper Functions\n\n\nShow Code\n# Define helper functions for calculating x, y coordinates and plotting the Stewart platform triangle\ndef get_x_y(theta, constants):\n    \"\"\"\n    Returns the coordinates x and y for the given angle theta and constants object.\n\n    Parameters:\n    theta (float): The angle in radians.\n    constants (Constants): An object containing the necessary constants.\n\n    Returns:\n    tuple: The coordinates (x, y).\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n    p1, p2, p3 = constants.p1, constants.p2, constants.p3\n\n    a2 = l3 * np.cos(theta) - x1\n    b2 = l3 * np.sin(theta)\n    a3 = l2 * np.cos(theta + gamma) - x2\n    b3 = l2 * np.sin(theta + gamma) - y2\n\n    d = 2 * (a2 * b3 - b2 * a3)\n    n1 = b3 * (p2**2 - p1**2 - a2**2 - b2**2) - b2 * (p3**2 - p1**2 - a3**2 - b3**2)\n    n2 = -a3 * (p2**2 - p1**2 - a2**2 - b2**2) + a2 * (p3**2 - p1**2 - a3**2 - b3**2)\n\n    x = n1 / d\n    y = n2 / d\n\n    return x, y\n\n\ndef get_points(x, y, theta, constants):\n    \"\"\"\n    Calculate the three points (vertices) of the triangle in the Stewart platform based on x, y, and θ.\n\n    Parameters:\n    x (float): The x-coordinate.\n    y (float): The y-coordinate.\n    theta (float): The angle in radians.\n    constants (Constants): Object containing the necessary constants.\n\n    Returns:\n    list: A list containing the three vertices (l1_point, l2_point, l3_point) of the triangle.\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n\n    # First vertex (base point)\n    l1_point = (x, y)\n\n    # Second vertex of the triangle\n    l2_x = x + (l3 * np.cos(theta))\n    l2_y = y + (l3 * np.sin(theta))\n    l2_point = (np.round(l2_x, 3), np.round(l2_y))  # Rounded to 3 decimal places for clarity\n\n    # Third vertex of the triangle\n    l3_x = x + (l2 * np.cos(theta + gamma))\n    l3_y = y + (l2 * np.sin(theta + gamma))\n    l3_point = (np.round(l3_x), np.round(l3_y))  # Rounded to 3 decimal places for clarity\n\n    return [l1_point, l2_point, l3_point]\n\ndef get_anchor_points(constants):\n    \"\"\"\n    Get the anchor points for the Stewart platform based on the constants.\n\n    Parameters:\n    constants (Constants): Object containing the necessary constants.\n\n    Returns:\n    list: A list of tuples representing the anchor points.\n    \"\"\"\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n\n    return [(0, 0), (x1, 0), (x2, y2)]\n\ndef plot_triangle(ax, points, anchor_points, x_limits=None, y_limits=None, x_step=None, y_step=None):\n    \"\"\"\n    Plots a triangle given the points and anchor points on the provided axis.\n\n    Parameters:\n    ax: The axis on which to plot the triangle.\n    points: The points of the triangle (list of 3 points).\n    anchor_points: The anchor points (list of 2 or more points).\n    x_limits (tuple, optional): Tuple specifying the x-axis limits (x_min, x_max).\n    y_limits (tuple, optional): Tuple specifying the y-axis limits (y_min, y_max).\n    x_step (float, optional): Step size for the x-axis grid.\n    y_step (float, optional): Step size for the y-axis grid.\n\n    Returns:\n    None\n    \"\"\"\n    points = np.array(points)\n    anchor_points = np.array(anchor_points)\n\n    # Extract x and y coordinates for the triangle points\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by appending the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Plot the triangle with red lines\n    ax.plot(x_closed, y_closed, 'r-', linewidth=3.5)\n\n    # Plot blue dots at the triangle vertices\n    ax.plot(x_coords, y_coords, 'bo', markersize=8)\n\n    # Plot lines from anchor points to triangle points\n    for i, anchor in enumerate(anchor_points):\n        if i &lt; len(points):  # Ensure we stay within bounds\n            ax.plot([anchor[0], points[i, 0]], [anchor[1], points[i, 1]], 'b-', linewidth=1.5)\n\n    # Plot blue dots at the anchor points\n    ax.plot(anchor_points[:, 0], anchor_points[:, 1], 'bo', markersize=8)\n\n    # Set axis labels\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\n    # Set x-axis limits if provided\n    if x_limits is not None:\n        ax.set_xlim(x_limits)\n    # Set y-axis limits if provided\n    if y_limits is not None:\n        ax.set_ylim(y_limits)\n    # Set grid step increments if limits are provided\n    if x_step is not None and x_limits is not None:\n        ax.set_xticks(np.arange(x_limits[0], x_limits[1] + x_step, x_step))  # Adjust x-axis ticks\n    if y_step is not None and y_limits is not None:\n        ax.set_yticks(np.arange(y_limits[0], y_limits[1] + y_step, y_step))  # Adjust y-axis ticks\n\n    # Add grid for better visualization\n    ax.grid(True)\n\n\n\n\nCreate Plot\n\n\nShow code\n# Create a plot to visualize the Stewart platform configurations for two different angles\ntheta = np.pi / 4\ntheta_negative = -np.pi / 4\n\n# Calculate the coordinates and points for the triangles\nx, y = get_x_y(theta_negative, constants)\npoints1 = get_points(x, y, theta_negative, constants)\nanchor_points = get_anchor_points(constants)\n\nx, y = get_x_y(theta, constants)\npoints2 = get_points(x, y, theta, constants)\n\n# Create side-by-side subplots to visualize the two triangles\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the triangles on each subplot\nplot_triangle(axes[0], points1, anchor_points, x_limits=(-0.25, 4.25), y_limits=(-0.25, 4.25))\nplot_triangle(axes[1], points2, anchor_points, x_limits=(-0.25, 4.25), y_limits=(-0.25, 4.25))\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 4\n\n\nSolve the forward kinematics problem for the planar Stewart platform specified by x_1 = 5, (x_2, y_2) = (0,6), L_1 = L_3 = 3, L_2 = 3\\sqrt{2}, \\gamma = \\pi / 4, p_1 = p_2 = 5, p_3 = 3. Begin by plotting f(\\theta). Use an equation solver of your choice to find all four poses (roots of f(\\theta)), and plot them. Check your answers by verifying that p_1, p_2, p_3 are the lengths of the struts in your plot.\nI organized all the fixed parameters into a Constants object and plotted the function f(\\theta) over the interval [-π, π] to visualize its behavior. Using the fsolve function with strategically chosen initial guesses, I identified all four roots of f(\\theta), each root representing a unique pose of the Stewart platform. For each detected root, I plotted the corresponding triangle configuration and verified that the strut lengths p_1, p_2, p_3 matched the expected values.\n\n\n4A)\n\n\nShow Code\n# Create new constants object\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=5,\n    p3=3\n)\n\n# Generate an array of θ values between -π and π\ntheta_values = np.linspace(-np.pi, np.pi, 400)\n\n# Plot the function f(θ) over the range of θ values using the given constants\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, f(theta_values, constants), label=r'$f(\\theta)$')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4B)\n\n\nShow Code\n# Function to find roots of f(θ) using fsolve\ndef find_roots(constants, initial_guesses):\n    \"\"\"\n    Finds roots of f(θ) using different initial guesses and the fsolve method.\n\n    Parameters:\n    constants (Constants): Object containing the necessary constants.\n    initial_guesses (list or array): List of initial guesses for fsolve to start from.\n\n    Returns:\n    list: A list of unique roots.\n    \"\"\"\n\n    # Create an empty list to store the roots found\n    roots = []\n    # Iterate over each initial guess and find the root using fsolve\n    for guess in initial_guesses:\n        root = fsolve(f, guess, args=(constants), xtol=1e-12)[0] # Find root for each guess\n        roots.append(root) # Append the found root to the list\n\n    # Return only unique roots to avoid duplicates\n    unique_roots = np.unique(roots)\n    return unique_roots\n\n# Define initial guesses for fsolve to start the root-finding process\ninitial_guesses = [- 1, np.pi / 3, .5, 2]\n\n# Find and print the roots using the initial guesses\nroots = find_roots(constants, initial_guesses)\nprint(f\"The roots of f(θ) in the interval are : {roots}\")\n\n# Function to calculate the length of the struts\ndef calculate_strut_lengths(points, anchor_points):\n    lengths = []\n    # Loop through the 3 points and calculate the Euclidean distance to each corresponding anchor point\n    for i in range(3):\n        length = np.sqrt((points[i][0] - anchor_points[i][0])**2 + (points[i][1] - anchor_points[i][1])**2)\n        lengths.append(length) # Append each calculated length to the list\n    return lengths\n\n\nThe roots of f(θ) in the interval are : [-0.7208492  -0.33100518  1.14368552  2.11590901]\n\n\n\n\nShow Code\n# Create a 2x2 grid of subplots to visualize the four roots and their corresponding triangles\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten() # Flatten the 2D array of subplots into a 1D array for easier access\n\n# Get the anchor points for the Stewart platform\nanchor_points = get_anchor_points(constants)\n\n# Loop through up to four roots and plot the corresponding triangles\nfor i, theta in enumerate(roots[:4]):\n    x, y = get_x_y(theta, constants)\n    points = get_points(x, y, theta, constants)\n\n    # Plot the triangle in the corresponding subplot with custom limits\n    plot_triangle(axes[i], points, anchor_points, x_limits=(-2.5, 7.5), y_limits=(-2, 7), x_step=2.5, y_step=2)\n    axes[i].set_title(rf\"$\\theta$ = {theta}\")\n\n    # Calculate and verify strut lengths\n    lengths = calculate_strut_lengths(points, anchor_points)\n    print(f\"For root {np.round(theta, 3)}, strut lengths are: {np.round(lengths)}\")\n    print(f\"Expected: p1={constants.p1}, p2={constants.p2}, p3={constants.p3}\\n\")\n\n# Turn off any unused subplots if fewer than four roots\nfor j in range(len(roots), 4):\n    axes[j].axis('off')\n\n# Adjust layout\nplt.tight_layout()\n\nplt.show()\n\n\nFor root -0.721, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root -0.331, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root 1.144, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root 2.116, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 5\n\n\nChange strut length to p_2 = 7 and re-solve the problem. For these parameters, there are six poses.\nI updated the strut length p_2 to 7 and re-solved the forward kinematics for the Stewart platform. To do that I modified the Constants object with the new p_2 value and plotted the updated function f(\\theta) over the interval [-π, π] to see its behavior. I made a new set of initial guesses for the find_roots() function and successfully found all six roots corresponding to six possible poses. For each root, I plotted the corresponding triangle configuration and verified that the strut lengths p_1, p_2, p_3 matched the expected values.\n\n\n5A)\n\n\nShow Code\n# Update the constants to reflect the new strut length p2 = 7\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=7,\n    p3=3\n)\n\n# Generate the θ values again to visualize the updated f(θ)\ntheta_values = np.linspace(-np.pi, np.pi, 400)\n\n# Plot f(θ) for the new strut length\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, f(theta_values, constants), label=r'$f(\\theta)$')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5B)\n\n\nShow Code\n# Provide new initial guesses to find six distinct roots for this configuration\ninitial_guesses = [-.7, -.4, .01, .4, .9, 2.5 ]  # Customize this list\n\n# Find and print the roots using the initial guesses\nroots = find_roots(constants, initial_guesses)\nprint(f\"The roots of f(θ) in the interval are : {roots}\")\n\n# Set up the 2x3 grid for plotting the six poses\nfig, axes = plt.subplots(2, 3, figsize=(9, 6))  # Create a 2x3 grid\naxes = axes.flatten()  # Flatten the 2D array of axes for easier access\n\n# Get the anchor points\nanchor_points = get_anchor_points(constants)\n\n# Loop through the six roots and plot each pose\nfor i, theta in enumerate(roots[:6]):\n    x, y = get_x_y(theta, constants)\n    points = get_points(x, y, theta, constants)\n    # Plot the triangle in the corresponding subplot with custom limits\n    plot_triangle(axes[i], points, anchor_points, x_limits=(-5.5, 5.5), y_limits=(-.5, 10), )\n    axes[i].set_title(rf\"$\\theta$ = {theta}\")\n\n    # Calculate and verify strut lengths\n    lengths = calculate_strut_lengths(points, anchor_points)\n    print(f\"For root {np.round(theta, 3)}, strut lengths are: {np.round(lengths)}\")\n    print(f\"Expected: p1={constants.p1}, p2={constants.p2}, p3={constants.p3}\\n\")\n\n# Turn off any unused subplots (though in this case, we should have exactly 6)\nfor j in range(len(roots), 6):\n    axes[j].axis('off')\n\n# Adjust layout\nplt.tight_layout()\n\n\nplt.show()\n\n\nThe roots of f(θ) in the interval are : [-0.67315749 -0.35474027  0.03776676  0.45887818  0.9776729   2.5138528 ]\nFor root -0.673, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root -0.355, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.038, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.459, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.978, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 2.514, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 6\n\n\nFind a strut length p_2, with the rest of the parameters as in Step 4, for which there are only two poses.\nTo identify a strut length for p_2 that results in exactly two poses for the Stewart platform I systematically adjusted p_2 and utilizing the fsolve function to find the corresponding roots of the function f(\\theta). This method enabled me to determine a specific p_2 value that achieves the desired two-pose configuration.\n\n\nShow Code\n# Set a threshold for considering a valid root (how close to zero we want f(theta) to be)\nROOT_THRESHOLD = 1e-6\n\n# Function to find roots for a given p2, and check if they are valid\ndef find_roots_for_p2(p2_value, constants, initial_guesses, ax=None):\n    \"\"\"\n    Adjusts p2 in the constants object, finds the roots, and returns the number of unique roots.\n    Also plots f(theta) for the current p2 value on the provided axis.\n    \"\"\"\n    # Update p2 in constants\n    constants.p2 = p2_value\n\n    # Generate theta values and compute f(theta)\n    theta_values = np.linspace(-np.pi, np.pi, 400)\n    f_values = [f(theta, constants) for theta in theta_values]\n\n    # Plot f(theta) for the current p2 value on the provided axis\n    ax.plot(theta_values, f_values,)\n    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    ax.set_xlabel(r'$\\theta$', fontsize=14)\n    ax.set_ylabel(r'$f(\\theta)$', fontsize=14)\n    ax.set_title(fr'$p_2 = {p2_value:.3f}$')\n    ax.legend(fontsize=10)\n    ax.grid(True)\n\n    # Find the roots for the given p2 value\n    roots = []\n    for guess in initial_guesses:\n        root = fsolve(f, guess, args=(constants))[0]\n\n        # Check if the found root is valid (i.e., f(root) is close to zero)\n        if abs(f(root, constants)) &lt; ROOT_THRESHOLD:\n            roots.append(root)\n\n    # Convert to numpy array and round the roots to avoid precision issues\n    roots = np.round(np.array(roots), decimals=6)\n    unique_roots = np.unique(roots)\n\n    # Print the number of valid roots and the roots themselves\n    print(f\"p2 = {p2_value:.3f}: Found {len(unique_roots)} valid roots: {unique_roots}\")\n\n    return unique_roots\n\n# Function to iterate over possible p2 values and append plots in a grid (wrap after 3)\ndef find_p2_with_two_roots(constants, initial_guesses, p2_start=-1, total_plots=6):\n    \"\"\"\n    Iterates over possible p2 values starting at p2_start, plots f(theta), and prints the number of roots.\n    The plots wrap after 3 per row.\n\n    Parameters:\n    - constants: The Constants object.\n    - initial_guesses: List of initial guesses for root finding.\n    - p2_start: Starting value of p2.\n    - total_plots: Number of plots to show before stopping.\n    \"\"\"\n    p2 = p2_start\n    plot_count = 0\n    max_plots_per_row = 3  # Wrap after 3 plots per row\n\n    # Calculate the number of rows needed (wrap after 3)\n    num_rows = (total_plots + max_plots_per_row - 1) // max_plots_per_row\n\n    # Create a figure with a 3xN grid\n    fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(10, num_rows * 3))\n    axes = axes.flatten()  # Flatten the 2D array of axes for easier access\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)  # Adjust the space between subplots\n\n    # Iterate to plot p2 and find roots\n    while plot_count &lt; total_plots:\n        # Plot for the current p2 value and check the roots\n        unique_roots = find_roots_for_p2(p2, constants, initial_guesses, ax=axes[plot_count])\n\n        if len(unique_roots) == 2:  # Check if there are exactly 2 unique roots\n            print(f\"Found p2={p2} with two distinct roots: {unique_roots}\")\n\n        # Increment p2 and plot the next iteration\n        p2 += 1\n        plot_count += 1\n\n    # Show the final figure with all appended plots\n    plt.tight_layout()\n\n\n    plt.show()\n\n# Example constants (with p2 placeholder)\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=None,  # To be found\n    p3=3\n)\n\n# Initial guesses for root finding\ninitial_guesses = [-np.pi/2, 0, np.pi/2]\n\n# Start p2 at -1 and increment by 1 each time, looking for exactly 2 roots\nfind_p2_with_two_roots(constants, initial_guesses, p2_start=-1, total_plots=6)\n\n\np2 = -1.000: Found 0 valid roots: []\np2 = 0.000: Found 0 valid roots: []\np2 = 1.000: Found 0 valid roots: []\np2 = 2.000: Found 0 valid roots: []\np2 = 3.000: Found 0 valid roots: []\np2 = 4.000: Found 2 valid roots: [1.331642 1.777514]\nFound p2=4 with two distinct roots: [1.331642 1.777514]\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 7\n\n\nCalculate the intervals in p_2, with the rest of the parameters as in Step 4, for which there are 0, 2, 4, and 6 poses, respectively.\nIn transitioning from Problem 6 to Problem 7, I found that using fsolve with predefined initial guesses was too inaccurate for reliably identifying roots. This method often missed valid roots or produced duplicates due to its sensitivity to starting points. To improve accuracy, I switched to detecting sign changes in the function f(\\theta) and used the brentq algorithm, which efficiently locates roots where the function changes from positive to negative or vice versa. This approach greatly improved the precision of root detection.\n\n\nShow Code\ndef count_roots(constants, theta_min=-np.pi, theta_max=np.pi, num_points=1000):\n    \"\"\"\n    Counts roots of f(theta) = 0 within [theta_min, theta_max].\n\n    Parameters:\n    constants (Constants): Stewart platform constants.\n    theta_min (float): Lower bound of theta.\n    theta_max (float): Upper bound of theta.\n    num_points (int): Sampling points.\n\n    Returns:\n    int: Number of unique roots.\n    list: Root values.\n    \"\"\"\n    theta_vals = np.linspace(theta_min, theta_max, num_points)\n\n    # Evaluate f(theta) over the range\n    f_vals = np.array([f(theta, constants) for theta in theta_vals])\n\n    roots = []\n\n    # Detect sign changes indicating roots\n    for i in range(len(theta_vals)-1):\n        if np.sign(f_vals[i]) != np.sign(f_vals[i+1]):\n            try:\n                root = brentq(f, theta_vals[i], theta_vals[i+1], args=(constants,))\n                if theta_min &lt;= root &lt;= theta_max:\n                    roots.append(root)\n            except ValueError:\n                pass  # No root in this interval\n\n    # Eliminate duplicate roots\n    unique_roots = []\n    for r in roots:\n        if not any(np.isclose(r, ur, atol=1e-5) for ur in unique_roots):\n            unique_roots.append(r)\n\n    return len(unique_roots), unique_roots\n\ndef find_p2_intervals(constants, p2_min, p2_max, p2_step):\n    \"\"\"\n    Finds p2 intervals with specific numbers of roots.\n\n    Parameters:\n    constants (Constants): Stewart platform constants.\n    p2_min (float): Starting p2 value.\n    p2_max (float): Ending p2 value.\n    p2_step (float): Increment step for p2.\n\n    Returns:\n    dict: Pose counts as keys and p2 lists as values.\n    \"\"\"\n    p2_values = np.arange(p2_min, p2_max + p2_step, p2_step)\n    root_counts = {0: [], 2: [], 4: [], 6: []}\n\n    for p2 in p2_values:\n        constants.p2 = p2\n        num_roots, _ = count_roots(constants)\n        if num_roots in root_counts:\n            root_counts[num_roots].append(p2)\n\n    return root_counts\n\n#### 4. Implement Problem 7\n\n# Initialize constants\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=5,  # Initial p2; will be varied\n    p3=3\n)\n\n# Set p2 range\np2_min = 0.0\np2_max = 12.98  # Extended to capture p2 &gt;= 9.27\np2_step = 0.01\n\n# Get root counts\nroot_counts = find_p2_intervals(constants, p2_min, p2_max, p2_step)\n\n# Plotting\nplt.figure(figsize=(12, 6))\ncolors = {0: 'blue', 2: 'green', 4: 'orange', 6: 'red'}\n\nfor num_roots, p2_list in root_counts.items():\n    plt.scatter(p2_list, [num_roots]*len(p2_list), label=f'{num_roots} poses', s=10, color=colors.get(num_roots, 'grey'))\n\nplt.xlabel('$p_2$', fontsize=14)\nplt.ylabel('Number of Poses (Roots)', fontsize=14)\nplt.title('Number of Poses vs Length of Strut $p_2$', fontsize=16)\nplt.legend()\nplt.grid(True)\n\nplt.show()\n\n# Identify intervals\nintervals_dict = {0: [], 2: [], 4: [], 6: []}\ntol = 1e-6  # Tolerance for precision\n\nfor num_roots, p2_list in root_counts.items():\n    if p2_list:\n        p2_sorted = np.sort(p2_list)\n        diffs = np.diff(p2_sorted)\n        split_indices = np.where(diffs &gt; (p2_step + tol))[0] + 1\n        intervals = np.split(p2_sorted, split_indices)\n\n        for interval in intervals:\n            p2_start, p2_end = interval[0], interval[-1]\n            intervals_dict[num_roots].append((p2_start, p2_end))\n\n# Print first and last intervals for each pose count\nfor num_roots, intervals in intervals_dict.items():\n    if intervals:\n        print(f\"\\nIntervals with {num_roots} poses:\")\n        if len(intervals) == 1:\n            p2_start, p2_end = intervals[0]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n        else:\n            # First interval\n            p2_start, p2_end = intervals[0]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n\n            # Last interval\n            p2_start, p2_end = intervals[-1]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n\n\n\n\n\n\n\n\n\n\nIntervals with 0 poses:\n  p2 from 0.00 to 3.71\n  p2 from 9.27 to infinity\n\nIntervals with 2 poses:\n  p2 from 3.72 to 4.86\n  p2 from 7.85 to 9.26\n\nIntervals with 4 poses:\n  p2 from 4.87 to 6.96\n  p2 from 7.03 to 7.84\n\nIntervals with 6 poses:\n  p2 from 6.97 to 7.02"
  },
  {
    "objectID": "homework/w03/c0-p2.html",
    "href": "homework/w03/c0-p2.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array–choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n\n\nCreate function and figure\n# Define the function\ndef plot_triangle(points, save_path='triangle_plot.png'):\n    \"\"\"\n    Takes an input of three points (a list of 3 tuples or a 3x2 numpy array)\n    and plots a triangle with small open circles at each of the points.\n    The triangle is rendered with lines connecting each point.\n\n    Parameters:\n    points (list of tuples or numpy array): Points representing the vertices of the triangle.\n    save_path (str): File path to save the plotted figure.\n    \"\"\"\n\n    # Ensure points is a numpy array\n    points = np.array(points)\n\n    # Check if the input is in the correct shape (3x2)\n    if points.shape != (3, 2):\n        raise ValueError(\"Input should be a list of 3 points, each as a pair of (x, y) coordinates.\")\n\n    # Extract the x and y coordinates\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by repeating the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Create the plot\n    plt.figure(figsize=(6, 6))\n\n    # Plot the triangle with open circles at each vertex\n    plt.plot(x_closed, y_closed, 'b-', marker='o', markerfacecolor='none',\n             markeredgecolor='r', markersize=10, label='Triangle')\n\n    # Set labels and title\n    plt.title(\"Triangle with Given Vertices\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    # Set axis limits for better visualization\n    plt.xlim(min(x_closed) - 1, max(x_closed) + 1)\n    plt.ylim(min(y_closed) - 1, max(y_closed) + 1)\n\n    # Add grid for better visualization\n    plt.grid(True)\n\n\n    # Save the figure\n    plt.savefig(save_path, dpi=300)\n\n    # Show the plot\n   #  plt.show()\n\n# Test the function with the points (1, 2), (2, 1), and (2, 3)\ntest_points = [(1, 2), (2, 1), (2, 3)]\nplot_triangle(test_points, save_path='triangle_plot.png')\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Definition:\n\nplot_triangle: This function takes in three points and an optional save_path parameter to specify where to save the plot.\nParameters:\n\npoints: A list of three tuples representing the vertices of the triangle or a 3x2 numpy array.\nsave_path: The file path where the plot image will be saved (default is 'triangle_plot.png').\n\n\nInput Validation:\n\nThe function first converts the input points to a numpy array and checks if it has the correct shape (3, 2). If not, it raises a ValueError.\n\nPlotting:\n\nClosing the Triangle: To draw a complete triangle, the first point is appended to the end of the x_coords and y_coords arrays.\nPlotting Lines and Markers:\n\nThe triangle is plotted with blue lines ('b-') connecting the points.\nSmall open red circles (marker='o', markerfacecolor='none', markeredgecolor='r') are placed at each vertex.\n\nLabels and Grid: The plot includes titles, axis labels, and a grid for better visualization.\n\nSaving and Displaying the Plot:\n\nThe plot is saved as a .png file with a resolution of 300 DPI.\nThe plot window is then displayed using plt.show().\n\n\n\n\n\nAfter running the function with the test points (1, 2), (2, 1), and (2, 3), the resulting triangle will be saved as triangle_plot.png and displayed as shown below:\n\n\n\nTriangle Plot"
  },
  {
    "objectID": "homework/w03/c0-p2.html#question",
    "href": "homework/w03/c0-p2.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array–choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n\n\nCreate function and figure\n# Define the function\ndef plot_triangle(points, save_path='triangle_plot.png'):\n    \"\"\"\n    Takes an input of three points (a list of 3 tuples or a 3x2 numpy array)\n    and plots a triangle with small open circles at each of the points.\n    The triangle is rendered with lines connecting each point.\n\n    Parameters:\n    points (list of tuples or numpy array): Points representing the vertices of the triangle.\n    save_path (str): File path to save the plotted figure.\n    \"\"\"\n\n    # Ensure points is a numpy array\n    points = np.array(points)\n\n    # Check if the input is in the correct shape (3x2)\n    if points.shape != (3, 2):\n        raise ValueError(\"Input should be a list of 3 points, each as a pair of (x, y) coordinates.\")\n\n    # Extract the x and y coordinates\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by repeating the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Create the plot\n    plt.figure(figsize=(6, 6))\n\n    # Plot the triangle with open circles at each vertex\n    plt.plot(x_closed, y_closed, 'b-', marker='o', markerfacecolor='none',\n             markeredgecolor='r', markersize=10, label='Triangle')\n\n    # Set labels and title\n    plt.title(\"Triangle with Given Vertices\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    # Set axis limits for better visualization\n    plt.xlim(min(x_closed) - 1, max(x_closed) + 1)\n    plt.ylim(min(y_closed) - 1, max(y_closed) + 1)\n\n    # Add grid for better visualization\n    plt.grid(True)\n\n\n    # Save the figure\n    plt.savefig(save_path, dpi=300)\n\n    # Show the plot\n   #  plt.show()\n\n# Test the function with the points (1, 2), (2, 1), and (2, 3)\ntest_points = [(1, 2), (2, 1), (2, 3)]\nplot_triangle(test_points, save_path='triangle_plot.png')\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Definition:\n\nplot_triangle: This function takes in three points and an optional save_path parameter to specify where to save the plot.\nParameters:\n\npoints: A list of three tuples representing the vertices of the triangle or a 3x2 numpy array.\nsave_path: The file path where the plot image will be saved (default is 'triangle_plot.png').\n\n\nInput Validation:\n\nThe function first converts the input points to a numpy array and checks if it has the correct shape (3, 2). If not, it raises a ValueError.\n\nPlotting:\n\nClosing the Triangle: To draw a complete triangle, the first point is appended to the end of the x_coords and y_coords arrays.\nPlotting Lines and Markers:\n\nThe triangle is plotted with blue lines ('b-') connecting the points.\nSmall open red circles (marker='o', markerfacecolor='none', markeredgecolor='r') are placed at each vertex.\n\nLabels and Grid: The plot includes titles, axis labels, and a grid for better visualization.\n\nSaving and Displaying the Plot:\n\nThe plot is saved as a .png file with a resolution of 300 DPI.\nThe plot window is then displayed using plt.show().\n\n\n\n\n\nAfter running the function with the test points (1, 2), (2, 1), and (2, 3), the resulting triangle will be saved as triangle_plot.png and displayed as shown below:\n\n\n\nTriangle Plot"
  },
  {
    "objectID": "homework/w02/exercise3-1-2a.html",
    "href": "homework/w02/exercise3-1-2a.html",
    "title": "Exercise 3.1.2a (C3-P2)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\n\n\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n1\n\n\n\n\n2\n3\n1\n\n\n\n3\n0\n-3\n-\\frac{4}{3}\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = 1, \\quad f[x_2] = y_2 = 3, \\quad f[x_3] = y_3 = 0 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{3 - 1}{2 - 0} = \\frac{2}{2} = 1 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{0 - 3}{3 - 2} = \\frac{-3}{1} = -3 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-3 - 1}{3 - 0} = \\frac{-4}{3} \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = 1 + 1(x - 0) + \\left(\\frac{-4}{3}\\right)(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = 1 + x - \\frac{4}{3}(x(x - 2)) = 1 + x - \\frac{4}{3}(x^2 - 2x)\n\n\nP(x) = 1 + x - \\frac{4}{3}x^2 + \\frac{8}{3}x\n\nCombine like terms:\n\nP(x) = 1 + \\left(x + \\frac{8}{3}x\\right) - \\frac{4}{3}x^2 = 1 + \\frac{11x}{3} - \\frac{4x^2}{3}\n\nSo the final polynomial is:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the Newton interpolating polynomial for the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-2a.html#question",
    "href": "homework/w02/exercise3-1-2a.html#question",
    "title": "Exercise 3.1.2a (C3-P2)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\n\n\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n1\n\n\n\n\n2\n3\n1\n\n\n\n3\n0\n-3\n-\\frac{4}{3}\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = 1, \\quad f[x_2] = y_2 = 3, \\quad f[x_3] = y_3 = 0 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{3 - 1}{2 - 0} = \\frac{2}{2} = 1 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{0 - 3}{3 - 2} = \\frac{-3}{1} = -3 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-3 - 1}{3 - 0} = \\frac{-4}{3} \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = 1 + 1(x - 0) + \\left(\\frac{-4}{3}\\right)(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = 1 + x - \\frac{4}{3}(x(x - 2)) = 1 + x - \\frac{4}{3}(x^2 - 2x)\n\n\nP(x) = 1 + x - \\frac{4}{3}x^2 + \\frac{8}{3}x\n\nCombine like terms:\n\nP(x) = 1 + \\left(x + \\frac{8}{3}x\\right) - \\frac{4}{3}x^2 = 1 + \\frac{11x}{3} - \\frac{4x^2}{3}\n\nSo the final polynomial is:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the Newton interpolating polynomial for the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-1a.html",
    "href": "homework/w02/exercise3-1-1a.html",
    "title": "Exercise 3.1.1a (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, 1)):\n\n\n1 \\cdot \\frac{(x - 2)(x - 3)}{(0 - 2)(0 - 3)} = 1 \\cdot \\frac{(x - 2)(x - 3)}{(-2)(-3)} = \\frac{(x - 2)(x - 3)}{6}\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 3))\n\n\n3 \\cdot \\frac{(x - 0)(x - 3)}{(2 - 0)(2 - 3)} = 3 \\cdot \\frac{(x)(x - 3)}{(2)(-1)} = -\\frac{3x(x - 3)}{2}\n\n\nThird term (corresponding to (x_3, y_3) = (3, 0)):\n\n\n0 \\cdot \\frac{(x - 0)(x - 2)}{(3 - 0)(3 - 2)} = 0\n\n\n\n\n\nP(x) = \\frac{(x - 2)(x - 3)}{6} - \\frac{3x(x - 3)}{2}\n\n\n\n\nFirst term:\n\n\\frac{(x - 2)(x - 3)}{6} = \\frac{x^2 - 5x + 6}{6}\n\nSecond term:\n\n-\\frac{3x(x - 3)}{2} = -\\frac{3(x^2 - 3x)}{2} = -\\frac{3x^2}{2} + \\frac{9x}{2}\n\nNow, combine these two terms:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\left(\\frac{3x^2}{2} - \\frac{9x}{2}\\right)\n\nTo combine, first rewrite everything with a denominator of 6:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\frac{9x^2 - 27x}{6}\n\nNow simplify:\n\nP(x) = \\frac{x^2 - 5x + 6 - 9x^2 + 27x}{6}\n\n\nP(x) = \\frac{-8x^2 + 22x + 6}{6}\n\nThis is the final polynomial:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the interpolating polynomial that passes through the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-1a.html#question",
    "href": "homework/w02/exercise3-1-1a.html#question",
    "title": "Exercise 3.1.1a (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, 1)):\n\n\n1 \\cdot \\frac{(x - 2)(x - 3)}{(0 - 2)(0 - 3)} = 1 \\cdot \\frac{(x - 2)(x - 3)}{(-2)(-3)} = \\frac{(x - 2)(x - 3)}{6}\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 3))\n\n\n3 \\cdot \\frac{(x - 0)(x - 3)}{(2 - 0)(2 - 3)} = 3 \\cdot \\frac{(x)(x - 3)}{(2)(-1)} = -\\frac{3x(x - 3)}{2}\n\n\nThird term (corresponding to (x_3, y_3) = (3, 0)):\n\n\n0 \\cdot \\frac{(x - 0)(x - 2)}{(3 - 0)(3 - 2)} = 0\n\n\n\n\n\nP(x) = \\frac{(x - 2)(x - 3)}{6} - \\frac{3x(x - 3)}{2}\n\n\n\n\nFirst term:\n\n\\frac{(x - 2)(x - 3)}{6} = \\frac{x^2 - 5x + 6}{6}\n\nSecond term:\n\n-\\frac{3x(x - 3)}{2} = -\\frac{3(x^2 - 3x)}{2} = -\\frac{3x^2}{2} + \\frac{9x}{2}\n\nNow, combine these two terms:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\left(\\frac{3x^2}{2} - \\frac{9x}{2}\\right)\n\nTo combine, first rewrite everything with a denominator of 6:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\frac{9x^2 - 27x}{6}\n\nNow simplify:\n\nP(x) = \\frac{x^2 - 5x + 6 - 9x^2 + 27x}{6}\n\n\nP(x) = \\frac{-8x^2 + 22x + 6}{6}\n\nThis is the final polynomial:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the interpolating polynomial that passes through the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html",
    "title": "Forward Error in Linear Systems",
    "section": "",
    "text": "The forward error is a critical concept in numerical linear algebra, measuring the difference between the approximate solution \\mathbf{x_a} and the true solution \\mathbf{x} of a linear system A\\mathbf{x} = \\mathbf{b}. It quantifies how far the computed solution is from the exact solution, providing insight into the accuracy of numerical methods.\nThe forward error is defined as:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the vector."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#overview",
    "title": "Forward Error in Linear Systems",
    "section": "",
    "text": "The forward error is a critical concept in numerical linear algebra, measuring the difference between the approximate solution \\mathbf{x_a} and the true solution \\mathbf{x} of a linear system A\\mathbf{x} = \\mathbf{b}. It quantifies how far the computed solution is from the exact solution, providing insight into the accuracy of numerical methods.\nThe forward error is defined as:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the vector."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#what-forward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#what-forward-error-represents",
    "title": "Forward Error in Linear Systems",
    "section": "What Forward Error Represents",
    "text": "What Forward Error Represents\n\nSolution Accuracy:\n\nThe forward error reflects the maximum difference between the components of the true solution and the approximate solution.\n\nComponent-wise Deviation:\n\nIt indicates the largest deviation in any component of the solution vector.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then:\n\n\\text{FE} = 0"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#why-forward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#why-forward-error-matters",
    "title": "Forward Error in Linear Systems",
    "section": "Why Forward Error Matters",
    "text": "Why Forward Error Matters\n\nAssessing Solution Quality:\n\nThe forward error directly measures the accuracy of the approximate solution, helping determine how close it is to the true solution.\n\nError Propagation:\n\nUnderstanding the forward error aids in analyzing how errors in computations propagate through the solution process.\n\nNumerical Stability:\n\nA small forward error indicates that the numerical method is producing reliable results."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#example",
    "title": "Forward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nTo enhance the visualization, we’ll adjust the approximate solution \\mathbf{x_a} so that the differences in both components are whole numbers but not equal, providing a clearer depiction of forward error in multiple dimensions.\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\nStep 1: Verify the True Solution\nCheck that \\mathbf{x} satisfies A\\mathbf{x} = \\mathbf{b}:\n\nA\\mathbf{x} = \\begin{bmatrix} 1 \\times 2 + 1 \\times 1 \\\\ 3 \\times 2 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\mathbf{b}\n\n\n\nStep 2: Compute the Forward Error\nCompute the difference between \\mathbf{x} and \\mathbf{x_a}:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 - 1 \\\\ 1 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nThe forward error is the infinity norm of \\mathbf{e}:\n\n\\text{FE} = \\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\n\n\nStep 3: Interpretation\n\nThe maximum deviation between the true and approximate solutions is 2, occurring in the second component.\nThe approximate solution \\mathbf{x_a} differs from \\mathbf{x} by 1 in the x-component and 2 in the y-component."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#visualization-of-forward-error",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#visualization-of-forward-error",
    "title": "Forward Error in Linear Systems",
    "section": "Visualization of Forward Error",
    "text": "Visualization of Forward Error\nTo illustrate the forward error, we will visualize the true solution \\mathbf{x} and the approximate solution \\mathbf{x_a}, along with the error vector \\mathbf{e} = \\mathbf{x} - \\mathbf{x_a}. By plotting these vectors, we can see how the approximate solution deviates from the true solution in both components.\n\nBlack Vector (\\mathbf{x}): The true solution vector.\nGray Vector (\\mathbf{x_a}): The approximate solution vector.\nOrange Vector (\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a}): The error vector.\nError Components: Projections of \\mathbf{e} onto the x and y axes.\n\n\n\nShow Code\n# Adjusting vector widths and removing dashed styling\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# True solution and approximate solution\nx_true = np.array([2, 1])\nx_a = np.array([1, -1])\n\n# Compute the error vector\ne = x_true - x_a\n\n# Improved visualization with consistent vector widths and no dashed styling\nplt.figure(figsize=(12, 8))  # Wider figure\norigin = np.zeros(2)\n\n# Plot vectors\nplt.quiver(*origin, *x_true, color='black', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{x}$ (True Solution)', width=0.01)\nplt.quiver(*origin, *x_a, color='gray', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{x}_a$ (Approximate Solution)', width=0.01)\nplt.quiver(*x_a, *e, color='orange', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e} = \\mathbf{x} - \\mathbf{x}_a$', width=0.01)\n\n# Annotate vectors with offset and clean positioning\nplt.annotate(r'$\\mathbf{x}_a$', (x_a[0], x_a[1]), textcoords=\"offset points\", xytext=(-30, -15), ha='center', color='gray', fontsize=14)\nplt.annotate(r'$\\mathbf{x}$', (x_true[0], x_true[1]), textcoords=\"offset points\", xytext=(0, 10), ha='center', color='black', fontsize=14)\nplt.annotate(r'$\\mathbf{e}$', (x_a[0] + e[0]/2, x_a[1] + e[1]/2), textcoords=\"offset points\", xytext=(15, -15), ha='center', color='orange', fontsize=14)\n\n# Show vector addition\nplt.plot([x_a[0], x_true[0]], [x_a[1], x_true[1]], color='orange', linewidth=1.5)\n\n# Plot error components with consistent styling\nplt.quiver(*x_a, e[0], 0, color='red', angles='xy', scale_units='xy', scale=1, width=0.01, label=r'$\\mathbf{e_1}$')\nplt.quiver(x_a[0] + e[0], x_a[1], 0, e[1], color='purple', angles='xy', scale_units='xy', scale=1, width=0.01, label=r'$\\mathbf{e_2}$')\n\n# Annotate error components\nplt.annotate(r'$\\mathbf{e_1}$', (x_a[0] + e[0]/2, x_a[1] - 0.2), textcoords=\"offset points\", xytext=(0, -20), ha='center', color='red', fontsize=14)\nplt.annotate(r'$\\mathbf{e_2}$', (x_a[0] + e[0] + 0.1, x_a[1] + e[1]/2), textcoords=\"offset points\", xytext=(20, 0), ha='center', color='purple', fontsize=14)\n\n# Highlight the maximum component of the error with clearer placement\nplt.text(x_a[0] + e[0] + 0.3, x_a[1] + e[1] + 0.3, r'$\\max(|\\mathbf{e_1}|, |\\mathbf{e_2}|) = |\\mathbf{e_2}| = 2$', color='purple', fontsize=14)\n\n# Graph details for cleaner look\nplt.xlim(-1, 5)\nplt.ylim(-2, 3)\nplt.axhline(0, color='black', linewidth=0.8)\nplt.axvline(0, color='black', linewidth=0.8)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.7)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Forward Error with Error Components', fontsize=18)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nVector Addition:\n\nThe error vector \\mathbf{e} is drawn starting from \\mathbf{x_a} and pointing towards \\mathbf{x}, demonstrating that:\n\n\\mathbf{x_a} + \\mathbf{e} = \\mathbf{x}\n\n\nError Components:\n\nThe error vector \\mathbf{e} is decomposed into its x-component e_1 (red dashed arrow) and y-component e_2 (purple dashed arrow).\nThe components are:\n\ne_1 = 1, \\quad e_2 = 2\n\n\nInfinity Norm Highlighted:\n\nThe maximum absolute component of the error is |e_2| = 2, which is the forward error \\text{FE} = \\|\\mathbf{e}\\|_\\infty.\nThis is highlighted in the graph with a text annotation.\n\nUnderstanding the Forward Error:\n\nBy visualizing the error components, we see that the deviation occurs in both the x and y components.\nThe largest error is in the y-component, which determines the forward error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#conclusion",
    "title": "Forward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nForward Error Significance:\n\nThe forward error provides a direct measure of the accuracy of the approximate solution \\mathbf{x_a} in relation to the true solution \\mathbf{x}.\n\nVisualization Enhancements:\n\nBy adjusting \\mathbf{x_a} to differ by whole numbers in both components, the visualization effectively demonstrates how errors in multiple dimensions contribute to the overall forward error.\nDecomposing the error vector into its components and highlighting the infinity norm offers a clearer understanding of how the forward error is calculated.\n\nPractical Implications:\n\nIn numerical computations, minimizing the forward error is crucial for obtaining accurate solutions."
  },
  {
    "objectID": "notes/w03/runge-phenomenon.html",
    "href": "notes/w03/runge-phenomenon.html",
    "title": "Runge Phenomenon",
    "section": "",
    "text": "The Runge Phenomenon refers to the oscillatory behavior that occurs when using high-degree polynomial interpolation on evenly spaced data points, particularly near the endpoints of an interval. The phenomenon is named after Carl Runge, who discovered this issue while studying interpolation of functions with large oscillations near the boundaries of an interval.\n\n\nWhen interpolating a smooth function using polynomials of high degree, the interpolation can become highly oscillatory near the edges of the interval, even if the function being interpolated is smooth and well-behaved. This is particularly problematic with equally spaced points, as the polynomial tries to fit too closely to the data points near the boundaries, leading to large errors.\n\n\n\nThe most famous example illustrating the Runge phenomenon is based on the Runge function:\n\\[\nf(x) = \\frac{1}{1 + 25x^2}\n\\]\nRunge showed that using high-degree polynomial interpolation on this function with evenly spaced points over the interval \\([-5, 5]\\) leads to significant oscillations near the edges. The interpolation error grows as the degree of the polynomial increases, especially near the endpoints of the interval.\n\n\n\nThe Runge phenomenon arises because high-degree polynomials tend to oscillate more as their degree increases, especially when they are forced to pass through many points. With equally spaced points, the interpolation error is concentrated near the edges of the interval, causing large deviations from the true function in those regions.\nMathematically, for a function \\(f(x)\\) interpolated at \\(n\\) evenly spaced points using a polynomial \\(P_n(x)\\) of degree \\(n-1\\), the interpolation error is given by:\n\\[\n|f(x) - P_n(x)| = \\frac{|f^{(n)}(c)|}{n!} \\prod_{i=1}^{n}(x - x_i)\n\\]\nAs the number of interpolation points \\(n\\) increases, the product term \\(\\prod_{i=1}^{n}(x - x_i)\\) becomes very large near the endpoints of the interval, causing the interpolation error to increase dramatically.\n\n\n\nThe following figure illustrates the Runge phenomenon. For the Runge function \\(f(x) = \\frac{1}{1 + 25x^2}\\), the polynomial interpolation for large \\(n\\) shows excessive oscillations near the interval boundaries:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\n# Define the x values\nx_values = np.linspace(-5, 5, 1000)\n\n# Plot the original Runge function\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, runge_function(x_values), label=\"Runge Function\", color=\"blue\")\n\n# Show interpolation for degree 5 and degree 10 polynomials\nplt.title(\"Runge Phenomenon\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe Runge phenomenon can be mitigated by using Chebyshev nodes instead of equally spaced points. Chebyshev nodes distribute the points more densely near the edges of the interval, where the oscillations are most likely to occur. This results in more accurate interpolation with less oscillation near the boundaries.\nChebyshev nodes \\(x_i\\) for \\(n\\) interpolation points on the interval \\([-1, 1]\\) are given by:\n\\[\nx_i = \\cos\\left(\\frac{(2i - 1)\\pi}{2n}\\right)\n\\]\nUsing Chebyshev interpolation reduces the risk of large oscillations near the endpoints, providing a more stable approximation.\n\n\n\nThe Runge phenomenon highlights the dangers of using high-degree polynomials for interpolation with evenly spaced points. To avoid this issue, it is recommended to use Chebyshev interpolation or lower-degree polynomial interpolation in smaller intervals (piecewise interpolation)."
  },
  {
    "objectID": "notes/w03/runge-phenomenon.html#introduction",
    "href": "notes/w03/runge-phenomenon.html#introduction",
    "title": "Runge Phenomenon",
    "section": "",
    "text": "The Runge Phenomenon refers to the oscillatory behavior that occurs when using high-degree polynomial interpolation on evenly spaced data points, particularly near the endpoints of an interval. The phenomenon is named after Carl Runge, who discovered this issue while studying interpolation of functions with large oscillations near the boundaries of an interval.\n\n\nWhen interpolating a smooth function using polynomials of high degree, the interpolation can become highly oscillatory near the edges of the interval, even if the function being interpolated is smooth and well-behaved. This is particularly problematic with equally spaced points, as the polynomial tries to fit too closely to the data points near the boundaries, leading to large errors.\n\n\n\nThe most famous example illustrating the Runge phenomenon is based on the Runge function:\n\\[\nf(x) = \\frac{1}{1 + 25x^2}\n\\]\nRunge showed that using high-degree polynomial interpolation on this function with evenly spaced points over the interval \\([-5, 5]\\) leads to significant oscillations near the edges. The interpolation error grows as the degree of the polynomial increases, especially near the endpoints of the interval.\n\n\n\nThe Runge phenomenon arises because high-degree polynomials tend to oscillate more as their degree increases, especially when they are forced to pass through many points. With equally spaced points, the interpolation error is concentrated near the edges of the interval, causing large deviations from the true function in those regions.\nMathematically, for a function \\(f(x)\\) interpolated at \\(n\\) evenly spaced points using a polynomial \\(P_n(x)\\) of degree \\(n-1\\), the interpolation error is given by:\n\\[\n|f(x) - P_n(x)| = \\frac{|f^{(n)}(c)|}{n!} \\prod_{i=1}^{n}(x - x_i)\n\\]\nAs the number of interpolation points \\(n\\) increases, the product term \\(\\prod_{i=1}^{n}(x - x_i)\\) becomes very large near the endpoints of the interval, causing the interpolation error to increase dramatically.\n\n\n\nThe following figure illustrates the Runge phenomenon. For the Runge function \\(f(x) = \\frac{1}{1 + 25x^2}\\), the polynomial interpolation for large \\(n\\) shows excessive oscillations near the interval boundaries:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\n# Define the x values\nx_values = np.linspace(-5, 5, 1000)\n\n# Plot the original Runge function\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, runge_function(x_values), label=\"Runge Function\", color=\"blue\")\n\n# Show interpolation for degree 5 and degree 10 polynomials\nplt.title(\"Runge Phenomenon\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe Runge phenomenon can be mitigated by using Chebyshev nodes instead of equally spaced points. Chebyshev nodes distribute the points more densely near the edges of the interval, where the oscillations are most likely to occur. This results in more accurate interpolation with less oscillation near the boundaries.\nChebyshev nodes \\(x_i\\) for \\(n\\) interpolation points on the interval \\([-1, 1]\\) are given by:\n\\[\nx_i = \\cos\\left(\\frac{(2i - 1)\\pi}{2n}\\right)\n\\]\nUsing Chebyshev interpolation reduces the risk of large oscillations near the endpoints, providing a more stable approximation.\n\n\n\nThe Runge phenomenon highlights the dangers of using high-degree polynomials for interpolation with evenly spaced points. To avoid this issue, it is recommended to use Chebyshev interpolation or lower-degree polynomial interpolation in smaller intervals (piecewise interpolation)."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html",
    "href": "notes/w08/gram-schmidt-orthogonalization.html",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Gram-Schmidt Orthogonalization process is a fundamental technique in linear algebra for transforming a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This method is widely used in applications such as QR factorization, solving least squares problems, and numerical linear algebra. This note explores the definition, properties, computation, and applications of the Gram-Schmidt process."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#overview",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#overview",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Gram-Schmidt Orthogonalization process is a fundamental technique in linear algebra for transforming a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This method is widely used in applications such as QR factorization, solving least squares problems, and numerical linear algebra. This note explores the definition, properties, computation, and applications of the Gram-Schmidt process."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#definition-and-process",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#definition-and-process",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Definition and Process",
    "text": "Definition and Process\nGiven a set of linearly independent vectors \\(A_1, A_2, \\dots, A_n\\), the Gram-Schmidt process produces an orthogonal (or orthonormal) set of vectors \\(q_1, q_2, \\dots, q_n\\) such that:\n\nEach vector \\(q_i\\) is orthogonal to the previous vectors \\(q_1, q_2, \\dots, q_{i-1}\\).\nThe span of \\(q_1, q_2, \\dots, q_n\\) is the same as the span of \\(A_1, A_2, \\dots, A_n\\).\n\nThis orthogonal set can also be normalized to create an orthonormal basis."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#steps-of-the-gram-schmidt-process",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#steps-of-the-gram-schmidt-process",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Steps of the Gram-Schmidt Process",
    "text": "Steps of the Gram-Schmidt Process\nThe Gram-Schmidt process involves the following steps:\n\nInitialize with the First Vector: Set \\(q_1\\) as the normalized version of \\(A_1\\):\n\\[\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\\]\nCompute Subsequent Vectors: For each vector \\(A_i\\), construct a new vector \\(u_i\\) by subtracting components that align with previously computed \\(q\\)-vectors. Normalize \\(u_i\\) to obtain \\(q_i\\):\n\nDefine the non-normalized vector \\(u_i\\): \\[\nu_i = A_i - \\sum_{j=1}^{i-1} (q_j \\cdot A_i) \\, q_j\n\\]\nNormalize \\(u_i\\) to obtain \\(q_i\\): \\[\nq_i = \\frac{u_i}{\\|u_i\\|}\n\\]\n\n\nEach vector \\(q_i\\) is thus orthogonal to the preceding \\(q\\)-vectors and has unit length if normalized."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#example",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#example",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Example",
    "text": "Example\nGiven vectors \\(A_1\\) and \\(A_2\\), the Gram-Schmidt process works as follows:\n\nCalculate \\(q_1\\):\n\\[\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\\]\nCalculate \\(q_2\\):\n\nFirst, remove the component of \\(A_2\\) in the direction of \\(q_1\\) to obtain \\(u_2\\): \\[\nu_2 = A_2 - (q_1 \\cdot A_2) q_1\n\\]\nThen, normalize \\(u_2\\) to get \\(q_2\\): \\[\nq_2 = \\frac{u_2}{\\|u_2\\|}\n\\]"
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#properties-of-gram-schmidt-orthogonalization",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#properties-of-gram-schmidt-orthogonalization",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Properties of Gram-Schmidt Orthogonalization",
    "text": "Properties of Gram-Schmidt Orthogonalization\n\nOrthogonality: Each vector \\(q_i\\) is orthogonal to all previously generated vectors \\(q_1, \\dots, q_{i-1}\\).\nSpan Preservation: The set \\(\\{q_1, q_2, \\dots, q_n\\}\\) spans the same subspace as the original set \\(\\{A_1, A_2, \\dots, A_n\\}\\).\nOrthonormal Basis: By normalizing each \\(u_i\\) to get \\(q_i\\), the process yields an orthonormal basis for the subspace."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#applications-of-gram-schmidt-orthogonalization",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#applications-of-gram-schmidt-orthogonalization",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Applications of Gram-Schmidt Orthogonalization",
    "text": "Applications of Gram-Schmidt Orthogonalization\nGram-Schmidt orthogonalization is widely applied in various fields due to its utility in creating orthogonal bases:\n\nQR Factorization: Used to decompose a matrix into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\).\nLeast Squares Problems: Assists in minimizing the error in fitting data to a model by creating orthogonal projections.\nSignal Processing and Data Compression: Forms the foundation for methods that reduce redundancy by representing data in orthogonal bases.\nMachine Learning and Statistics: Simplifies computations by projecting data onto orthogonal components."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#example-problem",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#example-problem",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Given the vectors\n\\[\nA_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}\n\\]\n\nUse Gram-Schmidt to find orthogonal vectors \\(q_1\\) and \\(q_2\\).\nNormalize \\(q_1\\) and \\(q_2\\) to form an orthonormal basis.\n\n\nSolution Steps\n\nCompute \\(q_1\\) by normalizing \\(A_1\\).\nCalculate \\(u_2\\) by removing the component of \\(A_2\\) in the direction of \\(q_1\\).\nNormalize \\(u_2\\) to obtain \\(q_2\\)."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#conclusion",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#conclusion",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Conclusion",
    "text": "Conclusion\nThe Gram-Schmidt process is a valuable tool in linear algebra for constructing orthogonal (or orthonormal) bases. By transforming a set of linearly independent vectors, it simplifies many matrix operations and lays the groundwork for QR factorization, data projections, and error minimization in least squares problems."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html",
    "href": "notes/w07/norms/infinity-vector-norm.html",
    "title": "Infinity Vector Norm",
    "section": "",
    "text": "The infinity norm (also called the maximum norm or \\ell_\\infty-norm) measures the size of a vector by taking the maximum absolute value of its components. Unlike the Euclidean or Taxicab norms, which involve summing components, the infinity norm focuses on the “largest step” in any single direction."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#overview",
    "href": "notes/w07/norms/infinity-vector-norm.html#overview",
    "title": "Infinity Vector Norm",
    "section": "",
    "text": "The infinity norm (also called the maximum norm or \\ell_\\infty-norm) measures the size of a vector by taking the maximum absolute value of its components. Unlike the Euclidean or Taxicab norms, which involve summing components, the infinity norm focuses on the “largest step” in any single direction."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#definition",
    "href": "notes/w07/norms/infinity-vector-norm.html#definition",
    "title": "Infinity Vector Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the infinity norm is defined as:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|.\n\nThis norm is often used in settings where the largest component dominates or in grid-based systems where movement is limited by a single axis."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#properties",
    "href": "notes/w07/norms/infinity-vector-norm.html#properties",
    "title": "Infinity Vector Norm",
    "section": "Properties",
    "text": "Properties\nThe infinity norm satisfies the following properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_\\infty \\geq 0, and \\|\\mathbf{v}\\|_\\infty = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_\\infty = |c| \\|\\mathbf{v}\\|_\\infty.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_\\infty \\leq \\|\\mathbf{u}\\|_\\infty + \\|\\mathbf{v}\\|_\\infty."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#examples",
    "href": "notes/w07/norms/infinity-vector-norm.html#examples",
    "title": "Infinity Vector Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Example\nFor \\mathbf{v} = [3, -4]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|3|, |-4|) = \\max(3, 4) = 4.\n\n\n\n2. 3D Example\nFor \\mathbf{v} = [1, -2, 3]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|1|, |-2|, |3|) = \\max(1, 2, 3) = 3.\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\dots, |v_n|)."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#applications",
    "href": "notes/w07/norms/infinity-vector-norm.html#applications",
    "title": "Infinity Vector Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Error Analysis\nThe infinity norm is used to measure the largest error in numerical solutions, ensuring that no individual error component dominates the result.\n\n\n2. Optimization\nIn optimization problems, the infinity norm simplifies constraints by focusing on the largest deviation in variables.\n\n\n3. Machine Learning\nThe infinity norm is used in regularization techniques and as a metric in certain classification problems.\n\n\n4. Computational Efficiency\nSince the infinity norm involves only a maximum operation, it is computationally inexpensive compared to other norms."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#visualization",
    "href": "notes/w07/norms/infinity-vector-norm.html#visualization",
    "title": "Infinity Vector Norm",
    "section": "Visualization",
    "text": "Visualization\nIn 2D, the set of points at a fixed infinity norm distance from the origin forms a square aligned with the coordinate axes. For example, all points satisfying \\|\\mathbf{v}\\|_\\infty = 3 in \\mathbb{R}^2 would form the square:\n\n\\max(|x|, |y|) = 3,\n\nor equivalently:\n\n-3 \\leq x \\leq 3, \\quad -3 \\leq y \\leq 3."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#example-problem",
    "href": "notes/w07/norms/infinity-vector-norm.html#example-problem",
    "title": "Infinity Vector Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the infinity norm of \\mathbf{v} = [-3, 4, -5].\n\nSolution:\n\nTake the absolute values of the components: |-3| = 3, |4| = 4, |-5| = 5.\nFind the maximum: \\|\\mathbf{v}\\|_\\infty = \\max(3, 4, 5) = 5."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#conclusion",
    "href": "notes/w07/norms/infinity-vector-norm.html#conclusion",
    "title": "Infinity Vector Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe infinity norm provides a simple and efficient way to measure vector size by focusing on the largest component. It is particularly useful in applications like error analysis, optimization, and machine learning where the largest deviation or influence is of primary interest."
  },
  {
    "objectID": "notes/w07/norms/index.html",
    "href": "notes/w07/norms/index.html",
    "title": "NORMS",
    "section": "",
    "text": "Euclidean Vector Norm\nTaxicab Vector Norm\nInfinity Vector Norm\nInfinity Norm for Matrices"
  },
  {
    "objectID": "notes/w07/lu-factorization.html",
    "href": "notes/w07/lu-factorization.html",
    "title": "LU Factorization",
    "section": "",
    "text": "LU Factorization (or LU Decomposition) is a powerful technique in linear algebra for breaking down a matrix \\(A\\) into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\). This factorization is commonly used for solving linear systems, computing determinants, and inverting matrices. This note explores LU Factorization’s definition, properties, computation, and applications."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#overview",
    "href": "notes/w07/lu-factorization.html#overview",
    "title": "LU Factorization",
    "section": "",
    "text": "LU Factorization (or LU Decomposition) is a powerful technique in linear algebra for breaking down a matrix \\(A\\) into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\). This factorization is commonly used for solving linear systems, computing determinants, and inverting matrices. This note explores LU Factorization’s definition, properties, computation, and applications."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#definition-and-decomposition",
    "href": "notes/w07/lu-factorization.html#definition-and-decomposition",
    "title": "LU Factorization",
    "section": "Definition and Decomposition",
    "text": "Definition and Decomposition\nFor a square matrix \\(A\\), LU Factorization is given by:\n\\[\nA = LU\n\\]\nwhere:\n\n\\(L\\) is a lower triangular matrix with ones on the diagonal.\n\\(U\\) is an upper triangular matrix.\n\nIf \\(A\\) cannot be decomposed directly, partial pivoting may be applied, resulting in:\n\\[\nPA = LU\n\\]\nwhere \\(P\\) is a permutation matrix that records row exchanges."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#conditions-for-lu-factorization",
    "href": "notes/w07/lu-factorization.html#conditions-for-lu-factorization",
    "title": "LU Factorization",
    "section": "Conditions for LU Factorization",
    "text": "Conditions for LU Factorization\nLU Factorization is valid when:\n\nMatrix is Square: \\(A\\) must be a square matrix.\nNon-Singular Leading Submatrices: Each leading principal submatrix (upper-left submatrix) of \\(A\\) must be non-singular.\n\nWhen these conditions are not met, row pivoting enables decomposition."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#factorization-process",
    "href": "notes/w07/lu-factorization.html#factorization-process",
    "title": "LU Factorization",
    "section": "Factorization Process",
    "text": "Factorization Process\nTo factorize \\(A\\) into \\(L\\) and \\(U\\):\n\nEliminate Elements: Perform row operations to create zeros below the main diagonal of \\(U\\).\nStore Multipliers: Record the multipliers in \\(L\\).\n\n\nExample\nGiven a \\(3 \\times 3\\) matrix:\n\\[\nA = \\begin{pmatrix} 2 & 3 & 1 \\\\ 4 & 7 & -1 \\\\ -2 & 3 & 5 \\end{pmatrix}\n\\]\n\nTransform \\(A\\) into \\(U\\) using row operations.\nRecord elimination factors in \\(L\\).\nThe result satisfies \\(A = LU\\)."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#solving-systems-with-lu-factorization",
    "href": "notes/w07/lu-factorization.html#solving-systems-with-lu-factorization",
    "title": "LU Factorization",
    "section": "Solving Systems with LU Factorization",
    "text": "Solving Systems with LU Factorization\nLU Factorization allows us to solve \\(Ax = b\\) by breaking it down into two simpler systems:\n\nSolve \\(Ly = b\\): Use forward substitution, as \\(L\\) is lower triangular.\nSolve \\(Ux = y\\): Use back substitution with \\(U\\) as an upper triangular matrix.\n\n\nEfficiency\nThis approach reduces computation time, especially when solving multiple systems with the same \\(A\\) but different \\(b\\) vectors, as the factorization needs to be computed only once."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#pivoting-and-permutation",
    "href": "notes/w07/lu-factorization.html#pivoting-and-permutation",
    "title": "LU Factorization",
    "section": "Pivoting and Permutation",
    "text": "Pivoting and Permutation\nIn cases where \\(A\\) has zeros or small values on the diagonal, partial pivoting improves numerical stability by reordering rows to place a larger element on the diagonal:\n\\[\nPA = LU\n\\]\nwhere \\(P\\) is a permutation matrix that tracks row exchanges."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#advantages-of-lu-factorization",
    "href": "notes/w07/lu-factorization.html#advantages-of-lu-factorization",
    "title": "LU Factorization",
    "section": "Advantages of LU Factorization",
    "text": "Advantages of LU Factorization\n\nEfficient Linear System Solving: Faster than Gaussian elimination for repeated systems.\nDeterminant Computation: The determinant of \\(A\\) is the product of the diagonal elements of \\(U\\).\nMatrix Inversion: LU Factorization simplifies inversion by inverting \\(L\\) and \\(U\\) separately."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#applications-of-lu-factorization",
    "href": "notes/w07/lu-factorization.html#applications-of-lu-factorization",
    "title": "LU Factorization",
    "section": "Applications of LU Factorization",
    "text": "Applications of LU Factorization\nLU Factorization is widely used across various fields due to its computational efficiency:\n\nNumerical Linear Algebra: Fundamental for solving linear systems.\nOptimization: Integral in algorithms that rely on matrix decompositions.\nComputer Graphics: Enables transformations and projections.\nScientific Computing: Common in simulations and solving differential equations."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#example-problem",
    "href": "notes/w07/lu-factorization.html#example-problem",
    "title": "LU Factorization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: For the matrix\n\\[\nA = \\begin{pmatrix} 3 & -7 & -2 \\\\ -3 & 5 & 1 \\\\ 6 & -4 & 0 \\end{pmatrix}\n\\]\n\nFactorize \\(A\\) into \\(L\\) and \\(U\\).\nSolve \\(Ax = b\\) for \\(b = \\begin{pmatrix} 5 \\\\ -1 \\\\ 3 \\end{pmatrix}\\).\n\n\nSolution Steps\n\nPerform row operations to decompose \\(A\\) into \\(L\\) and \\(U\\).\nSolve \\(Ly = b\\) via forward substitution.\nSolve \\(Ux = y\\) via back substitution."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#conclusion",
    "href": "notes/w07/lu-factorization.html#conclusion",
    "title": "LU Factorization",
    "section": "Conclusion",
    "text": "Conclusion\nLU Factorization is an essential tool in linear algebra, providing a simplified method to solve linear systems efficiently. By breaking matrices into triangular forms, it reduces computational complexity and lays the groundwork for more advanced numerical techniques."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/index.html",
    "href": "notes/w07/errors-analysis-linear-systems/index.html",
    "title": "LINEAR SYSTEM ERROR ANALYSIS",
    "section": "",
    "text": "Residual\nBackward Error\nRelative Backward Error"
  },
  {
    "objectID": "notes/w04/trapezoidal-rule.html",
    "href": "notes/w04/trapezoidal-rule.html",
    "title": "Trapezoidal Rule",
    "section": "",
    "text": "Overview\n\nThe trapezoidal rule is a numerical method for approximating definite integrals. It works by dividing the interval of integration into sub-intervals and approximating the area under the curve as a series of trapezoids. This method is particularly useful for complex functions or when an analytical solution is not feasible.\n\n\nGeneral Formula\n\nGiven the integral:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\nThe trapezoidal rule estimates the integral by approximating the region under the curve with trapezoids.\n\n\nEqual Sub-Intervals (Uniform Spacing)\n\nWhen the interval \\([a, b]\\) is divided into \\(n\\) equal sub-intervals of width:\n\\[\n\\Delta x = \\frac{b - a}{n}\n\\]\nThe composite trapezoidal rule can be expressed as:\n\\[\nT_n = \\frac{\\Delta x}{2} \\left( y_0 + 2 y_1 + 2 y_2 + \\dots + 2 y_{n-1} + y_n \\right)\n\\]\nwhere:\n\n\\(y_0 = f(a)\\) and \\(y_n = f(b)\\) are the function values at the endpoints.\n\\(y_1, y_2, \\dots, y_{n-1}\\) are the function values at the interior points, each multiplied by 2 because they are shared by two adjacent trapezoids.\n\n\nStep-by-Step Formula in Function Terms\nUsing function values at each point \\(x_i = a + i \\Delta x\\), the formula becomes:\n\\[\nT_n = \\frac{\\Delta x}{2} \\left( f(a) + 2 f(a + \\Delta x) + 2 f(a + 2 \\Delta x) + \\dots + 2 f(a + (n - 1) \\Delta x) + f(b) \\right)\n\\]\nThis formula simplifies integration when the sub-intervals are uniformly spaced.\n\n\nSummation Notation for Uniform Spacing\nAlternatively, the composite trapezoidal rule for uniform spacing can be written as:\n\\[\nT_n = \\frac{\\Delta x}{2} \\left( f(x_0) + 2 \\sum_{i=1}^{n-1} f(x_i) + f(x_n) \\right)\n\\]\nwhere:\n\n\\(x_0 = a\\), \\(x_n = b\\), and \\(x_i = a + i \\Delta x\\).\nThe summation accounts for the contributions of interior points.\n\n\n\n\nUnequal Sub-Intervals (Non-Uniform Spacing)\n\nWhen the sub-intervals are not equally spaced, the formula adjusts to account for varying widths \\(\\Delta x_i\\) between points. The non-uniform trapezoidal rule becomes:\n\\[\nT = \\sum_{i=1}^{n} \\frac{\\Delta x_i}{2} \\left( f(x_{i-1}) + f(x_i) \\right)\n\\]\nwhere:\n\nEach term computes the area of a trapezoid over the individual sub-interval \\([x_{i-1}, x_i]\\).\nThe width of each sub-interval is \\(\\Delta x_i = x_i - x_{i-1}\\), which may vary.\n\n\n\nComparison: Equal vs. Unequal Sub-Intervals\n\n\n\n\n\n\n\n\nProperty\nEqual Sub-Intervals\nUnequal Sub-Intervals\n\n\n\n\nWidth of Sub-Intervals\n\\(\\Delta x = \\frac{b - a}{n}\\) (constant)\n\\(\\Delta x_i = x_i - x_{i-1}\\) (varies)\n\n\nFormula\n\\(\\frac{\\Delta x}{2} \\left( y_0 + 2 y_1 + \\dots + 2 y_{n-1} + y_n \\right)\\)\n\\(\\sum \\frac{\\Delta x_i}{2} \\left( f(x_{i-1}) + f(x_i) \\right)\\)\n\n\nComputational Effort\nEasier to compute with uniform spacing\nMore complicated due to variable widths"
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html",
    "href": "notes/w03/chebyshev-interpolation.html",
    "title": "Chebyshev Interpolation",
    "section": "",
    "text": "Chebyshev Interpolation is a powerful technique in numerical analysis used to approximate functions with polynomials, particularly minimizing errors and avoiding issues like Runge’s phenomenon. This note provides an overview of Chebyshev Interpolation, including its definition, properties, advantages, and practical implementation."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#introduction",
    "href": "notes/w03/chebyshev-interpolation.html#introduction",
    "title": "Chebyshev Interpolation",
    "section": "Introduction",
    "text": "Introduction\nInterpolation involves approximating a function \\(f(x)\\) using a polynomial \\(P_n(x)\\) that passes through a set of points (nodes). While polynomial interpolation is straightforward, selecting appropriate nodes is crucial to ensure accuracy and stability. Chebyshev Interpolation leverages Chebyshev nodes to construct interpolating polynomials that minimize the maximum error across the interpolation interval."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#chebyshev-polynomials",
    "href": "notes/w03/chebyshev-interpolation.html#chebyshev-polynomials",
    "title": "Chebyshev Interpolation",
    "section": "Chebyshev Polynomials",
    "text": "Chebyshev Polynomials\nChebyshev polynomials are a sequence of orthogonal polynomials that arise in various approximation and interpolation problems. They are defined recursively and have properties that make them ideal for minimizing interpolation errors.\n\nDefinition\nThe Chebyshev polynomials of the first kind, \\(T_n(x)\\), are defined by:\n\\[\nT_n(x) = \\cos(n \\arccos x), \\quad \\text{for } x \\in [-1, 1]\n\\]\n\n\nProperties\n\nOrthogonality: Chebyshev polynomials are orthogonal with respect to the weight \\(w(x) = \\frac{1}{\\sqrt{1 - x^2}}\\) on the interval \\([-1, 1]\\).\nExtremal Property: Among all polynomials of degree \\(n\\) with leading coefficient \\(2^{n-1}\\), \\(T_n(x)\\) has the smallest maximum deviation from zero on \\([-1, 1]\\).\nRoots and Extremes: The roots of \\(T_n(x)\\) are given by:\n\\[\nx_k = \\cos\\left( \\frac{2k - 1}{2n} \\pi \\right), \\quad k = 1, 2, \\dots, n\n\\]\nThe extrema (maximum and minimum points) of \\(T_n(x)\\) occur at:\n\\[\nx_k = \\cos\\left( \\frac{k}{n} \\pi \\right), \\quad k = 0, 1, \\dots, n\n\\]"
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#chebyshev-nodes",
    "href": "notes/w03/chebyshev-interpolation.html#chebyshev-nodes",
    "title": "Chebyshev Interpolation",
    "section": "Chebyshev Nodes",
    "text": "Chebyshev Nodes\nChebyshev nodes are specific points in the interval \\([-1, 1]\\) used as interpolation nodes to minimize the interpolation error.\n\nDefinition\nFor \\(n+1\\) Chebyshev nodes, the \\(k\\)-th node \\(x_k\\) is given by:\n\\[\nx_k = \\cos\\left( \\frac{2k + 1}{2n + 2} \\pi \\right), \\quad k = 0, 1, 2, \\dots, n\n\\]\nAlternatively, they can be expressed as:\n\\[\nx_k = \\cos\\left( \\frac{(2k + 1)\\pi}{2(n + 1)} \\right), \\quad k = 0, 1, 2, \\dots, n\n\\]\n\n\nMapping to Arbitrary Intervals\nFor an interval \\([a, b]\\), Chebyshev nodes are mapped as:\n\\[\nx_k = \\frac{a + b}{2} + \\frac{b - a}{2} \\cos\\left( \\frac{(2k + 1)\\pi}{2(n + 1)} \\right), \\quad k = 0, 1, 2, \\dots, n\n\\]\n\n\nImportance\nUsing Chebyshev nodes instead of equally spaced nodes helps in minimizing the oscillatory behavior (Runge’s phenomenon) and ensures better convergence properties of the interpolating polynomial."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#interpolation-process",
    "href": "notes/w03/chebyshev-interpolation.html#interpolation-process",
    "title": "Chebyshev Interpolation",
    "section": "Interpolation Process",
    "text": "Interpolation Process\nThe Chebyshev Interpolation process involves the following steps:\n\nSelect Chebyshev Nodes: Determine the \\(n+1\\) Chebyshev nodes \\(x_0, x_1, \\dots, x_n\\) in the interval \\([-1, 1]\\).\nEvaluate the Function: Compute the function values \\(f(x_0), f(x_1), \\dots, f(x_n)\\).\nConstruct the Interpolating Polynomial: Use methods such as the Chebyshev series expansion or the barycentric interpolation formula to construct the interpolating polynomial \\(Q_n(x)\\).\nApproximate the Function: Use \\(Q_n(x)\\) to approximate \\(f(x)\\) within the interval.\n\n\nBarycentric Interpolation Formula\nOne efficient method to compute the interpolating polynomial is the barycentric interpolation formula:\n\\[\nQ_n(x) = \\frac{\\sum_{k=0}^n \\frac{w_k f(x_k)}{x - x_k}}{\\sum_{k=0}^n \\frac{w_k}{x - x_k}}\n\\]\nwhere \\(w_k\\) are the barycentric weights defined as:\n\\[\nw_k = (-1)^k \\sin\\left( \\frac{(2k + 1)\\pi}{2n + 2} \\right)\n\\]"
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#advantages-of-chebyshev-interpolation",
    "href": "notes/w03/chebyshev-interpolation.html#advantages-of-chebyshev-interpolation",
    "title": "Chebyshev Interpolation",
    "section": "Advantages of Chebyshev Interpolation",
    "text": "Advantages of Chebyshev Interpolation\n\nMinimized Error: Chebyshev nodes minimize the maximum error (uniform convergence) of the interpolating polynomial.\nReduced Oscillations: Avoids Runge’s phenomenon, where high-degree polynomial interpolations at equally spaced nodes exhibit large oscillations near the interval endpoints.\nEfficient Computation: The barycentric interpolation formula allows for efficient and stable computation of interpolating polynomials.\nOrthogonality: Leveraging the orthogonality of Chebyshev polynomials aids in various approximation and numerical integration techniques."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#error-analysis",
    "href": "notes/w03/chebyshev-interpolation.html#error-analysis",
    "title": "Chebyshev Interpolation",
    "section": "Error Analysis",
    "text": "Error Analysis\nUnderstanding the error associated with Chebyshev Interpolation is crucial for assessing the approximation’s reliability.\n\nInterpolation Error Formula\nFor a function \\(f(x)\\) sufficiently smooth on \\([-1, 1]\\), the error of the Chebyshev interpolating polynomial \\(Q_n(x)\\) of degree \\(n\\) is given by:\n\\[\n|f(x) - Q_n(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\frac{1}{2^{n+1}}\n\\]\nwhere:\n\n\\(M\\) is an upper bound on the \\((n+1)\\)-th derivative of \\(f(x)\\) on \\([-1, 1]\\).\n\n\n\nWorst-Case Error Estimate\nFor example, if \\(f(x) = e^x\\), all derivatives are \\(f^{(k)}(x) = e^x\\). On \\([-1, 1]\\), \\(e^x \\leq e\\), so \\(M = e\\).\nFor a fifth-degree polynomial (\\(n = 5\\)):\n\\[\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{2^5} = \\frac{e}{720 \\times 32} \\approx 0.000118\n\\]\n\n\nImplications\nAn error bound of approximately \\(1.18 \\times 10^{-4}\\) implies that at least three decimal digits of the approximation \\(Q_5(x)\\) are accurate across the interval \\([-1, 1]\\)."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#example",
    "href": "notes/w03/chebyshev-interpolation.html#example",
    "title": "Chebyshev Interpolation",
    "section": "Example",
    "text": "Example\nProblem: Approximate \\(f(x) = e^x\\) on \\([-1, 1]\\) using a fifth-degree Chebyshev interpolating polynomial \\(Q_5(x)\\). Estimate the worst-case error and determine the number of correct decimal digits in the approximation.\nSolution:\n\nDetermine \\(M\\):\n\n\\(f^{(6)}(x) = e^x\\), so \\(M = e\\).\n\nCompute the Error Bound:\n\\[\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{2^5} = \\frac{2.71828}{720 \\times 32} \\approx 0.000118\n\\]\nInterpret the Error:\n\nThe approximation \\(Q_5(x)\\) deviates from \\(e^x\\) by less than \\(1.18 \\times 10^{-4}\\).\nAt least three decimal digits of \\(Q_5(x)\\) are accurate.\n\n\nConclusion: The fifth-degree Chebyshev interpolating polynomial \\(Q_5(x)\\) approximates \\(e^x\\) with an error less than \\(0.00012\\), ensuring at least three correct decimal digits across \\([-1, 1]\\)."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#applications",
    "href": "notes/w03/chebyshev-interpolation.html#applications",
    "title": "Chebyshev Interpolation",
    "section": "Applications",
    "text": "Applications\nChebyshev Interpolation is widely used in various fields due to its robustness and efficiency:\n\nNumerical Integration: Chebyshev polynomials are used in Gaussian quadrature methods for approximating integrals.\nApproximation Theory: Provides optimal polynomial approximations for continuous functions.\nSignal Processing: Used in filter design and spectral analysis.\nComputer Graphics: Facilitates curve and surface modeling with minimal errors.\nScientific Computing: Enhances the accuracy of simulations and numerical solutions to differential equations."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#conclusion",
    "href": "notes/w03/chebyshev-interpolation.html#conclusion",
    "title": "Chebyshev Interpolation",
    "section": "Conclusion",
    "text": "Conclusion\nChebyshev Interpolation offers a reliable method for polynomial approximation, leveraging Chebyshev nodes to minimize interpolation errors and avoid common pitfalls like Runge’s phenomenon. Its mathematical foundations, combined with practical computational techniques, make it an essential tool in numerical analysis and various applied disciplines."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#references",
    "href": "notes/w03/chebyshev-interpolation.html#references",
    "title": "Chebyshev Interpolation",
    "section": "References",
    "text": "References\n\nTrefethen, L. N. (2000). Approximation Theory and Approximation Practice. SIAM.\nPress, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (2007). Numerical Recipes: The Art of Scientific Computing. Cambridge University Press.\nOlver, F. W. J., & Townsend, C. T. (2013). A First Course in Chebyshev and Fourier Spectral Methods. Cambridge University Press."
  },
  {
    "objectID": "notes/w02/lagrange-interpolation.html",
    "href": "notes/w02/lagrange-interpolation.html",
    "title": "Lagrange Interpolation",
    "section": "",
    "text": "Lagrange Interpolation is a method of constructing a polynomial that passes through a given set of points. It is particularly useful when you have a small number of data points and want to determine the polynomial function that exactly fits those points."
  },
  {
    "objectID": "notes/w02/lagrange-interpolation.html#the-lagrange-interpolating-polynomial",
    "href": "notes/w02/lagrange-interpolation.html#the-lagrange-interpolating-polynomial",
    "title": "Lagrange Interpolation",
    "section": "The Lagrange Interpolating Polynomial",
    "text": "The Lagrange Interpolating Polynomial\nGiven \\(n\\) distinct data points \\((x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\), the Lagrange interpolating polynomial is the polynomial \\(P(x)\\) of degree at most \\(n-1\\) that passes through all the points, meaning:\n\\[\nP(x_i) = y_i \\quad \\text{for each} \\ i = 1, 2, ..., n\n\\]\nThe Lagrange form of the polynomial is given by:\n\\[\nP(x) = \\sum_{i=1}^{n} y_i L_i(x)\n\\]\nwhere \\(L_i(x)\\) are the Lagrange basis polynomials, defined as:\n\\[\nL_i(x) = \\prod_{j=1, j \\neq i}^{n} \\frac{x - x_j}{x_i - x_j}\n\\]\nHere, the product is taken over all \\(j \\neq i\\), ensuring that \\(L_i(x_j) = 0\\) for \\(j \\neq i\\) and \\(L_i(x_i) = 1\\). This ensures that \\(P(x_i) = y_i\\) for each \\(i\\).\n\nStep-by-Step Calculation\nSuppose we are given a set of three points: \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\((x_3, y_3)\\). The Lagrange interpolating polynomial is:\n\\[\nP(x) = y_1 L_1(x) + y_2 L_2(x) + y_3 L_3(x)\n\\]\nwhere the Lagrange basis polynomials are:\n\\[\nL_1(x) = \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)}\n\\]\n\\[\nL_2(x) = \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)}\n\\]\n\\[\nL_3(x) = \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\n\\]\nBy evaluating these expressions and plugging in the values of \\(y_1\\), \\(y_2\\), and \\(y_3\\), we obtain the polynomial \\(P(x)\\) that passes through all the given points.\n\n\nExample\nLet’s go through an example where we are given three points: \\((1, 2)\\), \\((2, 3)\\), and \\((3, 5)\\).\n\nPoints:\n\\((x_1, y_1) = (1, 2)\\)\n\\((x_2, y_2) = (2, 3)\\)\n\\((x_3, y_3) = (3, 5)\\)\nLagrange basis polynomials:\n\n\\[\nL_1(x) = \\frac{(x - 2)(x - 3)}{(1 - 2)(1 - 3)} = \\frac{(x - 2)(x - 3)}{2}\n\\]\n\\[\nL_2(x) = \\frac{(x - 1)(x - 3)}{(2 - 1)(2 - 3)} = \\frac{(x - 1)(x - 3)}{-1}\n\\]\n\\[\nL_3(x) = \\frac{(x - 1)(x - 2)}{(3 - 1)(3 - 2)} = \\frac{(x - 1)(x - 2)}{2}\n\\]\n\nLagrange interpolating polynomial:\n\n\\[\nP(x) = 2 \\cdot L_1(x) + 3 \\cdot L_2(x) + 5 \\cdot L_3(x)\n\\]\nSubstituting the values for \\(L_1(x)\\), \\(L_2(x)\\), and \\(L_3(x)\\):\n\\[\nP(x) = 2 \\cdot \\frac{(x - 2)(x - 3)}{2} + 3 \\cdot \\frac{(x - 1)(x - 3)}{-1} + 5 \\cdot \\frac{(x - 1)(x - 2)}{2}\n\\]\nSimplifying this expression will give you the final polynomial \\(P(x)\\) that passes through all three points.\n\n\nGeneral Properties of Lagrange Interpolation\n\nUniqueness: There is exactly one polynomial of degree \\(n-1\\) that interpolates \\(n\\) points. This is guaranteed by the fundamental theorem of algebra, which states that a polynomial of degree \\(n-1\\) is uniquely determined by \\(n\\) distinct points.\nEfficiency: Lagrange interpolation is not the most computationally efficient method for large datasets, because each term depends on all the data points, making the calculation costly for large \\(n\\). Methods like Newton’s divided differences are generally preferred for interpolation with larger datasets.\nAccuracy: Interpolation works well if the points are well-distributed and the function is smooth. However, for unevenly spaced points or functions with high curvature, the interpolation polynomial may oscillate significantly, a phenomenon known as Runge’s phenomenon.\n\n\n\nApplications of Lagrange Interpolation\n\nCurve Fitting: Lagrange interpolation can be used to construct a polynomial that exactly fits a given set of data points.\nNumerical Integration: The interpolating polynomial can be used to approximate integrals through methods such as Newton-Cotes formulas.\nGraphics and Animation: In computer graphics, Lagrange interpolation is used to smoothly interpolate between keyframes in animations.\nSignal Processing: It is used in digital signal processing for reconstructing missing samples from a set of known data points.\n\n\n\nDrawbacks\n\nRunge’s Phenomenon: Lagrange interpolation can lead to significant oscillation, especially when interpolating over large intervals with a high degree polynomial.\nNot Easily Updateable: If a new point is added, the entire Lagrange polynomial must be recalculated. In contrast, methods like Newton’s divided differences allow for easier updates when new points are added.\n\n\n\nConclusion\nLagrange interpolation is a powerful tool for constructing polynomials that pass through a set of points, but it can suffer from inefficiencies and oscillations for large datasets. It’s important to understand its benefits and limitations to use it effectively in applications."
  },
  {
    "objectID": "notes/w01/newtons-method.html",
    "href": "notes/w01/newtons-method.html",
    "title": "Newton’s Method",
    "section": "",
    "text": "Newton’s Method, also known as the Newton-Raphson Method, is a widely used numerical method for finding successively better approximations to the roots (or zeros) of a real-valued function. It is particularly efficient when the initial guess is close to the actual root and when the function is well-behaved (smooth and differentiable)."
  },
  {
    "objectID": "notes/w01/newtons-method.html#the-newtons-method-formula",
    "href": "notes/w01/newtons-method.html#the-newtons-method-formula",
    "title": "Newton’s Method",
    "section": "The Newton’s Method Formula",
    "text": "The Newton’s Method Formula\nNewton’s Method is based on using the tangent line at an approximation of the root to generate a better approximation. The formula for generating the next approximation \\(x_{k+1}\\) from the current approximation \\(x_k\\) is given by:\n\\[\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n\\]\nwhere:\n\n\\(f(x)\\) is the function whose root we are trying to find.\n\\(f'(x)\\) is the derivative of \\(f(x)\\).\n\\(x_k\\) is the current approximation, and \\(x_{k+1}\\) is the next approximation.\n\n\nGeometrical Interpretation\nNewton’s Method can be interpreted geometrically: given an approximation \\(x_k\\), the tangent line to the curve \\(y = f(x)\\) at the point $ (x*k, f(x_k)) $ is used to estimate where the curve crosses the x-axis, which provides the next approximation \\(x*{k+1}\\).\n\n\nConvergence Criteria\nNewton’s Method converges quadratically under certain conditions, which means that the number of correct digits roughly doubles with each iteration. However, this fast convergence occurs only if:\n\nThe function \\(f(x)\\) is continuous and differentiable in the vicinity of the root.\nThe derivative \\(f'(x)\\) is non-zero at the root.\nThe initial guess is sufficiently close to the actual root.\n\nIf the initial guess is too far from the root, Newton’s Method may fail to converge or may converge very slowly.\n\n\nStep-by-Step Procedure\n\nInitial Guess: Choose an initial approximation \\(x_0\\).\nIteration Formula: Compute successive approximations using the formula:\n\n\\[\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n\\]\n\nRepeat: Continue iterating until \\(|x_{k+1} - x_k| &lt; \\epsilon\\), where \\(\\epsilon\\) is a small tolerance value, or until \\(|f(x_k)| &lt; \\epsilon\\).\n\n\n\nExample\nLet’s solve the equation \\(f(x) = x^2 - 2 = 0\\) using Newton’s Method, which has a root at \\(x = \\sqrt{2}\\).\n\nFunction and Derivative:\n\\[\nf(x) = x^2 - 2\n\\]\n\\[\nf'(x) = 2x\n\\]\nInitial Guess: Let \\(x_0 = 1.5\\).\nFirst Iteration:\n\\[\nx_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} = 1.5 - \\frac{1.5^2 - 2}{2(1.5)} = 1.4167\n\\]\nSecond Iteration:\n\\[\nx_2 = x_1 - \\frac{f(x_1)}{f'(x_1)} = 1.4167 - \\frac{1.4167^2 - 2}{2(1.4167)} = 1.4142\n\\]\nFurther Iterations: Repeat until the difference between successive approximations is less than a specified tolerance (e.g., \\(\\epsilon = 10^{-5}\\)).\n\nIn this case, after just a few iterations, we have a highly accurate approximation of \\(\\sqrt{2}\\).\n\n\nGeneral Properties of Newton’s Method\n\nQuadratic Convergence: When close to the root, Newton’s Method converges quadratically, meaning that the error decreases roughly as the square of the previous error.\nRequires Derivatives: Unlike the Bisection Method, Newton’s Method requires that the derivative \\(f'(x)\\) be known and be non-zero at the root.\nSensitive to Initial Guess: The method is sensitive to the initial guess, and poor choices of \\(x_0\\) can lead to divergence or slow convergence.\n\n\n\nApplications of Newton’s Method\n\nRoot Finding: Newton’s Method is widely used to find roots of non-linear equations in mathematics, physics, engineering, and economics.\nOptimization: Newton’s Method is the basis of Newton’s optimization method, which is used to find local minima or maxima of differentiable functions by solving \\(f'(x) = 0\\).\nEngineering and Modeling: It is used to solve non-linear models and systems, especially in fields like structural engineering, fluid dynamics, and electrical circuit analysis.\n\n\n\nAdvantages of Newton’s Method\n\nFast Convergence: When it converges, Newton’s Method is extremely fast due to its quadratic convergence rate.\nSimple Iterative Formula: The iteration formula is straightforward and easy to implement.\nFew Iterations: For well-behaved functions and good initial guesses, only a few iterations are required to obtain a highly accurate result.\n\n\n\nLimitations of Newton’s Method\n\nRequires Derivatives: The method requires that \\(f(x)\\) is differentiable, and that the derivative \\(f'(x)\\) can be computed analytically or numerically.\nRisk of Divergence: If the initial guess is too far from the root or if \\(f'(x)\\) is zero or near zero, the method may diverge or fail to converge.\nSlow or No Convergence: For functions with inflection points or flat regions near the root, the method may converge very slowly or not at all. In these cases, alternative methods like the Secant Method or Bisection Method may be better suited.\n\n\n\nConclusion\nNewton’s Method is a powerful and efficient tool for finding roots of equations, especially when the initial guess is close to the solution. While it requires the calculation of derivatives, its fast convergence makes it a preferred method when applicable. However, care must be taken with the choice of initial guess to avoid issues with divergence or slow convergence."
  },
  {
    "objectID": "notes/w01/bisection-method.html",
    "href": "notes/w01/bisection-method.html",
    "title": "Bisection Method",
    "section": "",
    "text": "Bisection Method is one of the simplest and most reliable numerical methods for finding a root of a continuous function \\(f(x) = 0\\) over a closed interval \\([a, b]\\). The method works by repeatedly bisecting the interval and then selecting the subinterval in which the function changes sign, ensuring that a root lies within that subinterval."
  },
  {
    "objectID": "notes/w01/bisection-method.html#the-bisection-method-formula",
    "href": "notes/w01/bisection-method.html#the-bisection-method-formula",
    "title": "Bisection Method",
    "section": "The Bisection Method Formula",
    "text": "The Bisection Method Formula\nThe Bisection Method requires that the function \\(f(x)\\) be continuous over the interval \\([a, b]\\), and that the function has opposite signs at the endpoints \\(a\\) and \\(b\\), i.e., \\(f(a) \\cdot f(b) &lt; 0\\). This guarantees that there is at least one root in the interval by the Intermediate Value Theorem.\nThe basic idea of the method is to repeatedly bisect the interval and check the sign of \\(f(x)\\) at the midpoint to determine the subinterval containing the root.\n\nAlgorithm\n\nInitial Guess: Choose an interval \\([a_0, b_0]\\) such that \\(f(a_0) \\cdot f(b_0) &lt; 0\\).\nMidpoint Calculation: Compute the midpoint \\(c_k = \\frac{a_k + b_k}{2}\\) of the interval \\([a_k, b_k]\\).\nCheck the Sign: Evaluate \\(f(c_k)\\).\n\nIf \\(f(c_k) = 0\\), then \\(c_k\\) is the root.\nIf \\(f(a_k) \\cdot f(c_k) &lt; 0\\), set \\(b_{k+1} = c_k\\), and the root lies in \\([a_k, c_k]\\).\nIf \\(f(c_k) \\cdot f(b_k) &lt; 0\\), set \\(a_{k+1} = c_k\\), and the root lies in \\([c_k, b_k]\\).\n\nRepeat: Continue bisecting the interval until the length of the interval is smaller than a specified tolerance \\(\\epsilon\\), or until \\(|f(c_k)| &lt; \\epsilon\\).\n\nThe approximate root will be:\n\\[\nx^* = \\frac{a_k + b_k}{2}\n\\]\n\n\nConvergence\nThe Bisection Method converges linearly. The length of the interval halves at each iteration, ensuring that the method always converges to a solution (if one exists) within the interval.\nThe number of iterations \\(n\\) required to achieve an accuracy of \\(\\epsilon\\) can be estimated by:\n\\[\nn \\geq \\frac{\\log \\left( \\frac{b_0 - a_0}{\\epsilon} \\right)}{\\log 2}\n\\]\n\n\nExample\nLet’s solve the equation \\(f(x) = x^3 - x - 2 = 0\\) in the interval \\([1, 2]\\).\n\nInitial Interval:\n\\(f(1) = 1^3 - 1 - 2 = -2\\)\n\\(f(2) = 2^3 - 2 - 2 = 4\\)\nSince \\(f(1) \\cdot f(2) &lt; 0\\), there is a root in \\([1, 2]\\).\nFirst Iteration:\nMidpoint: \\(c_1 = \\frac{1 + 2}{2} = 1.5\\)\n\\(f(1.5) = 1.5^3 - 1.5 - 2 = -0.125\\)\nSince \\(f(1) \\cdot f(1.5) &lt; 0\\), the root lies in \\([1, 1.5]\\).\nSecond Iteration:\nMidpoint: \\(c_2 = \\frac{1 + 1.5}{2} = 1.25\\)\n\\(f(1.25) = 1.25^3 - 1.25 - 2 = -1.796875\\)\nSince \\(f(1) \\cdot f(1.25) &lt; 0\\), the root lies in \\([1, 1.25]\\).\nFurther Iterations:\nRepeat the process until the interval width is smaller than the desired tolerance \\(\\epsilon\\).\n\n\n\nGeneral Properties of the Bisection Method\n\nGuaranteed Convergence: The method is guaranteed to converge to a root if \\(f(a) \\cdot f(b) &lt; 0\\) and \\(f(x)\\) is continuous on \\([a, b]\\).\nRate of Convergence: The Bisection Method has linear convergence, meaning the error decreases by a constant factor with each iteration. This makes the method slower than other methods like Newton’s Method, but much more reliable.\nRobustness: The method is very robust as it does not require the derivative of the function and is insensitive to the initial guesses, provided the condition \\(f(a) \\cdot f(b) &lt; 0\\) holds.\n\n\n\nApplications of the Bisection Method\n\nRoot Finding: The Bisection Method is used in various fields, including physics, engineering, and mathematics, to find roots of non-linear equations.\nModeling and Simulation: It is used when precise solutions are needed and the function is known to be continuous over the interval.\nInitial Root Estimates: The Bisection Method is often used to find a good initial approximation for more efficient methods like Newton’s Method or the Secant Method.\n\n\n\nAdvantages of the Bisection Method\n\nGuaranteed Convergence: The method always converges if the initial interval contains a root.\nNo Derivatives Needed: Unlike Newton’s Method, the Bisection Method does not require the computation of derivatives.\nSimplicity: The method is easy to understand and implement.\n\n\n\nLimitations of the Bisection Method\n\nSlow Convergence: The method converges linearly, which makes it slower compared to methods like Newton’s Method, which has quadratic convergence.\nOnly One Root: The Bisection Method only finds one root in the interval. If multiple roots exist, it cannot find them all without running the method on different intervals.\nInitial Interval Requirement: The method requires an initial interval \\([a, b]\\) where the function changes sign, which may not always be easy to determine.\n\n\n\nConclusion\nThe Bisection Method is a reliable and simple method for finding roots of continuous functions, especially when no derivative information is available. While slower than other root-finding algorithms, its guaranteed convergence and robustness make it a valuable tool in numerical analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FALL 2024 - SCHEDULE",
    "section": "",
    "text": "Week\n\n\nMonday\n\n\nTuesday\n\n\nWednesday\n\n\nThursday\n\n\nFriday\n\n\n\n\n1\n\n\n16Chapter 1\n\n\n17\n\n\n18Chapter 1\n\n\n19\n\n\n20Chapter 1\n\n\n\n\n2\n\n\n23Add/DropChapter 1\n\n\n24\n\n\n25Chapter 1Quiz 1\n\n\n26\n\n\n27Chapter 3\n\n\n\n\n3\n\n\n30Chapter 3\n\n\n1\n\n\n2Chapter 3Quiz 2\n\n\n3\n\n\n4Chapter 3\n\n\n\n\n4\n\n\n7Chapter 3Reality Check 1\n\n\n8No W drop date\n\n\n9Chapter 5Quiz 3\n\n\n10\n\n\n11Chapter 5\n\n\n\n\n5\n\n\n14Chapter 5\n\n\n15\n\n\n16Chapter 5Quiz 4\n\n\n17\n\n\n18Chapter 5\n\n\n\n\n6\n\n\n21Exam 1 ReviewExam 1-2 Opens\n\n\n22\n\n\n23Exam 1-1In Class\n\n\n24Exam 1-2 Closes\n\n\n25Chapter 2\n\n\n\n\n7\n\n\n28Chapter 2\n\n\n29\n\n\n30Chapter 2\n\n\n31\n\n\n1Chapter 2\n\n\n\n\n8\n\n\n4Chapter 2\n\n\n5\n\n\n6Chapter 4Quiz 5\n\n\n7\n\n\n8Chapter 4\n\n\n\n\n9\n\n\n11Drop w/W dateChapter 42nd Reality Check\n\n\n12\n\n\n13Chapter 4Quiz 6\n\n\n14\n\n\n15Chapter 4\n\n\n\n\n10\n\n\n18Chapter 4\n\n\n19\n\n\n20Chapter 4\n\n\n21\n\n\n22Exam 2 ReviewExam 2-2 Opens\n\n\n\n\n11\n\n\n25Exam 2-1In Class\n\n\n26Exam 2-2 Closes\n\n\n27No Classes\n\n\n28Holiday\n\n\n29Holiday\n\n\n\n\n12\n\n\n2Chapter 10\n\n\n3Discontinuance\n\n\n4Chapter 10\n\n\n5\n\n\n6Chapter 10Quiz 7 (?)\n\n\n\n\n13\n\n\n9Chapter 10\n\n\n10\n\n\n11Chapter 10\n\n\n12\n\n\n13Chapter 10Exam 3-2 Opens\n\n\n\n\n14\n\n\n16Last Day of Class\n\n\n1710:30 Exam 3-1Exam 3-2 Closes3rd Reality Check\n\n\n18\n\n\n19Commencement\n\n\n20"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html",
    "href": "homework/w09/exercise4-3-1d.html",
    "title": "Exercise 4.3.1d",
    "section": "",
    "text": "Apply classical Gram–Schmidt orthogonalization to find the full QR factorization of the matrix:\n\n\\mathbf{A} = \\begin{bmatrix} 4 & 8 & 1 \\\\ 0 & 2 & -2 \\\\ 3 & 6 & 7 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#define-the-columns-of-matrix-mathbfa",
    "href": "homework/w09/exercise4-3-1d.html#define-the-columns-of-matrix-mathbfa",
    "title": "Exercise 4.3.1d",
    "section": "Define the Columns of Matrix \\mathbf{A}",
    "text": "Define the Columns of Matrix \\mathbf{A}\nRepresent the columns of \\mathbf{A} as vectors:\n\n\\mathbf{a}_1 = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix}, \\quad \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#finding-mathbfq_1",
    "href": "homework/w09/exercise4-3-1d.html#finding-mathbfq_1",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_1",
    "text": "Finding \\mathbf{q}_1\nTo find the first orthonormal vector \\mathbf{q}_1, normalize \\mathbf{a}_1.\n\nCalculate the norm of \\mathbf{a}_1:\n\n\\|\\mathbf{a}_1\\| = \\sqrt{4^2 + 0^2 + 3^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\n\nNormalize \\mathbf{a}_1:\n\n\\mathbf{q}_1 = \\frac{\\mathbf{a}_1}{\\|\\mathbf{a}_1\\|} = \\frac{1}{5} \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#finding-mathbfq_2",
    "href": "homework/w09/exercise4-3-1d.html#finding-mathbfq_2",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_2",
    "text": "Finding \\mathbf{q}_2\nTo find \\mathbf{q}_2, project \\mathbf{a}_2 onto \\mathbf{q}_1 and then subtract this projection from \\mathbf{a}_2.\n\nCalculate the projection of \\mathbf{a}_2 onto \\mathbf{q}_1:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\left( \\mathbf{a}_2 \\cdot \\mathbf{q}_1 \\right) \\mathbf{q}_1\n\nSince \\mathbf{q}_1 is a unit vector, \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\nCompute \\mathbf{a}_2 \\cdot \\mathbf{q}_1:\n\n\\mathbf{a}_2 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{32}{5} + 0 + \\frac{18}{5} = \\frac{50}{5} = 10\n\nCalculate \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = 10 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix}\n\nCalculate \\mathbf{u}_2:\nSubtracting the projection from \\mathbf{a}_2 gives \\mathbf{u}_2:\n\n\\mathbf{u}_2 = \\mathbf{a}_2 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix}\n\nNormalize \\mathbf{u}_2 to obtain \\mathbf{q}_2:\nThe norm of \\mathbf{u}_2 is:\n\n\\|\\mathbf{u}_2\\| = \\sqrt{0^2 + 2^2 + 0^2} = 2\n\nThus,\n\n\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{\\|\\mathbf{u}_2\\|} = \\frac{1}{2} \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#finding-mathbfq_3",
    "href": "homework/w09/exercise4-3-1d.html#finding-mathbfq_3",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_3",
    "text": "Finding \\mathbf{q}_3\nTo find \\mathbf{q}_3, project \\mathbf{a}_3 onto both \\mathbf{q}_1 and \\mathbf{q}_2, and then subtract these projections from \\mathbf{a}_3.\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_1:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{4}{5} + 0 + \\frac{21}{5} = \\frac{25}{5} = 5\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 = 5 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_2:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_2 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = -2\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = -2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix}\n\nCalculate \\mathbf{u}_3:\nSubtracting both projections from \\mathbf{a}_3 yields \\mathbf{u}_3:\n\n\\mathbf{u}_3 = \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} - \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix}\n\nNormalize \\mathbf{u}_3 to obtain \\mathbf{q}_3:\nThe norm of \\mathbf{u}_3 is:\n\n\\|\\mathbf{u}_3\\| = \\sqrt{(-3)^2 + 0^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\nThus,\n\n\\mathbf{q}_3 = \\frac{\\mathbf{u}_3}{\\|\\mathbf{u}_3\\|} = \\frac{1}{5} \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ 0 \\\\ \\frac{4}{5} \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "href": "homework/w09/exercise4-3-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "title": "Exercise 4.3.1d",
    "section": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices",
    "text": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices\n\nOrthogonal Matrix \\mathbf{Q}\nCombine \\mathbf{q}_1, \\mathbf{q}\\_2, and \\mathbf{q}_3 as columns to form \\mathbf{Q}:\n\n\\mathbf{Q} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix}\n\n\n\nUpper Triangular Matrix \\mathbf{R}\nThe entries of \\mathbf{R} are calculated as the dot products of the original columns of \\mathbf{A} with the orthonormal vectors \\mathbf{q}_1, \\mathbf{q}_2, and \\mathbf{q}_3:\n\n\\mathbf{R} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{q}_1 & \\mathbf{a}_2 \\cdot \\mathbf{q}_1 & \\mathbf{a}_3 \\cdot \\mathbf{q}_1 \\\\ 0 & \\mathbf{a}_2 \\cdot \\mathbf{q}_2 & \\mathbf{a}_3 \\cdot \\mathbf{q}_2 \\\\ 0 & 0 & \\mathbf{a}_3 \\cdot \\mathbf{q}_3 \\end{bmatrix} = \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}\n\n\n\nFinal Answer\nThe QR factorization of \\mathbf{A} is:\n\n\\mathbf{A} = \\mathbf{Q} \\mathbf{R} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix} \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html",
    "href": "homework/w07/exercise2-4-4a.html",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "",
    "text": "Solve the system by finding the \\(PA = LU\\) factorization and then carrying out the two-step back substitution:\n\\[\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 4 & 4 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html#lu-factorization-with-partial-pivoting",
    "href": "homework/w07/exercise2-4-4a.html#lu-factorization-with-partial-pivoting",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "LU Factorization with Partial Pivoting",
    "text": "LU Factorization with Partial Pivoting\nStep 1: First Column\n\nPivot Selection: Both \\(a_{11} = 4\\) and \\(a_{21} = 4\\) are tied for the largest absolute value. Choose \\(a_{11}\\) as the pivot (no row swap needed).\nCompute Multipliers and Eliminate Below Pivot:\n\nRow 2:\n\\[\nL_{21} = \\dfrac{a_{21}}{a_{11}} = \\dfrac{4}{4} = 1\n\\]\n\\[\n\\text{Row 2} \\rightarrow \\text{Row 2} - L_{21} \\times \\text{Row 1}\n\\]\n\\[\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix}\n\\]\nRow 3: \\[\nL_{31} = \\dfrac{a_{31}}{a_{11}} = \\dfrac{2}{4} = \\dfrac{1}{2}\n\\] \\[\n\\text{Row 3} \\rightarrow \\text{Row 3} - L_{31} \\times \\text{Row 1}\n\\] \\[\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 0 & 1 & 3 \\end{pmatrix}\n\\]\n\n\nStep 2: Second Column\n\nPivot Selection: \\(U_{22} = 2\\) is the largest absolute value below the pivot (no row swap needed).\nCompute Multiplier and Eliminate Below Pivot:\n\nRow 3: \\[\nL_{32} = \\dfrac{U_{32}}{U_{22}} = \\dfrac{1}{2}\n\\] \\[\n\\text{Row 3} \\rightarrow \\text{Row 3} - L_{32} \\times \\text{Row 2}\n\\] \\[\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n\\]\n\n\nResulting Matrices\n\nLower Triangular Matrix \\(L\\):\n\\[\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\dfrac{1}{2} & \\dfrac{1}{2} & 1\n\\end{pmatrix}\n\\]\nUpper Triangular Matrix \\(U\\):\n\\[\nU = \\begin{pmatrix}\n4 & 2 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\]\nPermutation Matrix \\(P\\): \\[\nP = I \\quad (\\text{identity matrix, since no row swaps were performed})\n\\]"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html#forward-substitution-solve-ly-b",
    "href": "homework/w07/exercise2-4-4a.html#forward-substitution-solve-ly-b",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "Forward Substitution: Solve \\(Ly = b\\)",
    "text": "Forward Substitution: Solve \\(Ly = b\\)\n\\[\nL \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}\n\\]\n\nEquation 1:\n\\[\ny_1 = 2\n\\]\nEquation 2:\n\\[\ny_2 = b_2 - L_{21} y_1 = 4 - (1)(2) = 2\n\\]\nEquation 3: \\[\ny_3 = b_3 - L_{31} y_1 - L_{32} y_2 = 6 - \\left( \\dfrac{1}{2} \\times 2 \\right) - \\left( \\dfrac{1}{2} \\times 2 \\right) = 6 - 1 - 1 = 4\n\\]\n\nSolution:\n\\[\ny = \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html#back-substitution-solve-ux-y",
    "href": "homework/w07/exercise2-4-4a.html#back-substitution-solve-ux-y",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "Back Substitution: Solve \\(Ux = y\\)",
    "text": "Back Substitution: Solve \\(Ux = y\\)\n\\[\nU \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix}\n\\]\n\nEquation 3:\n\\[\n2 x_3 = y_3 \\implies x_3 = \\dfrac{y_3}{2} = \\dfrac{4}{2} = 2\n\\]\nEquation 2:\n\\[\n2 x_2 + 2 x_3 = y_2 \\implies x_2 = \\dfrac{y_2 - 2 x_3}{2} = \\dfrac{2 - (2 \\times 2)}{2} = \\dfrac{-2}{2} = -1\n\\]\nEquation 1: \\[\n4 x_1 + 2 x_2 = y_1 \\implies x_1 = \\dfrac{y_1 - 2 x_2}{4} = \\dfrac{2 - (2 \\times -1)}{4} = \\dfrac{2 + 2}{4} = \\dfrac{4}{4} = 1\n\\]\n\nSolution:\n\\[\nx = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\]\nFinal Answer\nThe solution to the system is:\n\\[\nx_1 = 1, \\quad x_2 = -1, \\quad x_3 = 2\n\\]"
  },
  {
    "objectID": "homework/w03/exercise3-3-3.html",
    "href": "homework/w03/exercise3-3-3.html",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "",
    "text": "Assume that Chebyshev interpolation is used to find a fifth-degree interpolating polynomial Q_5(x) on the interval [-1, 1] for the function f(x) = e^x. Use the interpolation error formula to find a worst-case estimate for the error |e^x - Q_5(x)| that is valid for x throughout the interval [-1, 1]. How many digits after the decimal point will be correct when Q_5(x) is used to approximate e^x?\n\n\n\nWe need to compute the worst-case error for the interpolation of f(x) = e^x using a Chebyshev interpolating polynomial Q_5(x). We will use the interpolation error formula:\n\n|f(x) - P(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\max_{x \\in [-1,1]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|\n\nWhere M is an upper bound on the 6th derivative of f(x) = e^x over [-1, 1], and x_1, x_2, \\dots, x_n are the Chebyshev nodes.\n\n\n\n\nChebyshev Node Bound: For Chebyshev interpolation on the interval [-1, 1], the product (x - x_1)(x - x_2) \\cdots (x - x_n) is bounded by \\frac{1}{2^n}. For n = 5, this becomes:\n\n\\frac{1}{2^5} = \\frac{1}{32}\n\nSixth Derivative of e^x: The 6th derivative of f(x) = e^x is f^{(6)}(x) = e^x, and the maximum value of this derivative on the interval [-1, 1] is at x = 1, where f^{(6)}(1) = e \\approx 2.718.\nFactorial Term: The term 6! = 720.\nError Bound Formula: Now, we plug these values into the error bound formula:\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{32}\n\nSubstituting e \\approx 2.718, we get:\n\n|e^x - Q_5(x)| \\leq \\frac{2.718}{720 \\times 32} \\approx \\frac{2.718}{23,040} \\approx 0.000118\n\nThis is the worst-case error estimate for the approximation of e^x on the interval [-1, 1].\nCorrect Decimal Places: Since the error bound is approximately 0.000118, this means we can expect approximately 3 correct digits after the decimal point. The error affects the fourth decimal place, but the first three digits are expected to be correct.\nTherefore, the approximation Q_5(x) will be accurate to 3 digits after the decimal point when approximating e^x on the interval [-1, 1]."
  },
  {
    "objectID": "homework/w03/exercise3-3-3.html#question",
    "href": "homework/w03/exercise3-3-3.html#question",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "",
    "text": "Assume that Chebyshev interpolation is used to find a fifth-degree interpolating polynomial Q_5(x) on the interval [-1, 1] for the function f(x) = e^x. Use the interpolation error formula to find a worst-case estimate for the error |e^x - Q_5(x)| that is valid for x throughout the interval [-1, 1]. How many digits after the decimal point will be correct when Q_5(x) is used to approximate e^x?\n\n\n\nWe need to compute the worst-case error for the interpolation of f(x) = e^x using a Chebyshev interpolating polynomial Q_5(x). We will use the interpolation error formula:\n\n|f(x) - P(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\max_{x \\in [-1,1]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|\n\nWhere M is an upper bound on the 6th derivative of f(x) = e^x over [-1, 1], and x_1, x_2, \\dots, x_n are the Chebyshev nodes.\n\n\n\n\nChebyshev Node Bound: For Chebyshev interpolation on the interval [-1, 1], the product (x - x_1)(x - x_2) \\cdots (x - x_n) is bounded by \\frac{1}{2^n}. For n = 5, this becomes:\n\n\\frac{1}{2^5} = \\frac{1}{32}\n\nSixth Derivative of e^x: The 6th derivative of f(x) = e^x is f^{(6)}(x) = e^x, and the maximum value of this derivative on the interval [-1, 1] is at x = 1, where f^{(6)}(1) = e \\approx 2.718.\nFactorial Term: The term 6! = 720.\nError Bound Formula: Now, we plug these values into the error bound formula:\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{32}\n\nSubstituting e \\approx 2.718, we get:\n\n|e^x - Q_5(x)| \\leq \\frac{2.718}{720 \\times 32} \\approx \\frac{2.718}{23,040} \\approx 0.000118\n\nThis is the worst-case error estimate for the approximation of e^x on the interval [-1, 1].\nCorrect Decimal Places: Since the error bound is approximately 0.000118, this means we can expect approximately 3 correct digits after the decimal point. The error affects the fourth decimal place, but the first three digits are expected to be correct.\nTherefore, the approximation Q_5(x) will be accurate to 3 digits after the decimal point when approximating e^x on the interval [-1, 1]."
  },
  {
    "objectID": "homework/w03/exercise3-2-2.html",
    "href": "homework/w03/exercise3-2-2.html",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "",
    "text": "Question\n\n\nGiven the data points (1, 0), (2, \\ln 2), and (4, \\ln 4), find the degree 2 interpolating polynomial.\n\nUse the result of (a) to approximate \\ln 3.\n\nUse Theorem 3.3 to give an error bound for the approximation in part (b).\n\nCompare the actual error to your error bound.\n\n\n\nKey Concepts\n\n\nError in Polynomial Interpolation:\nThe error between the actual function f(x) and the interpolated polynomial P_{n-1}(x) is given by the following bound:\n\n|f(x) - P_{n-1}(x)| \\leq \\left| \\frac{(x - x_0)(x - x_1) \\dots (x - x_n)}{n!} f^{(n)}(c) \\right|\n\nwhere:\n\nn is the number of interpolation points.\nx_0, x_1, \\dots, x_n are the known data points.\nf^{(n)}(c) is the n-th derivative of the actual function f(x), evaluated at some point c in the interval [x_0, x_n].\n\n\n\n\nSolution\n\n\n(a) Finding the Degree 2 Interpolating Polynomial\nGiven the points (1, 0), (2, \\ln 2), and (4, \\ln 4), we apply the Lagrange interpolation formula.\n\nLagrange Basis Polynomials:\n\nL_0(x):\n\nL_0(x) = \\frac{(x - 2)(x - 4)}{(1 - 2)(1 - 4)} = \\frac{(x - 2)(x - 4)}{3}\n\nL_1(x):\n\nL_1(x) = \\frac{(x - 1)(x - 4)}{(2 - 1)(2 - 4)} = -\\frac{(x - 1)(x - 4)}{2}\n\nL_2(x): \nL_2(x) = \\frac{(x - 1)(x - 2)}{(4 - 1)(4 - 2)} = \\frac{(x - 1)(x - 2)}{6}\n\n\n\n\nPolynomial Construction:\nNow, the interpolating polynomial becomes:\n\nP(x) = y_0 L_0(x) + y_1 L_1(x) + y_2 L_2(x)\n\nSince y_0 = 0, we have:\n\nP(x) = \\ln 2 \\cdot \\left(-\\frac{(x - 1)(x - 4)}{2}\\right) + \\ln 4 \\cdot \\frac{(x - 1)(x - 2)}{6}\n\nWe know that \\ln 4 = 2 \\ln 2, so the polynomial simplifies to:\n\nP(x) = \\ln 2 \\cdot \\left(-\\frac{(x - 1)(x - 4)}{2} + \\frac{(x - 1)(x - 2)}{3}\\right)\n\n\n\n\n(b) Approximation of \\ln 3\nTo approximate \\ln 3, substitute x = 3 into the polynomial:\n\nP(3) = \\ln 2 \\cdot \\left(-\\frac{(3 - 1)(3 - 4)}{2} + \\frac{(3 - 1)(3 - 2)}{3}\\right)\n\nSimplifying:\n\nP(3) = \\ln 2 \\cdot \\left(1 + \\frac{2}{3}\\right) = \\ln 2 \\cdot \\frac{5}{3}\n\nSince \\ln 2 \\approx 0.6931, we have:\n\nP(3) \\approx \\frac{5}{3} \\cdot 0.6931 \\approx 1.1552\n\nThus, the approximation for \\ln 3 is:\n\nP(3) \\approx 1.1552\n\n\n\n(c) Error Bound using Theorem 3.3\nThe error formula for degree 2 interpolation is:\n\n|f(x) - P(x)| \\leq \\left| \\frac{(x - x_0)(x - x_1)(x - x_2)}{3!} f^{(3)}(c) \\right|\n\nwhere f(x) = \\ln(x) and f^{(3)}(x) = \\frac{2}{x^3}. The maximum of f^{(3)}(x) occurs at x = 1, giving:\n\nf^{(3)}(1) = 2\n\nSubstituting into the error formula for x = 3:\n\nE(3) = \\frac{(3 - 1)(3 - 2)(3 - 4)}{6} \\cdot 2 = \\frac{(2)(1)(-1)}{6} \\cdot 2 = -\\frac{4}{6} = -\\frac{2}{3}\n\nTherefore, the error bound is approximately -\\frac{2}{3} \\approx -0.6667.\n\n\n(d) Comparison of Actual Error and Error Bound\nThe actual value of \\ln 3 is approximately:\n\n\\ln 3 \\approx 1.0986\n\nOur approximation was P(3) \\approx 1.1552. Therefore, the actual error is:\n\n\\text{Actual error} = |1.0986 - 1.1552| \\approx 0.0566\n\nThe error bound is approximately 0.6667, which is larger than the actual error. This confirms that the actual error is well within the error bound, as expected."
  },
  {
    "objectID": "homework/math411-suggested-homework.html",
    "href": "homework/math411-suggested-homework.html",
    "title": "MATH411 Suggested Homework - Fall 2024",
    "section": "",
    "text": "Week 1\n\n(C0-P1) Read/work through the Getting Started with Python notebook (see Modules &gt; Homework &gt; Getting_Started_with_Python.ipynb). Then complete the following exercises in a separate notebook or .py file:\n\nWhat is the difference between the outputs generated by the following two lines of code?\n\nnp.array([i for i in range(10)])\nnp.linspace(0, 9, 10, endpoint=True)\n\nUse both np.linspace() and np.arange() to create an array containing floating point numbers starting at 1.0, ending at 4.0, equally spaced with separation 0.2. In other words, the array should contain 1.0, 1.2, 1.4, …, 3.8, 4.0.\nCreate an array consisting of the floats 1.0, 2.0, 3.0, 4.0, and 5.0. Create a second array containing the square root of each of these numbers. Then, use a for loop to compute the sum of the squared differences between the two arrays:\n\n\n\\sum_{i=1}^n \\left(x_i - \\sqrt{x_i}\\right)^2\n\nExtra Challenge: Can you do this without a loop?\n\nStarting with x = 1, use a while loop to divide by 2 until x &lt; 10^{-4}. Display (print) the list 1.0, 0.5, 0.25, …, and report the number of divisions by 2 needed such that the (k-1)th division produces x &gt; 10^{-4} and the kth division produces x &lt; 10^{-4}.\nWrite code to create a function to compute f(x) = e^{-x} \\cos x, where x is a vector (array) of one or more numbers. Then evaluate f(x) at the points 0, 0.1, 0.2, …, 1.0.\nWrite code to plot the function h(x) = e^{x} \\cos^2 x - 2 on the interval -0.5 to 5.5 and visually estimate the roots of h(x) on that interval.\n\n(C1-P1) Exercise 1.1.4ab\n(C1-P2) Exercise 1.2.2\n(C1-P3) Computer Problem 1.2.2ab. For each equation, find an initial point x_0 and a function g(x) such that the fixed-point iteration x_{k+1} = g(x_k) converges to x, where g(x) = x. If this is not possible, explain why.\n(C1-P4) Exercise 1.2.14\n(C1-P5) Exercise 1.4.1\n(C1-P6) Exercise 1.4.3\n\n\nWeek 2\n\n(C1-P7) Exercise 1.4.6\n(C1-P8) Exercise 1.4.8\n(C1-P9) Computer Problem 1.4.7\n(C1-P10) Exercise 1.5.1\n(C1-P11) Use Python to compare results obtained using the Bisection Method, Newton’s Method, and the Secant Method to solve the equation \\ln x + x^2 = 3. Note that Python code for these methods is available on I-Learn.\n\nSolve the problem using each of the three methods. Report starting values and the number of iterations required to obtain 6 correct decimal places of accuracy. Hint: a graph of the function may help with starting values.\nOn the same axes, plot \\log(\\epsilon_{i+1}) vs. \\log(\\epsilon_i) for the three methods. Explain your plot. How is it related to the rate or order of convergence? Use the errors to determine if your results are consistent with theory. How would you compute the error if you didn’t have an exact value for the root?\n\n(C3-P1) Exercise 3.1.1ac\n(C3-P2) Exercise 3.1.2ac\n(C3-P3) Exercise 3.1.6\n\n\nWeek 3\n\n(C0-P2) Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array—choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n(C3-P4) Exercise 3.2.2\n(C3-P5) Exercise 3.2.5\n(C3-P6) Exercise 3.2.6 Note: the two additional points in the next-to-last sentence should be (x_7, y_7) = (0.1, f(0.1)) and (x_8, y_8) = (0.5, f(0.5)).\n(C3-P7) Computer Problem 3.1.3. To demonstrate that your function works, interpolate \\sin(x) on the interval [-\\pi, \\pi] using nodes -\\pi, -\\frac{\\pi}{2}, 0, \\frac{\\pi}{2}, \\pi. Plot your interpolating polynomial. Plot \\sin(x) on the same graph, and use the numpy functions polyfit and polyval to plot an interpolating polynomial from Python on the same graph. Use a legend to make clear which curve is yours and which one came from Python. Hint: the code on p. 146 should help. Python versions of newtdd and nest (p. 3) are available in Canvas.\n(C3-P8) Exercise 3.3.2ac\n(C3-P9) Exercise 3.3.2ac\n(C3-P10) Exercise 3.3.3\n\n\nWeek 4\n\n(C5-P1) Exercise 5.2.1ab\n(C5-P2) Exercise 5.2.2ab\n(C5-P3) Exercise 5.2.3ab\n(C5-P4) Exercise 5.2.10\n(C5-P5) Exercise 5.2.12\n(C5-P6) Computer Problem 5.2.1ac\n(C5-P7) Computer Problem 5.2.2de\n(C5-P8) Computer Problem 5.2.9bf\n\n\nWeek 5\n\n(C5-P9) Exercise 5.5.1ab\n(C5-P10) Exercise 5.5.4cd\n(C5-P11) Computer Problem 5.4.1acd\n(C5-P12) Computer Problem 5.4.2\n(C5-P13) Computer Problem 5.4.3acd\n(C5-P14) Exercise 5.5.5cd\n(C5-P15) Exercise 5.5.7\n\n\nWeek 6\n\n(C2-P1) Exercise 2.1.2ac\n(C2-P2) Computer Problem 2.1.2ac\n(C2-P3) Exercise 2.2.1ab\n(C2-P4) Exercise 2.2.2ab\n\n\nWeek 7\n\n(C2-P5) Exercise 2.2.4\n(C2-P6) Computer Problem 2.2.1ab\n(C2-P7) Exercise 2.4.1ab\n(C2-P8) Exercise 2.4.2ab\n(C2-P9) Exercise 2.4.4a\n(C2-P10) Exercise 2.4.6\n\n\nWeek 8\n\n(C2-P11) Exercise 2.5.2ab\n(C2-P12) Computer Problem 2.5.2 (solve using both Jacobi and Gauss-Seidel, compare results)\n(C4-P1) Exercise 4.1.2\n(C4-P2) Computer Problem 4.1.5 (also use a quadratic fit and compare)\n(C4-P3) Exercise 4.3.2\n\n\nWeek 9\n\n(C4-P4) Exercise 4.3.4 (use the matrix from Exercise 4.3.1d)\n(C4-P5) Exercise 4.3.7 (you can use your QR factorizations from Exercise 4.3.2)\n(C4-P6) Computer Problem 4.3.4 Additional instructions: Write a classical Gram-Schmidt code only. Use the matrices in Exercise 4.3.2 to check your code. If you use the code I provided, you must comment it (explain what every line does).\n\n\nWeek 10\n\n(C4-P7) Exercise 4.4.2\n(C4-P8) Exercise 4.4.3\n(C4-P9) Computer Problem 4.4.2 (find a preconditioned GMRES Python code and use it)\n\n\nWeek 11\n\nNone\nExam 2 in class on Monday Week 11\nAttempt the first several Week 12 problems before Monday Week 12\n\n\nWeek 12\n\n(C10-P1) Exercise 10.1.1ad (also, find the inverse DFT of your result, compare to the original vector)\n(C10-P2) Exercise 10.1.8\n(C10-P3) Exercise 10.2.1ab\n(C10-P4) Exercise 10.2.3ab\n(C10-P5) Exercise 10.2.3 (plot data and function to show your interpolating function does interpolate the data)\n(C10-P6) Computer Problem 10.2.4\n(C10-P7) Exercise 10.3.2ab\n\n\nWeek 13\n\n(C10-P8) Computer Problem 10.3.2cd\n(C10-P9) Exercise 10.3.5 (Complete the \\sum_{j=0}^{n-1} \\cos \\frac{2 \\pi j k}{n} \\cos \\frac{2 \\pi j l}{n} result only)\n(C10-P10) Exercise 10.1.6"
  },
  {
    "objectID": "homework/w03/exercise3-2-6.html",
    "href": "homework/w03/exercise3-2-6.html",
    "title": "MATH411",
    "section": "",
    "text": "w03/exercise3-2-5.qmd— title: “Exercise 3.1.6 (C3-P6)” subtitle: “MATH411” author: “Nathan Lunceford” format: html: self-contained: true page-layout: full title-block-banner: true toc: true toc-depth: 3 toc-location: body number-sections: false html-math-method: katex code-fold: true code-summary: “Show the code” code-overflow: wrap code-copy: hover code-tools: source: false toggle: true caption: See code"
  },
  {
    "objectID": "homework/w03/exercise3-2-6.html#question",
    "href": "homework/w03/exercise3-2-6.html#question",
    "title": "MATH411",
    "section": "Question",
    "text": "Question\nHow do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points \\((1, 1)\\), \\((2, 3)\\), \\((3, 3)\\), and \\((4, 4)\\), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\nKey Concepts\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree \\(n-1\\) exists for \\(n\\) distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\nSolution\nTo create a degree 5 polynomial that still passes through all four points \\((1, 1)\\), \\((2, 3)\\), \\((3, 3)\\), and \\((4, 4)\\), follow these steps:\n\nFind the degree 3 polynomial \\(P_3(x)\\):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial \\(P_3(x)\\).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\n\\[P_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\\]\n\nThe new term \\((x - 1)(x - 2)(x - 3)(x - 4)\\) is zero at \\(x = 1, 2, 3, 4\\), so it won’t affect the interpolation at those points. By multiplying this term by a constant \\(c\\), we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\nFinal Degree 5 Polynomial\nThus, the degree 5 polynomial is given by:\n\\[P_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\\]\n\n\\(P_3(x)\\) is the degree 3 polynomial found using standard interpolation methods.\n\\(c\\) is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\nExample\nLet’s assume the degree 3 polynomial \\(P_3(x)\\) through the points \\((1, 1)\\), \\((2, 3)\\), \\((3, 3)\\), \\((4, 4)\\) is:\n\\[P_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\\]\nThen the degree 5 polynomial becomes:\n\\[P_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\\]\n\n\nDoes the Value of \\(c\\) Matter?\nNo, the value of \\(c\\) does not affect the interpolation at the given points. Since the additional term evaluates to 0 at \\(x = 1, 2, 3, 4\\), the polynomial will still pass through the points, regardless of \\(c\\).\nHowever, changing \\(c\\) affects the behavior of the polynomial outside the interpolation points. For different values of \\(c\\), the polynomial will look different beyond the four points, but it will still pass through \\((1, 1)\\), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "homework/index.html",
    "href": "homework/index.html",
    "title": "HOMEWORK",
    "section": "",
    "text": "Suggested Homework\n\nWEEK 01\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C0-P1)\n(C1-P1) Exercise 1.1.4ab\n(C1-P2) Exercise 1.2.2\n(C1-P3)\n(C1-P4) Exercise 1.2.14\n(C1-P5) Exercise 1.4.1\n(C1-P6) Exercise 1.4.3\n\n\n\n\n\n\nWEEK 02\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C1-P7) Exercise 1.4.6\n(C1-P8) Exercise 1.4.8\n(C1-P9) Computer Problem 1.4.7\n(C1-P10) Exercise 1.5.1\n(C1-P11)\n(C3-P1) Exercise 3.1.1a\n(C3-P1) Exercise 3.1.1c\n(C3-P2) Exercise 3.1.2a\n(C3-P2) Exercise 3.1.2c\n(C3-P3) Exercise 3.1.6\n\n\n\n\n\n\nWEEK 03\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C0-P2)\n(C3-P4) Exercise 3.2.2\n(C3-P5) Exercise 3.2.5\n(C3-P6) Exercise 3.2.6\n(C3-P7) Computer Problem 3.1.3\n(C3-P8) Exercise 3.3.1ac\n(C3-P9) Exercise 3.3.2ac\n(C3-P10) Exercise 3.3.3\n\n\n\n\n\n\nWEEK 04\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C5-P1) Exercise 5.2.1a\n(C5-P1) Exercise 5.2.1b\n(C5-P2) Exercise 5.2.2ab\n(C5-P3) Exercise 5.2.3ab\n(C5-P4) Exercise 5.2.10\n(C5-P5) Exercise 5.2.12\n(C5-P6) Computer Problem 5.2.1ac\n(C5-P7) Computer Problem 5.2.2de\n(C5-P8) Computer Problem 5.2.9bf\n\n\n\n\n\n\nWEEK 05\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C5-P9) Exercise 5.5.1ab\n(C5-P10) Exercise 5.5.4cd\n(C5-P11) Computer Problem 5.4.1acd\n(C5-P12) Computer Problem 5.4.2\n(C5-P13) Computer Problem 5.4.3acd\n(C5-P14) Exercise 5.5.5cd\n(C5-P15) Exercise 5.5.7\n\n\n\n\n\n\nWEEK 06\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C2-P1) Exercise 2.1.2a\n(C2-P1) Exercise 2.1.2c\n(C2-P2) Computer Problem 2.1.2ac\n(C2-P3) Exercise 2.2.1ab\n(C2-P4) Exercise 2.2.2ab\n\n\n\n\n\n\nWEEK 07\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C2-P5) Exercise 2.2.4\n(C2-P6) Computer Problem 2.2.1ab\n(C2-P7) Exercise 2.4.1ab\n(C2-P8) Exercise 2.4.2ab\n(C2-P9) Exercise 2.4.4a\n(C2-P10) Exercise 2.4.6\n\n\n\n\n\n\nWEEK 08\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C2-P11) Exercise 2.5.2ab\n(C2-P12) Computer Problem 2.5.2 (solve using both Jacobi and Gauss-Seidel, compare results)\n(C4-P1) Exercise 4.1.2\n(C4-P2) Computer Problem 4.1.5 (also use a quadratic fit and compare)\n(C4-P3) Exercise 4.3.2\n\n\n\n\n\n\nWEEK 09\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nExercise 4.3.1d\n(C4-P4) Exercise 4.3.4\n(C4-P5) Exercise 4.3.7 (you can use your QR factorizations from Exercise 4.3.2)\n(C4-P6) Computer Problem 4.3.4 Additional instructions: Write a classical Gram-Schmidt code only. Use the matrices in Exercise 4.3.2 to check your code. If you use the code I provided, you must comment it (explain what every line does).\n\n\n\n\n\n\nWEEK 10\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C4-P7) Exercise 4.4.2\n(C4-P8) Exercise 4.4.3\n(C4-P9) Computer Problem 4.4.2 (find a preconditioned GMRES Python code and use it)\n\n\n\n\n\n\nWEEK 11\n\n\n\n\n\n\n‎\n\n\n\n\n\nNone\n\n\n\n\n\nWEEK 12\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C10-P1) Exercise 10.1.1ad (also, find the inverse DFT of your result, compare to the original vector)\n(C10-P2) Exercise 10.1.8\n(C10-P3) Exercise 10.2.1ab\n(C10-P4) Exercise 10.2.3ab\n(C10-P5) Exercise 10.2.3 (plot data and function to show your interpolating function does interpolate the data)\n(C10-P6) Computer Problem 10.2.4\n(C10-P7) Exercise 10.3.2ab\n\n\n\n\n\n\nWEEK 13\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n(C10-P8) Computer Problem 10.3.2cd\n(C10-P9) Exercise 10.3.5 (Complete the \\sum\\_{j=0}^{n-1} \\cos \\frac{2 \\pi j k}{n} \\cos \\frac{2 \\pi j l}{n} result only)\n(C10-P10) Exercise 10.1.6"
  },
  {
    "objectID": "homework/w02/exercise3-1-6.html",
    "href": "homework/w02/exercise3-1-6.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\n\nWe will use Newton’s divided differences to first construct the degree 3 polynomial, and then extend it to a degree 5 polynomial.\n\n\n\nWe will use the given points (x_1, y_1) = (1, 1), (x_2, y_2) = (2, 3), (x_3, y_3) = (3, 3), and (x_4, y_4) = (4, 4) to compute the divided differences and build the polynomial.\n\n\n\n\n\n\n\n\n\n\nx\nf(x)\nFirst Difference f[x_i, x_{i+1}]\nSecond Difference f[x_i, x_{i+1}, x_{i+2}]\nThird Difference f[x_i, x_{i+1}, x_{i+2}, x_{i+3}]\n\n\n\n\n1\n1\n\n\n\n\n\n2\n3\n\\frac{3 - 1}{2 - 1} = 2\n\n\n\n\n3\n3\n\\frac{3 - 3}{3 - 2} = 0\n\\frac{0 - 2}{3 - 1} = -1\n\n\n\n4\n4\n\\frac{4 - 3}{4 - 3} = 1\n\\frac{1 - 0}{4 - 2} = \\frac{1}{2}\n\\frac{\\frac{1}{2} - (-1)}{4 - 1} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\nThe Newton’s divided difference form of the interpolating polynomial is:\n\nP_3(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + f[x_1, x_2, x_3, x_4](x - x_1)(x - x_2)(x - x_3)\n\nSubstituting the values from the divided difference table:\n\nP_3(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3)\n\n\n\n\nTo create a degree 5 polynomial, we add an extra term c(x - 1)(x - 2)(x - 3)(x - 4), which evaluates to zero at the given points:\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nSubstitute P_3(x):\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\n\nThe value of c does not affect the polynomial at the given points. The additional term evaluates to 0 at x = 1, 2, 3, 4, so no matter what c is, the polynomial will pass through the points (1, 1), (2, 3), (3, 3), (4, 4).\nHowever, changing c will affect the polynomial’s behavior outside the given points. For different values of c, the polynomial will look different outside the interpolation points.\n\n\n\n\nThus, the degree 5 polynomial is:\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThis polynomial passes through the points (1, 1), (2, 3), (3, 3), (4, 4), and c is an arbitrary constant that influences how the polynomial behaves outside of those points."
  },
  {
    "objectID": "homework/w02/exercise3-1-6.html#question",
    "href": "homework/w02/exercise3-1-6.html#question",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\n\nWe will use Newton’s divided differences to first construct the degree 3 polynomial, and then extend it to a degree 5 polynomial.\n\n\n\nWe will use the given points (x_1, y_1) = (1, 1), (x_2, y_2) = (2, 3), (x_3, y_3) = (3, 3), and (x_4, y_4) = (4, 4) to compute the divided differences and build the polynomial.\n\n\n\n\n\n\n\n\n\n\nx\nf(x)\nFirst Difference f[x_i, x_{i+1}]\nSecond Difference f[x_i, x_{i+1}, x_{i+2}]\nThird Difference f[x_i, x_{i+1}, x_{i+2}, x_{i+3}]\n\n\n\n\n1\n1\n\n\n\n\n\n2\n3\n\\frac{3 - 1}{2 - 1} = 2\n\n\n\n\n3\n3\n\\frac{3 - 3}{3 - 2} = 0\n\\frac{0 - 2}{3 - 1} = -1\n\n\n\n4\n4\n\\frac{4 - 3}{4 - 3} = 1\n\\frac{1 - 0}{4 - 2} = \\frac{1}{2}\n\\frac{\\frac{1}{2} - (-1)}{4 - 1} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\nThe Newton’s divided difference form of the interpolating polynomial is:\n\nP_3(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + f[x_1, x_2, x_3, x_4](x - x_1)(x - x_2)(x - x_3)\n\nSubstituting the values from the divided difference table:\n\nP_3(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3)\n\n\n\n\nTo create a degree 5 polynomial, we add an extra term c(x - 1)(x - 2)(x - 3)(x - 4), which evaluates to zero at the given points:\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nSubstitute P_3(x):\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\n\nThe value of c does not affect the polynomial at the given points. The additional term evaluates to 0 at x = 1, 2, 3, 4, so no matter what c is, the polynomial will pass through the points (1, 1), (2, 3), (3, 3), (4, 4).\nHowever, changing c will affect the polynomial’s behavior outside the given points. For different values of c, the polynomial will look different outside the interpolation points.\n\n\n\n\nThus, the degree 5 polynomial is:\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThis polynomial passes through the points (1, 1), (2, 3), (3, 3), (4, 4), and c is an arbitrary constant that influences how the polynomial behaves outside of those points."
  },
  {
    "objectID": "homework/w03/exercise3-2-5.html",
    "href": "homework/w03/exercise3-2-5.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\n\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\n\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "homework/w03/exercise3-2-5.html#question",
    "href": "homework/w03/exercise3-2-5.html#question",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\n\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\n\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "homework/w04/exercise5-2-1a.html",
    "href": "homework/w04/exercise5-2-1a.html",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "",
    "text": "Problem\n\nApply the composite Trapezoid Rule with m = 1, 2, and 4 panels to approximate the integral. Compute the error by comparing with the exact value from calculus.\n\n\\int_0^1 x^2 \\, dx\n\n\n\nKey Concepts\n\n\nComposite Trapezoidal Rule\nThe formula for the composite trapezoidal rule is:\n\nT_m = \\frac{h}{2} \\left( f(a) + 2 \\sum_{i=1}^{m-1} f(x_i) + f(b) \\right)\n\nwhere:\n\nh = \\frac{b - a}{m}\nx_i = a + i \\cdot h for i = 1, 2, \\ldots, m-1\n\n\n\n\nSolution\n\n\n1. Exact Value of the Integral\nWe first compute the exact value of the integral:\n\n\\int_0^1 x^2 \\, dx = \\left[ \\frac{x^3}{3} \\right]_0^1 = \\frac{1^3}{3} - \\frac{0^3}{3} = \\frac{1}{3} \\approx 0.3333\n\n\n\n\n2. Approximations Using the Composite Trapezoidal Rule\n\nCase m = 1:\n\nh = \\frac{1 - 0}{1} = 1\nPoints: x_0 = 0, x_1 = 1\nApproximation:\n\n\nT_1 = \\frac{1}{2} \\left( f(0) + f(1) \\right) = \\frac{1}{2} \\left( 0^2 + 1^2 \\right) = \\frac{1}{2} \\cdot 1 = 0.5\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.5 \\right| = 0.1667\n\n\n\n\nCase m = 2:\n\nh = \\frac{1 - 0}{2} = 0.5\nPoints: x_0 = 0, x_1 = 0.5, x_2 = 1\nApproximation:\n\n\nT_2 = \\frac{0.5}{2} \\left( f(0) + 2 \\cdot f(0.5) + f(1) \\right) = \\frac{0.5}{2} \\left( 0^2 + 2 \\cdot 0.5^2 + 1^2 \\right)\n\n\nT_2 = \\frac{0.5}{2} \\cdot (0 + 2 \\cdot 0.25 + 1) = \\frac{0.5}{2} \\cdot 1.5 = 0.375\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.375 \\right| = 0.0417\n\n\n\n\nCase m = 4:\n\nh = \\frac{1 - 0}{4} = 0.25\nPoints: x_0 = 0, x_1 = 0.25, x_2 = 0.5, x_3 = 0.75, x_4 = 1\nApproximation:\n\n\nT_4 = \\frac{0.25}{2} \\left( f(0) + 2 \\cdot \\left( f(0.25) + f(0.5) + f(0.75) \\right) + f(1) \\right)\n\n\nT_4 = \\frac{0.25}{2} \\left( 0^2 + 2 \\cdot (0.25^2 + 0.5^2 + 0.75^2) + 1^2 \\right)\n\n\nT_4 = \\frac{0.25}{2} \\cdot (0 + 2 \\cdot (0.0625 + 0.25 + 0.5625) + 1) = \\frac{0.25}{2} \\cdot 2.75 = 0.34375\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.34375 \\right| = 0.0104\n\n\n\n\n\n3. Summary of Results\n\n\n\nm\nApproximation\nError\n\n\n\n\n1\n0.5000\n0.1667\n\n\n2\n0.3750\n0.0417\n\n\n4\n0.3438\n0.0104\n\n\n\n\n\n\n4. Conclusion\nAs the number of panels m increases, the approximation becomes more accurate, and the error decreases. This demonstrates that the composite trapezoidal rule converges to the exact value as the number of panels increases."
  },
  {
    "objectID": "homework/w06/exercise2-1-2a.html",
    "href": "homework/w06/exercise2-1-2a.html",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "",
    "text": "Problem:\n\n\n\n\n\n\n2.1.2a\n\n\n\nSolve the following system of equations using Gaussian elimination:\n\n\\begin{aligned}\n2x - 2y - z &= -2, \\\\\n4x + y - 2z &= 1, \\\\\n-2x + y - z &= -3.\n\\end{aligned}\n\n\n\n\n\nSolution:\n\nRepresenting the System in Augmented Matrix Form\nThe system can be written in augmented matrix form as:\n\n\\begin{bmatrix}\n2 & -2 & -1 & -2 \\\\\n4 & 1 & -2 & 1 \\\\\n-2 & 1 & -1 & -3\n\\end{bmatrix}.\n\n\n\nRow Operations for Gaussian Elimination\n\nNormalize Row 1\nDivide Row 1 by 2:\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n4 & 1 & -2 & 1 \\\\\n-2 & 1 & -1 & -3\n\\end{bmatrix}.\n\n\n\nEliminate the First Column in Rows 2 and 3\n\nR_2 \\to R_2 - 4 \\cdot R_1\nR_3 \\to R_3 + 2 \\cdot R_1\n\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n0 & 5 & -\\frac{6}{2} & 5 \\\\\n0 & -1 & -2 & -5\n\\end{bmatrix}.\n\n\n\nNormalize Row 2\nDivide Row 2 by 5:\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & -1 & -2 & -5\n\\end{bmatrix}.\n\n\n\nEliminate the Second Column in Rows 1 and 3\n\nR_1 \\to R_1 + R_2\nR_3 \\to R_3 + R_2\n\n\n\\begin{bmatrix}\n1 & 0 & -\\frac{7}{10} & 0 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & 0 & -\\frac{13}{5} & -4\n\\end{bmatrix}.\n\n\n\nNormalize Row 3\nDivide Row 3 by -\\frac{13}{5}:\n\n\\begin{bmatrix}\n1 & 0 & -\\frac{7}{10} & 0 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & 0 & 1 & \\frac{20}{13}\n\\end{bmatrix}.\n\n\n\nBack Substitution\n\nUpdate R_2: R_2 \\to R_2 + \\frac{3}{5} \\cdot R_3\nUpdate R_1: R_1 \\to R_1 + \\frac{7}{10} \\cdot R_3\n\n\n\\begin{bmatrix}\n1 & 0 & 0 & \\frac{14}{13} \\\\\n0 & 1 & 0 & \\frac{19}{13} \\\\\n0 & 0 & 1 & \\frac{20}{13}\n\\end{bmatrix}.\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\n\nx = \\frac{14}{13}, \\quad y = \\frac{19}{13}, \\quad z = \\frac{20}{13}."
  },
  {
    "objectID": "homework/w07/exercise2-4-6.html",
    "href": "homework/w07/exercise2-4-6.html",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "",
    "text": "(a) Write down the \\(4 \\times 4\\) matrix \\(P\\) such that multiplying a matrix on the left by \\(P\\) causes the second and fourth rows of the matrix to be exchanged.\n(b) What is the effect of multiplying on the right by \\(P\\)? Demonstrate with an example."
  },
  {
    "objectID": "homework/w07/exercise2-4-6.html#a-constructing-the-permutation-matrix-p",
    "href": "homework/w07/exercise2-4-6.html#a-constructing-the-permutation-matrix-p",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "(a) Constructing the Permutation Matrix \\(P\\)",
    "text": "(a) Constructing the Permutation Matrix \\(P\\)\nTo create a permutation matrix \\(P\\) that exchanges the second and fourth rows when multiplied on the left, start with the \\(4 \\times 4\\) identity matrix \\(I_4\\) and swap its second and fourth rows.\nIdentity Matrix \\(I_4\\):\n\\[\nI_4 = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\]\nPermutation Matrix \\(P\\) (after swapping rows 2 and 4):\n\\[\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\\]\nWhen \\(P\\) multiplies any \\(4 \\times n\\) matrix \\(A\\) on the left (\\(PA\\)), it exchanges the second and fourth rows of \\(A\\)."
  },
  {
    "objectID": "homework/w07/exercise2-4-6.html#b-effect-of-multiplying-on-the-right-by-p",
    "href": "homework/w07/exercise2-4-6.html#b-effect-of-multiplying-on-the-right-by-p",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "(b) Effect of Multiplying on the Right by \\(P\\)",
    "text": "(b) Effect of Multiplying on the Right by \\(P\\)\nMultiplying a matrix on the right by \\(P\\) rearranges its columns (instead of rows) according to the pattern defined by \\(P\\). In this case, it specifically swaps the matrix’s second and fourth columns.\nDemonstration with an Example:\nLet’s consider a \\(4 \\times 4\\) matrix \\(A\\):\n\\[\nA = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44} \\\\\n\\end{pmatrix}\n\\]\nCompute \\(AP\\):\n\\[\nAP = A \\times P = A \\times \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\\]\nResulting Matrix \\(AP\\):\n\\[\nAP = \\begin{pmatrix}\na_{11} & a_{14} & a_{13} & a_{12} \\\\\na_{21} & a_{24} & a_{23} & a_{22} \\\\\na_{31} & a_{34} & a_{33} & a_{32} \\\\\na_{41} & a_{44} & a_{43} & a_{42} \\\\\n\\end{pmatrix}\n\\]\nNumerical Example:\n\\[\nA = \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12 \\\\\n13 & 14 & 15 & 16 \\\\\n\\end{pmatrix}\n\\]\nCompute \\(AP\\):\n\\[\nAP = A \\times \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 4 & 3 & 2 \\\\\n5 & 8 & 7 & 6 \\\\\n9 & 12 & 11 & 10 \\\\\n13 & 16 & 15 & 14 \\\\\n\\end{pmatrix}\n\\]\nFinal Answer\n(a) The \\(4 \\times 4\\) permutation matrix \\(P\\) that exchanges the second and fourth rows when multiplied on the left is:\n\\[\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\\]\n(b) Multiplying on the right by \\(P\\) exchanges the second and fourth columns of a matrix. For example, for the matrix:\n\\[\nA = \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12 \\\\\n13 & 14 & 15 & 16 \\\\\n\\end{pmatrix}\n\\]\nMultiplying on the right by \\(P\\) yields:\n\\[\nAP = \\begin{pmatrix}\n1 & 4 & 3 & 2 \\\\\n5 & 8 & 7 & 6 \\\\\n9 & 12 & 11 & 10 \\\\\n13 & 16 & 15 & 14 \\\\\n\\end{pmatrix}\n\\]\nwhich is \\(A\\) with its second and fourth columns exchanged."
  },
  {
    "objectID": "homework/w09/exercise4-3-4.html",
    "href": "homework/w09/exercise4-3-4.html",
    "title": "Exercise 4.3.4 (C4-P4)",
    "section": "",
    "text": "Problem\n\n\nSolution"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "NOTES",
    "section": "",
    "text": "WEEK 01\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nBisection Method\nFixed-Point Iteration\nNewtons’s Method\nSecant Method\n\n\n\n\n\n\nWEEK 02\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nLagrange Interpolation\nNewton’s Divided Differences\n\n\n\n\n\n\nWEEK 03\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nInterpolation Error Formula\nRunge Phenomenon\nChebyshev Interpolation\n\n\n\n\n\n\nWEEK 04\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nTrapezoidal Rule\n\n\n\n\n\n\nWEEK 05\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n\n\n\n\n\nWEEK 06\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n\n\n\n\n\nWEEK 07\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nChapter 2 Section 3 Notes\nNorms\nLinear System Error Analysis\nLU Factorization\n\n\n\n\n\n\nWEEK 08\n\n\n\n\n\n\n‎\n\n\n\n\n\n\nGram-Schmidt Orthogonalization\n\n\n\n\n\n\nWEEK 09\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n\n\n\n\n\nWEEK 10\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n\n\n\n\n\nWEEK 11\n\n\n\n\n\n\n‎\n\n\n\n\n\n\n\n\n\n\n\nWEEK 12\n\n\n\n\n\n\n‎"
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html",
    "href": "notes/w01/fixed-point-iteration.html",
    "title": "Fixed-Point Iteration",
    "section": "",
    "text": "Fixed-Point Iteration is a simple numerical method used to solve equations of the form \\(x = g(x)\\). The method is based on the idea that a root of the equation \\(f(x) = 0\\) can be found by rewriting it as \\(x = g(x)\\), and then iterating the function \\(g(x)\\) until the sequence of values converges to a solution."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#the-fixed-point-iteration-formula",
    "href": "notes/w01/fixed-point-iteration.html#the-fixed-point-iteration-formula",
    "title": "Fixed-Point Iteration",
    "section": "The Fixed-Point Iteration Formula",
    "text": "The Fixed-Point Iteration Formula\nThe fixed-point iteration method involves the recursive formula:\n\\[\nx_{k+1} = g(x_k)\n\\]\nHere, \\(x_k\\) is the \\(k\\)-th approximation of the root, and \\(g(x)\\) is a function chosen such that the equation \\(x = g(x)\\) has the same solution as \\(f(x) = 0\\). The sequence \\(x_k\\) is expected to converge to the fixed point \\(x^*\\), where \\(x^* = g(x^*)\\).\n\nConvergence Criteria\nFor fixed-point iteration to converge, certain conditions must be met:\n\nThe function \\(g(x)\\) must be continuous in the interval around the fixed point.\nThe derivative \\(g'(x)\\) must satisfy the following condition for convergence at the fixed point \\(x^*\\):\n\n\\[\n|g'(x^*)| &lt; 1\n\\]\nIf \\(|g'(x^*)| \\geq 1\\), the iteration may not converge.\n\n\nStep-by-Step Procedure\n\nChoose an initial guess \\(x_0\\).\nApply the recursive formula:\n\n\\[\nx_{k+1} = g(x_k)\n\\]\n\nRepeat the iteration until the difference between successive approximations is sufficiently small (i.e., \\(|x_{k+1} - x_k| &lt; \\epsilon\\), where \\(\\epsilon\\) is a small tolerance value).\nThe value of \\(x_k\\) is the approximate solution to the equation.\n\n\n\nExample\nLet’s go through an example where we solve the equation \\(x^2 - 2 = 0\\), which has a solution at \\(x = \\sqrt{2}\\). We can rewrite this equation in the form \\(x = g(x)\\) as:\n\\[\ng(x) = \\frac{2}{x}\n\\]\n\nInitial guess: Let \\(x_0 = 1.5\\).\nFirst iteration: Using the recursive formula \\(x_{k+1} = g(x_k)\\):\n\n\\[\nx_1 = g(x_0) = \\frac{2}{1.5} = 1.3333\n\\]\n\nSecond iteration:\n\n\\[\nx_2 = g(x_1) = \\frac{2}{1.3333} = 1.5\n\\]\n\nThird iteration:\n\n\\[\nx_3 = g(x_2) = \\frac{2}{1.5} = 1.3333\n\\]\nIn this case, the iterations quickly begin to oscillate around the solution. Further iterations will converge to \\(\\sqrt{2}\\), depending on the tolerance value.\n\n\nConvergence and Stability\nThe method converges when the derivative \\(|g'(x^*)| &lt; 1\\), ensuring that the successive values get closer to the actual solution. If \\(|g'(x)| &gt; 1\\) near the fixed point, the iterations may diverge, oscillate, or converge very slowly.\n\n\nAcceleration Techniques\n\nAitken’s \\(\\Delta^2\\)-Process: This method accelerates convergence by extrapolating the sequence of approximations.\nRelaxation Methods: Modifying the iterative formula to include a relaxation parameter \\(\\lambda\\), such as in under-relaxation and over-relaxation, to speed up convergence:\n\n\\[\nx_{k+1} = (1 - \\lambda)x_k + \\lambda g(x_k)\n\\]\nWhere \\(\\lambda \\in (0,1)\\) for under-relaxation and \\(\\lambda &gt; 1\\) for over-relaxation.\n\n\nApplications of Fixed-Point Iteration\n\nRoot-Finding: It is commonly used to find roots of non-linear equations.\nDynamical Systems: Fixed-point iterations are used to model and analyze the behavior of dynamical systems that stabilize at an equilibrium point.\nOptimization: In optimization problems, fixed-point methods can help solve systems of equations that arise from optimization constraints.\n\n\n\nAdvantages of Fixed-Point Iteration\n\nSimple Implementation: The method is easy to implement and requires only basic operations.\nApplicable to Various Problems: It can be applied to a wide range of equations and is useful in many scientific and engineering applications.\n\n\n\nLimitations of Fixed-Point Iteration\n\nSlow Convergence: Fixed-point iteration may converge slowly, especially if the function \\(g(x)\\) is poorly chosen.\nSensitive to Initial Guess: The success and speed of convergence depend heavily on the choice of the initial guess. A poor initial guess can lead to divergence.\nNot Always Convergent: If \\(|g'(x)| \\geq 1\\), the method may not converge to a solution, and alternative methods like Newton’s Method or Bisection Method may be preferred.\n\n\n\nConclusion\nFixed-point iteration is a fundamental and simple numerical method for finding roots of equations. However, it is essential to ensure the proper choice of function \\(g(x)\\) and initial guess to ensure convergence. While it is not always the fastest method, its simplicity makes it a good starting point for solving non-linear equations."
  },
  {
    "objectID": "notes/w01/secant-method.html",
    "href": "notes/w01/secant-method.html",
    "title": "Secant Method",
    "section": "",
    "text": "The Secant Method is a numerical method for finding roots of a nonlinear equation \\(f(x) = 0\\). It is similar to Newton’s Method, but it does not require the computation of the derivative \\(f'(x)\\). Instead, the Secant Method approximates the derivative using a secant line through two points on the function."
  },
  {
    "objectID": "notes/w01/secant-method.html#the-secant-method-formula",
    "href": "notes/w01/secant-method.html#the-secant-method-formula",
    "title": "Secant Method",
    "section": "The Secant Method Formula",
    "text": "The Secant Method Formula\nGiven two initial approximations \\(x_{k-1}\\) and \\(x_k\\), the next approximation \\(x_{k+1}\\) is computed using the secant line through these points. The formula is:\n\\[\nx_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}\n\\]\nThis equation is derived by approximating the derivative \\(f'(x_k)\\) using the difference quotient:\n\\[\nf'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n\\]\nThe main advantage of the Secant Method is that it avoids the need to compute the derivative \\(f'(x)\\), making it useful for functions where the derivative is difficult to compute or does not exist.\n\nAlgorithm\n\nInitial Guesses: Start with two initial approximations \\(x_0\\) and \\(x_1\\).\nIteration Formula: Compute successive approximations using the formula:\n\n\\[\nx_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}\n\\]\n\nRepeat: Continue iterating until the difference between successive approximations is less than a specified tolerance \\(\\epsilon\\), or until \\(|f(x_k)| &lt; \\epsilon\\).\n\n\n\nConvergence\nThe Secant Method typically converges faster than the Bisection Method but slower than Newton’s Method. It has a convergence rate of approximately \\(1.618\\), known as superlinear convergence. This is faster than the linear convergence of the Bisection Method, but slower than the quadratic convergence of Newton’s Method.\n\n\nExample\nLet’s solve the equation \\(f(x) = x^2 - 4 = 0\\) using the Secant Method, which has roots at \\(x = \\pm 2\\).\n\nInitial Guesses: Let \\(x_0 = 3\\) and \\(x_1 = 2.5\\).\nFirst Iteration:\n\\[\nx_2 = x_1 - \\frac{f(x_1)(x_1 - x_0)}{f(x_1) - f(x_0)} = 2.5 - \\frac{(2.5^2 - 4)(2.5 - 3)}{(2.5^2 - 4) - (3^2 - 4)} = 2.05\n\\]\nSecond Iteration:\n\\[\nx_3 = x_2 - \\frac{f(x_2)(x_2 - x_1)}{f(x_2) - f(x_1)} = 2.05 - \\frac{(2.05^2 - 4)(2.05 - 2.5)}{(2.05^2 - 4) - (2.5^2 - 4)} \\approx 2.0006\n\\]\nFurther Iterations: Continue until the difference between successive approximations is less than a specified tolerance (e.g., \\(\\epsilon = 10^{-5}\\)).\n\nIn this case, after just two iterations, we are already very close to the root \\(x = 2\\).\n\n\nGeneral Properties of the Secant Method\n\nNo Derivatives Needed: Unlike Newton’s Method, the Secant Method does not require the computation of the derivative \\(f'(x)\\), making it useful for functions that are not differentiable or where computing the derivative is expensive.\nSuperlinear Convergence: The Secant Method converges faster than the Bisection Method but slower than Newton’s Method. Its convergence rate is superlinear with a rate of approximately \\(1.618\\).\nRequires Two Initial Guesses: The method requires two initial approximations, \\(x_0\\) and \\(x_1\\), unlike Newton’s Method, which only needs one initial guess.\n\n\n\nApplications of the Secant Method\n\nRoot Finding: The Secant Method is widely used to find roots of non-linear equations, especially in cases where the derivative is not available or is costly to compute.\nOptimization: It can be used in optimization problems where the objective is to minimize or maximize a function without requiring the calculation of the derivative.\n\n\n\nAdvantages of the Secant Method\n\nNo Derivatives: The method does not require the calculation of \\(f'(x)\\), making it easier to apply in situations where the derivative is not known.\nFaster than Bisection: The Secant Method generally converges more quickly than the Bisection Method, especially when the initial guesses are close to the root.\n\n\n\nLimitations of the Secant Method\n\nSlower than Newton’s Method: While it converges faster than the Bisection Method, the Secant Method typically converges more slowly than Newton’s Method, which has quadratic convergence.\nConvergence is Not Guaranteed: The Secant Method does not always converge, especially if the initial guesses are not close to the actual root. If \\(f(x_k) = f(x_{k-1})\\), the method will fail due to division by zero.\nRequires Good Initial Guesses: Poor choices for the initial approximations \\(x_0\\) and \\(x_1\\) can result in slow convergence or failure to converge.\n\n\n\nConclusion\nThe Secant Method provides a good balance between speed and ease of use, especially when derivatives are difficult or costly to compute. It is faster than the Bisection Method but not as fast as Newton’s Method when derivatives are available. Careful selection of initial guesses is important for ensuring successful convergence."
  },
  {
    "objectID": "notes/w02/newtons-divided-differences.html",
    "href": "notes/w02/newtons-divided-differences.html",
    "title": "Newton’s Divided Differences",
    "section": "",
    "text": "Newton’s Divided Differences is an efficient method for computing an interpolating polynomial for a given set of data points. This method builds the polynomial iteratively and offers better efficiency for incremental data points compared to Lagrange interpolation."
  },
  {
    "objectID": "notes/w02/newtons-divided-differences.html#the-newton-divided-difference-formula",
    "href": "notes/w02/newtons-divided-differences.html#the-newton-divided-difference-formula",
    "title": "Newton’s Divided Differences",
    "section": "The Newton Divided Difference Formula",
    "text": "The Newton Divided Difference Formula\nGiven \\(n\\) data points \\((x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\), the Newton divided difference interpolating polynomial \\(P(x)\\) can be expressed as:\n\\[\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + \\cdots + f[x_1, x_2, \\ldots, x_n](x - x_1)(x - x_2)\\cdots(x - x_{n-1})\n\\]\nWhere \\(f[x_i, x_j, ..., x_k]\\) are the divided differences and are recursively defined as follows:\n\\[\nf[x_i] = y_i\n\\]\n\\[\nf[x_i, x_j] = \\frac{f[x_j] - f[x_i]}{x_j - x_i}\n\\]\n\\[\nf[x_i, x_j, x_k] = \\frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}\n\\]\nAnd so on for higher orders of divided differences.\n\nRecursive Formula for Divided Differences\nThe divided differences are computed recursively. For the first-order difference between two points, the formula is:\n\\[\nf[x_i, x_{i+1}] = \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}\n\\]\nFor the second-order difference between three points:\n\\[\nf[x_i, x_{i+1}, x_{i+2}] = \\frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i}\n\\]\nThis recursive approach continues for higher orders of differences.\n\n\nStep-by-Step Construction of the Newton Polynomial\n\nStart with the first point \\((x_1, y_1)\\), where \\(f[x_1] = y_1\\).\nFirst-order divided difference between \\((x_1, y_1)\\) and \\((x_2, y_2)\\) is:\n\n\\[\nf[x_1, x_2] = \\frac{y_2 - y_1}{x_2 - x_1}\n\\]\n\nSecond-order divided difference between \\((x_1, y_1)\\), \\((x_2, y_2)\\), and \\((x_3, y_3)\\):\n\n\\[\nf[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1}\n\\]\n\nContinue this process for higher-order divided differences.\n\nHere’s the corrected example in the same format as your original:\n\n\n\nExample\nLet’s consider the data points ( (1, 1) ), ( (2, 4) ), ( (3, 9) ), and ( (4, 16) ). The goal is to find the interpolating polynomial using Newton’s divided differences.\n\nFirst point:\n\n\\[\nf[1] = 1\n\\]\n\nFirst-order divided differences:\n\n\\[\nf[1, 2] = \\frac{4 - 1}{2 - 1} = 3\n\\]\n\\[\nf[2, 3] = \\frac{9 - 4}{3 - 2} = 5\n\\]\n\\[\nf[3, 4] = \\frac{16 - 9}{4 - 3} = 7\n\\]\n\nSecond-order divided differences:\n\n\\[\nf[1, 2, 3] = \\frac{f[2, 3] - f[1, 2]}{3 - 1} = \\frac{5 - 3}{2} = 1\n\\]\n\\[\nf[2, 3, 4] = \\frac{f[3, 4] - f[2, 3]}{4 - 2} = \\frac{7 - 5}{2} = 1\n\\]\n\nThird-order divided difference:\n\n\\[\nf[1, 2, 3, 4] = \\frac{f[2, 3, 4] - f[1, 2, 3]}{4 - 1} = \\frac{1 - 1}{3} = 0\n\\]\nNow, the Newton polynomial can be written as:\n\\[\nP(x) = 1 + 3(x - 1) + 1(x - 1)(x - 2) + 0(x - 1)(x - 2)(x - 3)\n\\]\nSimplifying:\n\\[\nP(x) = 1 + 3(x - 1) + (x - 1)(x - 2)\n\\]\nExpanding the terms:\n\\[\nP(x) = 1 + 3x - 3 + (x^2 - 3x + 2)\n\\]\nSimplifying further:\n\\[\nP(x) = x^2\n\\]\n\n\nGeneral Properties of Newton’s Divided Differences\n\nEfficiency: Newton’s divided differences offer better computational efficiency when adding new points to the data set compared to Lagrange interpolation because earlier divided differences can be reused.\nUniqueness: The Newton polynomial is unique, meaning for a given set of \\(n\\) distinct points, there is exactly one polynomial of degree \\(n-1\\) that interpolates the points.\nIterative Construction: The method allows for iterative construction, which is useful when dealing with real-time updates or adding new data points.\n\n\n\nApplications of Newton’s Divided Differences\n\nPolynomial Interpolation: Newton’s divided differences are commonly used to find an interpolating polynomial for a given set of data points.\nNumerical Differentiation: The method is used to approximate derivatives of functions when analytical differentiation is not feasible.\nCurve Fitting: It is used in applications requiring curve fitting, especially in scientific computing and data analysis.\n\n\n\nAdvantages of Newton’s Divided Differences\n\nEfficient for Incremental Data: If you need to add a new data point, you don’t have to recompute the entire polynomial. Only the new divided differences need to be computed.\nEasy to Implement: The recursive approach to finding divided differences makes this method easy to implement in code.\n\n\n\nLimitations of Newton’s Divided Differences\n\nNumerical Stability: Like other polynomial interpolation methods, Newton’s divided differences can suffer from numerical instability, especially with large datasets or unevenly spaced data.\nOscillations: High-degree interpolating polynomials may oscillate significantly between data points, especially if the data is not well-distributed (similar to Runge’s phenomenon).\n\n\n\nConclusion\nNewton’s Divided Differences is a powerful method for constructing interpolating polynomials, especially when efficiency and incremental data updates are needed. Its recursive nature allows for fast updates when new data points are added, making it useful in applications such as numerical analysis, interpolation, and curve fitting."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html",
    "href": "notes/w03/interpolation-error-formula.html",
    "title": "Interpolation Error Formula",
    "section": "",
    "text": "The interpolation error formula is an expression used to estimate the error between a true function \\(f(x)\\) and its interpolating polynomial \\(P(x)\\) at a specific point. This formula helps us understand how closely the interpolating polynomial approximates the function, especially based on the distribution of interpolation points and the smoothness of \\(f(x)\\).\nThe interpolation error formula is given by:\n\\[\nf(x) - P(x) = \\frac{(x - x_1)(x - x_2) \\cdots (x - x_n)}{n!} f^{(n)}(c)\n\\]\nwhere:\n\n\\(n\\) is the number of interpolated points.\n\\(P(x)\\) is the interpolating polynomial of degree \\(n - 1\\) that fits the points \\((x_1, y_1), \\ldots, (x_n, y_n)\\),\n\\(f^{(n)}(c)\\) is the \\(n\\)-th derivative of \\(f(x)\\) evaluated at some unknown point \\(c\\) in the interval \\([x_1, x_n]\\)."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html#overview",
    "href": "notes/w03/interpolation-error-formula.html#overview",
    "title": "Interpolation Error Formula",
    "section": "",
    "text": "The interpolation error formula is an expression used to estimate the error between a true function \\(f(x)\\) and its interpolating polynomial \\(P(x)\\) at a specific point. This formula helps us understand how closely the interpolating polynomial approximates the function, especially based on the distribution of interpolation points and the smoothness of \\(f(x)\\).\nThe interpolation error formula is given by:\n\\[\nf(x) - P(x) = \\frac{(x - x_1)(x - x_2) \\cdots (x - x_n)}{n!} f^{(n)}(c)\n\\]\nwhere:\n\n\\(n\\) is the number of interpolated points.\n\\(P(x)\\) is the interpolating polynomial of degree \\(n - 1\\) that fits the points \\((x_1, y_1), \\ldots, (x_n, y_n)\\),\n\\(f^{(n)}(c)\\) is the \\(n\\)-th derivative of \\(f(x)\\) evaluated at some unknown point \\(c\\) in the interval \\([x_1, x_n]\\)."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html#what-this-formula-shows",
    "href": "notes/w03/interpolation-error-formula.html#what-this-formula-shows",
    "title": "Interpolation Error Formula",
    "section": "What This Formula Shows",
    "text": "What This Formula Shows\nThis formula provides the error at a specific point \\(x\\) between the true function \\(f(x)\\) and the interpolating polynomial \\(P(x)\\). It does not directly give the maximum error across the entire interval, but rather the error at a particular \\(x\\) based on the distribution of interpolation points and the properties of \\(f(x)\\) at that point.\n\nKey Points\n\nPoint-Specific Error: This formula gives the interpolation error at a particular point \\(x\\). It shows the difference \\(f(x) - P(x)\\) at that point rather than over the entire interval.\nDependence on Higher Derivatives: The error depends on the \\(n\\)-th derivative of \\(f(x)\\) evaluated at some unknown point \\(c\\) within \\([x_1, x_n]\\). This term reflects how “curved” \\(f(x)\\) is over the interval. A larger \\(|f^{(n)}(c)|\\) generally results in a larger error, as higher derivatives capture more variation in \\(f(x)\\).\nApproximate Error Size: Although \\(c\\) is unknown, we can approximate the error by assuming \\(f^{(n)}(c)\\) reaches its maximum absolute value over \\([x_1, x_n]\\). This allows us to estimate an upper bound on the error at any point in the interval, though it’s still approximate.\nEffect of Distance from Nodes: The term \\((x - x_1)(x - x_2) \\cdots (x - x_n)\\) grows as \\(x\\) moves away from the interpolation nodes. This indicates that interpolation error typically increases the farther \\(x\\) is from the interpolation points, which is why interpolation tends to be most accurate near the nodes."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html#finding-the-maximum-error-over-the-interval",
    "href": "notes/w03/interpolation-error-formula.html#finding-the-maximum-error-over-the-interval",
    "title": "Interpolation Error Formula",
    "section": "Finding the Maximum Error Over the Interval",
    "text": "Finding the Maximum Error Over the Interval\nTo find the maximum error over the entire interval \\([x_1, x_n]\\), we can use the interpolation error formula to create an upper bound for the error on the interval:\n\nMaximize \\(|f^{(n)}(x)|\\) over the interval: Find the maximum of the \\(n\\)-th derivative of \\(f(x)\\), \\(f^{(n)}(x)\\), over the interval \\([x_1, x_n]\\). This value represents the largest possible influence of the function’s curvature on the error.\nMaximize \\(|(x - x_1)(x - x_2) \\cdots (x - x_n)|\\): Determine the maximum value of the product \\(|(x - x_1)(x - x_2) \\cdots (x - x_n)|\\) over the interval \\([x_1, x_n]\\). This product is largest near the midpoint of the interval (between the nodes) and tends to be smaller near the endpoints.\nCombine the Results: Multiply these two maximum values and divide by \\(n!\\) to get an upper bound on the maximum error over the interval:\n\\[\n\\max_{x \\in [x_1, x_n]} |f(x) - P(x)| \\approx \\frac{\\max_{x \\in [x_1, x_n]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|}{n!} \\cdot \\max_{x \\in [x_1, x_n]} |f^{(n)}(x)|\n\\]\n\nThis approach provides an approximate maximum error over the interval. By using these maximum values, we can ensure that the error does not exceed this bound anywhere in \\([x_1, x_n]\\).\nIn summary, the interpolation error formula gives insight into the local error at a specific point and can be used to estimate the maximum error on the interval by considering the maximum values of the components in the formula."
  },
  {
    "objectID": "notes/w07/2-3.html",
    "href": "notes/w07/2-3.html",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "",
    "text": "When solving linear systems of the form A\\mathbf{x} = \\mathbf{b}, two significant issues may arise:\n\nControllable Errors: Errors due to computational methods, which we can manage or minimize.\nUncontrollable Errors: Errors inherent to the problem’s nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving A\\mathbf{x} = \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/2-3.html#overview",
    "href": "notes/w07/2-3.html#overview",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "",
    "text": "When solving linear systems of the form A\\mathbf{x} = \\mathbf{b}, two significant issues may arise:\n\nControllable Errors: Errors due to computational methods, which we can manage or minimize.\nUncontrollable Errors: Errors inherent to the problem’s nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving A\\mathbf{x} = \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/2-3.html#vector-norms",
    "href": "notes/w07/2-3.html#vector-norms",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Vector Norms",
    "text": "Vector Norms\nA norm is a function that assigns a non-negative length or size to vectors in a vector space. Norms help measure the magnitude of vectors, which is essential in analyzing algorithms and numerical stability.\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n]^T, common norms include:\n\n1. \\ell_2-Norm (Euclidean Norm)\nMeasures the straight-line distance from the origin to the point \\mathbf{v}:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n\n\n\n2. \\ell_1-Norm (Taxicab or Manhattan Norm)\nSums the absolute values of the vector components:\n\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\dots + |v_n|\n\n\n\n3. \\ell_\\infty-Norm (Maximum or Infinity Norm)\nTakes the maximum absolute value among the components:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|\n\nAlso expressed as:\n\n\\|\\mathbf{v}\\|_\\infty = \\lim_{p \\to \\infty} \\left( \\sum_{i=1}^n |v_i|^p \\right)^{1/p}."
  },
  {
    "objectID": "notes/w07/2-3.html#matrix-norms",
    "href": "notes/w07/2-3.html#matrix-norms",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Matrix Norms",
    "text": "Matrix Norms\nMatrix norms measure the “size” of matrices and help understand how matrices affect vector norms when used as linear transformations.\nCommon matrix norms include:\n\n1. Spectral Norm (\\|A\\|_2)\nInvolves the largest singular value of A:\n\n\\|A\\|_2 = \\sqrt{\\lambda_{\\text{max}}(A^TA)},\n\nwhere \\lambda_{\\text{max}} is the largest eigenvalue of A^TA.\n\n\n2. Maximum Column Sum Norm (\\|A\\|_1)\nThe largest sum of absolute values in any column:\n\n\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|\n\n\n\n3. Maximum Row Sum Norm (\\|A\\|_\\infty)\nThe largest sum of absolute values in any row:\n\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|"
  },
  {
    "objectID": "notes/w07/2-3.html#the-matrix-infinity-norm-and-row-sums",
    "href": "notes/w07/2-3.html#the-matrix-infinity-norm-and-row-sums",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "The Matrix Infinity Norm and Row Sums",
    "text": "The Matrix Infinity Norm and Row Sums\n\nDefinition of \\|A\\|_\\infty\nFormally defined as:\n\n\\|A\\|_\\infty = \\sup_{\\mathbf{x} \\neq 0} \\frac{\\|A\\mathbf{x}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} = \\sup_{\\|\\mathbf{x}\\|_\\infty = 1} \\|A\\mathbf{x}\\|_\\infty\n\n\n\nKey Insight\n\nThe infinity norm of A is equal to the maximum row sum of A.\nIt represents the maximum effect A can have on any vector \\mathbf{x} with \\|\\mathbf{x}\\|_\\infty = 1."
  },
  {
    "objectID": "notes/w07/2-3.html#proof-a_infty-maximum-row-sum-of-a",
    "href": "notes/w07/2-3.html#proof-a_infty-maximum-row-sum-of-a",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Proof: \\|A\\|_\\infty = Maximum Row Sum of A",
    "text": "Proof: \\|A\\|_\\infty = Maximum Row Sum of A\nFor matrix A:\n\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|.\n\n\nExample\nConsider:\n\nA = \\begin{bmatrix}\n-1 & -2 & 3 \\\\\n3 & -4 & -5 \\\\\n-2 & 3 & -4\n\\end{bmatrix}\n\n\nCompute Row Sums\n\nRow 1: |-1| + |-2| + |3| = 6\nRow 2: |3| + |-4| + |-5| = 12\nRow 3: |-2| + |3| + |-4| = 9\n\nMaximum Row Sum: 12 (Row 2)\nTherefore, \\|A\\|_\\infty = 12.\n\n\n\nChoosing \\mathbf{\\hat{x}}\nTo achieve \\|A\\mathbf{\\hat{x}}\\|_\\infty = \\|A\\|_\\infty, select \\mathbf{\\hat{x}} with \\|\\mathbf{\\hat{x}}\\|_\\infty = 1 that aligns with the signs of Row 2:\n\n\\mathbf{\\hat{x}} = \\begin{bmatrix}\n1 \\\\\n-1 \\\\\n-1\n\\end{bmatrix}\n\n\nCompute A\\mathbf{\\hat{x}}:\n\nA\\mathbf{\\hat{x}} = \\begin{bmatrix}\n-1(1) + -2(-1) + 3(-1) \\\\\n3(1) + -4(-1) + -5(-1) \\\\\n-2(1) + 3(-1) + -4(-1)\n\\end{bmatrix} = \\begin{bmatrix}\n-2 \\\\\n12 \\\\\n-1\n\\end{bmatrix}\n\n\n\nCompute \\|A\\mathbf{\\hat{x}}\\|_\\infty:\n\n\\|A\\mathbf{\\hat{x}}\\|_\\infty = \\max(|-2|, |12|, |-1|) = 12 = \\|A\\|_\\infty\n\n\n\n\nConclusion\nIt is not possible to find \\mathbf{\\hat{x}} such that \\|A\\mathbf{\\hat{x}}\\|_\\infty &gt; \\|A\\|_\\infty when \\|\\mathbf{\\hat{x}}\\|_\\infty = 1, because \\|A\\|_\\infty is the supremum of \\|A\\mathbf{x}\\|_\\infty over all such \\mathbf{x}."
  },
  {
    "objectID": "notes/w07/2-3.html#error-analysis-and-condition-numbers",
    "href": "notes/w07/2-3.html#error-analysis-and-condition-numbers",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Error Analysis and Condition Numbers",
    "text": "Error Analysis and Condition Numbers\n\nWhy Care About Errors in A\\mathbf{x} = \\mathbf{b}?\nWhen solving A\\mathbf{x} = \\mathbf{b}, understanding errors helps improve numerical accuracy and stability.\nLet \\mathbf{x_a} be an approximate solution to A\\mathbf{x} = \\mathbf{b}, meaning:\n\nA\\mathbf{x_a} \\neq \\mathbf{b}\n\nWe quantify errors to assess the accuracy of \\mathbf{x_a} and evaluate the impact of approximations.\n\n\nDefinitions\n\nResidual: The difference between \\mathbf{b} and A\\mathbf{x_a}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nBackward Error (BE): Measures the infinity norm of the residual:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nRelative Backward Error (RBE): Normalizes the backward error relative to \\mathbf{b}:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nForward Error (FE): Measures the difference between the true solution \\mathbf{x} and the approximate solution \\mathbf{x_a}:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nRelative Forward Error (RFE): Normalizes the forward error relative to \\mathbf{x}:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|x\\|_\\infty}"
  },
  {
    "objectID": "notes/w07/2-3.html#error-magnification-factor-emf",
    "href": "notes/w07/2-3.html#error-magnification-factor-emf",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Error Magnification Factor (EMF)",
    "text": "Error Magnification Factor (EMF)\nThe error magnification factor (EMF) relates the relative forward error (RFE) to the relative backward error (RBE):\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n\nThis quantifies how much the backward error is amplified when reflected in the forward error."
  },
  {
    "objectID": "notes/w07/2-3.html#condition-number-of-a-matrix",
    "href": "notes/w07/2-3.html#condition-number-of-a-matrix",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Condition Number of a Matrix",
    "text": "Condition Number of a Matrix\nThe condition number of a matrix A measures the sensitivity of the solution \\mathbf{x} to changes in \\mathbf{b}. It is defined as:\n\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty\n\n\nInterpretation:\n\nA low condition number (close to 1) indicates a well-conditioned matrix.\nA high condition number suggests an ill-conditioned matrix, meaning small changes in \\mathbf{b} can result in large changes in \\mathbf{x}."
  },
  {
    "objectID": "notes/w07/2-3.html#example-1",
    "href": "notes/w07/2-3.html#example-1",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Example",
    "text": "Example\nGiven:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\n\n\nStep 1: Compute Errors\n\nForward Error (FE):\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty = \\left\\| \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\|_\\infty = 1\n\nRelative Forward Error (RFE):\n\n\\text{RFE} = \\frac{\\text{FE}}{\\|\\mathbf{x}\\|_\\infty} = \\frac{1}{2} = 0.5\n\nResidual:\n\nr = b - Ax_a = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nBackward Error (BE):\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = 3\n\nRelative Backward Error (RBE):\n\n\\text{RBE} = \\frac{\\text{BE}}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1\n\n\n\n\nStep 2: Compute EMF\nUsing:\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n\nwe find:\n\n\\text{EMF} = \\frac{0.5}{1} = 0.5\n\n\n\nStep 3: Compute Condition Number\n\nCompute \\|A\\|_\\infty:\n\n\\|A\\|_\\infty = \\max\\left( |1| + |1|, |3| + |-4| \\right) = \\max(2, 7) = 7\n\nCompute \\|A^{-1}\\|_\\infty:\nFrom A^{-1}, we find:\n\n\\|A^{-1}\\|_\\infty = \\max\\left( \\frac{4}{7} + \\frac{1}{7}, \\frac{3}{7} + \\frac{1}{7} \\right) = \\frac{5}{7}\n\nCondition Number:\n\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 7 \\cdot \\frac{5}{7} = 5"
  },
  {
    "objectID": "notes/w07/2-3.html#next-steps-pa-lu-decomposition-partial-pivoting",
    "href": "notes/w07/2-3.html#next-steps-pa-lu-decomposition-partial-pivoting",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Next Steps: PA = LU Decomposition (Partial Pivoting)",
    "text": "Next Steps: PA = LU Decomposition (Partial Pivoting)\n\nWhat is Partial Pivoting?\nPartial pivoting rearranges rows of A during LU decomposition to place the largest available pivot element on the diagonal. This ensures numerical stability by reducing rounding errors.\n\n\nConsequences of Partial Pivoting:\n\nControlled Multipliers: Ensures all multipliers satisfy |m_{ij}| \\leq 1.\nPrevents Swamping: Avoids large numerical errors caused by small pivot elements.\n\nUnderstanding norms, errors, and condition numbers is foundational for solving A\\mathbf{x} = \\mathbf{b} efficiently and accurately."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html",
    "title": "Residual in Linear Systems",
    "section": "",
    "text": "The residual is a fundamental concept in numerical linear algebra, used to quantify how far an approximate solution \\mathbf{x_a} to a linear system A\\mathbf{x} = \\mathbf{b} is from satisfying the system. It provides a direct measure of the “error” in the system when the computed solution \\mathbf{x_a} is substituted back into the equation.\nThe residual is defined as:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nwhere:\n\n\\mathbf{b}: The right-hand side vector of the system.\nA\\mathbf{x_a}: The result of substituting the approximate solution \\mathbf{x_a} into the system.\n\nIf \\mathbf{r} = \\mathbf{0}, the approximate solution is exact; otherwise, \\mathbf{r} quantifies the degree of error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#overview",
    "title": "Residual in Linear Systems",
    "section": "",
    "text": "The residual is a fundamental concept in numerical linear algebra, used to quantify how far an approximate solution \\mathbf{x_a} to a linear system A\\mathbf{x} = \\mathbf{b} is from satisfying the system. It provides a direct measure of the “error” in the system when the computed solution \\mathbf{x_a} is substituted back into the equation.\nThe residual is defined as:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nwhere:\n\n\\mathbf{b}: The right-hand side vector of the system.\nA\\mathbf{x_a}: The result of substituting the approximate solution \\mathbf{x_a} into the system.\n\nIf \\mathbf{r} = \\mathbf{0}, the approximate solution is exact; otherwise, \\mathbf{r} quantifies the degree of error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#what-the-residual-represents",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#what-the-residual-represents",
    "title": "Residual in Linear Systems",
    "section": "What the Residual Represents",
    "text": "What the Residual Represents\nThe residual measures how far \\mathbf{x_a} is from satisfying the system A\\mathbf{x} = \\mathbf{b}. Each component of \\mathbf{r} indicates the mismatch for the corresponding equation in the system.\n\nKey Points\n\nResidual as a Vector:\n\nThe residual \\mathbf{r} is a vector with the same dimensions as \\mathbf{b}.\nEach entry r_i measures the difference between b_i and the corresponding value of (A\\mathbf{x_a})_i.\n\nExact Solution:\n\nIf \\mathbf{x_a} = \\mathbf{x} (the exact solution), then: \n\\mathbf{r} = \\mathbf{0}\n\n\nApproximate Solution:\n\nFor an approximate solution \\mathbf{x_a}, the residual \\mathbf{r} \\neq \\mathbf{0}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#norm-of-the-residual",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#norm-of-the-residual",
    "title": "Residual in Linear Systems",
    "section": "Norm of the Residual",
    "text": "Norm of the Residual\nThe size of the residual can be measured using norms, such as the infinity norm:\n\n\\|\\mathbf{r}\\|_\\infty = \\max_{i} |r_i|\n\nThis provides a scalar measure of the largest discrepancy in the system. A smaller residual norm indicates that \\mathbf{x_a} is closer to satisfying the system."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#example",
    "title": "Residual in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nThe residual is:\n\n\\mathbf{r} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Residual Norm\nThe infinity norm of \\mathbf{r} is:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3\n\nThis indicates that the largest mismatch in the system is 3."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#applications-of-the-residual",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#applications-of-the-residual",
    "title": "Residual in Linear Systems",
    "section": "Applications of the Residual",
    "text": "Applications of the Residual\n\nError Analysis:\n\nThe residual is used to assess the accuracy of an approximate solution.\n\nIterative Methods:\n\nResiduals are central to iterative solvers, such as the Jacobi and Gauss-Seidel methods, to track convergence.\n\nNumerical Stability:\n\nA large residual often indicates instability or poor conditioning in the matrix A.\n\nRefining Solutions:\n\nResidual-based refinement techniques iteratively adjust \\mathbf{x_a} to minimize \\|\\mathbf{r}\\|."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#conclusion",
    "title": "Residual in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThe residual \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is a key tool in numerical linear algebra for evaluating the accuracy of an approximate solution. By analyzing the residual and its norm, we can diagnose errors, refine solutions, and ensure stability in solving linear systems."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html",
    "href": "notes/w07/norms/euclidean-vector-norm.html",
    "title": "Euclidean Vector Norm",
    "section": "",
    "text": "The Euclidean vector norm (or 2-norm) is a measure of the magnitude (or length) of a vector in Euclidean space. It is widely used in mathematics, physics, and engineering for calculating distances, measuring error, and normalizing vectors. This note covers the definition, properties, computation, and applications of the Euclidean norm."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#overview",
    "href": "notes/w07/norms/euclidean-vector-norm.html#overview",
    "title": "Euclidean Vector Norm",
    "section": "",
    "text": "The Euclidean vector norm (or 2-norm) is a measure of the magnitude (or length) of a vector in Euclidean space. It is widely used in mathematics, physics, and engineering for calculating distances, measuring error, and normalizing vectors. This note covers the definition, properties, computation, and applications of the Euclidean norm."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#definition",
    "href": "notes/w07/norms/euclidean-vector-norm.html#definition",
    "title": "Euclidean Vector Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the Euclidean norm is defined as:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}.\n\nThis formula calculates the straight-line distance from the origin to the point represented by \\mathbf{v} in n-dimensional space."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#properties",
    "href": "notes/w07/norms/euclidean-vector-norm.html#properties",
    "title": "Euclidean Vector Norm",
    "section": "Properties",
    "text": "Properties\nThe Euclidean norm satisfies several key properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_2 \\geq 0, and \\|\\mathbf{v}\\|_2 = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_2 = |c| \\|\\mathbf{v}\\|_2.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_2 \\leq \\|\\mathbf{u}\\|_2 + \\|\\mathbf{v}\\|_2."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#examples",
    "href": "notes/w07/norms/euclidean-vector-norm.html#examples",
    "title": "Euclidean Vector Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Vector\nFor \\mathbf{v} = [3, 4]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5.\n\n\n\n2. 3D Vector\nFor \\mathbf{v} = [1, 2, 3]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}.\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#applications",
    "href": "notes/w07/norms/euclidean-vector-norm.html#applications",
    "title": "Euclidean Vector Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Measuring Distance\nThe Euclidean norm is used to calculate the distance between two points \\mathbf{u} and \\mathbf{v} in space:\n\n\\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}.\n\n\n\n2. Normalizing Vectors\nTo convert a vector to unit length, divide it by its Euclidean norm:\n\n\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2}.\n\n\n\n3. Error Measurement\nIn numerical analysis, the Euclidean norm measures the error or residual of a solution.\n\n\n4. Machine Learning\nThe Euclidean norm underpins metrics like the Euclidean distance, used in clustering and regression."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#visualization",
    "href": "notes/w07/norms/euclidean-vector-norm.html#visualization",
    "title": "Euclidean Vector Norm",
    "section": "Visualization",
    "text": "Visualization\nIn two-dimensional space, the Euclidean norm corresponds to the length of the hypotenuse of a right triangle formed by the vector’s components. This is equivalent to the straight-line distance from the origin to the point represented by the vector."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#example-problem",
    "href": "notes/w07/norms/euclidean-vector-norm.html#example-problem",
    "title": "Euclidean Vector Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the Euclidean norm of \\mathbf{v} = [2, -3, 6].\n\nSolution:\n\nSquare each component: 2^2 = 4, (-3)^2 = 9, 6^2 = 36.\nSum the squares: 4 + 9 + 36 = 49.\nTake the square root: \\|\\mathbf{v}\\|_2 = \\sqrt{49} = 7."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#conclusion",
    "href": "notes/w07/norms/euclidean-vector-norm.html#conclusion",
    "title": "Euclidean Vector Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe Euclidean vector norm is an essential concept in mathematics and its applications, providing a straightforward measure of vector magnitude. Its simplicity and intuitive geometric interpretation make it a fundamental tool across diverse fields such as physics, engineering, and machine learning."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html",
    "href": "notes/w07/norms/infinity-norm-matrices.html",
    "title": "Infinity Norm for Matrices",
    "section": "",
    "text": "The infinity norm (or maximum row sum norm) of a matrix is a measure of its size, calculated as the maximum absolute row sum. It is often used to analyze the stability of numerical algorithms and the behavior of matrices in linear transformations. The infinity norm provides an upper bound on the effect a matrix can have on a vector in terms of row-wise contributions."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#overview",
    "href": "notes/w07/norms/infinity-norm-matrices.html#overview",
    "title": "Infinity Norm for Matrices",
    "section": "",
    "text": "The infinity norm (or maximum row sum norm) of a matrix is a measure of its size, calculated as the maximum absolute row sum. It is often used to analyze the stability of numerical algorithms and the behavior of matrices in linear transformations. The infinity norm provides an upper bound on the effect a matrix can have on a vector in terms of row-wise contributions."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#definition",
    "href": "notes/w07/norms/infinity-norm-matrices.html#definition",
    "title": "Infinity Norm for Matrices",
    "section": "Definition",
    "text": "Definition\nFor a matrix A = [a_{ij}] of size m \\times n, the infinity norm is defined as:\n\n\\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^n |a_{ij}|.\n\nThis is equivalent to finding the largest sum of absolute values of the elements in any row of the matrix.\n\nGeometric Interpretation\nThe infinity norm quantifies the maximum influence of a row of the matrix when applied to a vector. It answers the question: “What is the largest total contribution of any single row?”"
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#properties",
    "href": "notes/w07/norms/infinity-norm-matrices.html#properties",
    "title": "Infinity Norm for Matrices",
    "section": "Properties",
    "text": "Properties\nThe infinity norm satisfies the following key properties:\n\nNon-negativity: \\|A\\|_\\infty \\geq 0, and \\|A\\|_\\infty = 0 if and only if A = 0 (a zero matrix).\nHomogeneity: For any scalar c, \\|cA\\|_\\infty = |c| \\|A\\|_\\infty.\nTriangle Inequality: \\|A + B\\|_\\infty \\leq \\|A\\|_\\infty + \\|B\\|_\\infty."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#example",
    "href": "notes/w07/norms/infinity-norm-matrices.html#example",
    "title": "Infinity Norm for Matrices",
    "section": "Example",
    "text": "Example\nGiven a matrix A:\n\nA = \\begin{pmatrix}\n1 & -2 & 3 \\\\\n-4 & 5 & -6 \\\\\n7 & -8 & 9\n\\end{pmatrix},\n\n\nCompute the row sums:\n\nRow 1: |1| + |-2| + |3| = 1 + 2 + 3 = 6,\nRow 2: |-4| + |5| + |-6| = 4 + 5 + 6 = 15,\nRow 3: |7| + |-8| + |9| = 7 + 8 + 9 = 24.\n\nFind the maximum row sum: \n\\|A\\|_\\infty = \\max(6, 15, 24) = 24.\n\n\nThus, the infinity norm of A is 24."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#applications",
    "href": "notes/w07/norms/infinity-norm-matrices.html#applications",
    "title": "Infinity Norm for Matrices",
    "section": "Applications",
    "text": "Applications\n\nNumerical Stability:\n\nThe infinity norm is often used to measure the sensitivity of solutions to changes in the matrix.\n\nCondition Numbers:\n\nIt contributes to the calculation of matrix condition numbers, which measure how close a matrix is to being singular.\n\nError Analysis:\n\nThe infinity norm is used to bound errors in iterative algorithms and numerical solutions to matrix equations.\n\nOptimization:\n\nIt appears in optimization problems where constraints are defined by row-wise contributions."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#visualization",
    "href": "notes/w07/norms/infinity-norm-matrices.html#visualization",
    "title": "Infinity Norm for Matrices",
    "section": "Visualization",
    "text": "Visualization\nFor a matrix, the infinity norm represents the largest “row weight” when summing all the absolute values of the row’s entries. It gives insight into the matrix’s effect along the rows."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#example-problem",
    "href": "notes/w07/norms/infinity-norm-matrices.html#example-problem",
    "title": "Infinity Norm for Matrices",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the infinity norm of:\n\nA = \\begin{pmatrix}\n2 & -1 \\\\\n-3 & 4\n\\end{pmatrix}.\n\n\nSolution:\n\nCompute the row sums:\n\nRow 1: |2| + |-1| = 2 + 1 = 3,\nRow 2: |-3| + |4| = 3 + 4 = 7.\n\nFind the maximum row sum: \n\\|A\\|_\\infty = \\max(3, 7) = 7.\n\n\nThus, \\|A\\|_\\infty = 7."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#conclusion",
    "href": "notes/w07/norms/infinity-norm-matrices.html#conclusion",
    "title": "Infinity Norm for Matrices",
    "section": "Conclusion",
    "text": "Conclusion\nThe infinity norm provides a simple yet powerful way to measure the row-wise magnitude of a matrix. Its computational simplicity and relevance to numerical analysis make it an essential tool in matrix computations."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html",
    "href": "notes/w07/norms/taxicab-vector-norm.html",
    "title": "Taxicab Norm",
    "section": "",
    "text": "The Taxicab norm (also known as the Manhattan norm or \\ell_1-norm) is a way of measuring the “distance” of a vector in a grid-like space. Unlike the Euclidean norm, which measures the straight-line distance, the Taxicab norm sums the absolute values of a vector’s components. It is often used in scenarios where movement is constrained to axes, like navigating a city grid."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#overview",
    "href": "notes/w07/norms/taxicab-vector-norm.html#overview",
    "title": "Taxicab Norm",
    "section": "",
    "text": "The Taxicab norm (also known as the Manhattan norm or \\ell_1-norm) is a way of measuring the “distance” of a vector in a grid-like space. Unlike the Euclidean norm, which measures the straight-line distance, the Taxicab norm sums the absolute values of a vector’s components. It is often used in scenarios where movement is constrained to axes, like navigating a city grid."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#definition",
    "href": "notes/w07/norms/taxicab-vector-norm.html#definition",
    "title": "Taxicab Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the Taxicab norm is defined as:\n\n\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^n |v_i|.\n\nThis measures the total “travel distance” required along the axes to reach the endpoint of the vector."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#properties",
    "href": "notes/w07/norms/taxicab-vector-norm.html#properties",
    "title": "Taxicab Norm",
    "section": "Properties",
    "text": "Properties\nThe Taxicab norm satisfies the following properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_1 \\geq 0, and \\|\\mathbf{v}\\|_1 = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_1 = |c| \\|\\mathbf{v}\\|_1.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_1 \\leq \\|\\mathbf{u}\\|_1 + \\|\\mathbf{v}\\|_1."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#examples",
    "href": "notes/w07/norms/taxicab-vector-norm.html#examples",
    "title": "Taxicab Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Example\nFor \\mathbf{v} = [3, -4]:\n\n\\|\\mathbf{v}\\|_1 = |3| + |-4| = 3 + 4 = 7.\n\n\n\n2. 3D Example\nFor \\mathbf{v} = [1, -2, 3]:\n\n\\|\\mathbf{v}\\|_1 = |1| + |-2| + |3| = 1 + 2 + 3 = 6.\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#applications",
    "href": "notes/w07/norms/taxicab-vector-norm.html#applications",
    "title": "Taxicab Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Urban Navigation\nThe Taxicab norm models distance in grid-based systems, such as city streets, where movement is restricted to horizontal and vertical directions.\n\n\n2. Machine Learning\nIn machine learning, the \\ell_1-norm is used as a regularization term (Lasso regression) to encourage sparsity in models.\n\n\n3. Optimization\nThe Taxicab norm is used in linear programming and optimization problems where constraints align with the \\ell_1-metric.\n\n\n4. Signal Processing\nThe \\ell_1-norm is used in compressed sensing and sparse recovery to find solutions with minimal non-zero components."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#visualization",
    "href": "notes/w07/norms/taxicab-vector-norm.html#visualization",
    "title": "Taxicab Norm",
    "section": "Visualization",
    "text": "Visualization\nIn 2D, the set of points at a fixed Taxicab norm distance from the origin forms a diamond shape (rotated square). For example, all points satisfying \\|\\mathbf{v}\\|_1 = 3 in \\mathbb{R}^2 would form the square:\n\n|x| + |y| = 3."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#example-problem",
    "href": "notes/w07/norms/taxicab-vector-norm.html#example-problem",
    "title": "Taxicab Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the Taxicab norm of \\mathbf{v} = [-3, 4, -5].\n\nSolution:\n\nTake the absolute values of the components: |-3| = 3, |4| = 4, |-5| = 5.\nSum them: \\|\\mathbf{v}\\|_1 = 3 + 4 + 5 = 12."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#conclusion",
    "href": "notes/w07/norms/taxicab-vector-norm.html#conclusion",
    "title": "Taxicab Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe Taxicab norm is a useful measure of distance in grid-based systems and finds applications in various fields such as geometry, optimization, and machine learning. Its simplicity and intuitive interpretation make it a valuable tool for analyzing vector magnitudes under \\ell_1-metrics."
  },
  {
    "objectID": "reality-checks/index.html",
    "href": "reality-checks/index.html",
    "title": "REALITY CHECKS",
    "section": "",
    "text": "Reality Check 01\n\nStewart Platform Kinematics\n\n\n\nReality Check 05\n\nMotion Control in Computer-Aided Modeling"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html",
    "title": "Backward Error in Linear Systems",
    "section": "",
    "text": "The backward error is a crucial concept in numerical linear algebra, measuring the smallest perturbation in the right-hand side \\mathbf{b} that would make the approximate solution \\mathbf{x_a} satisfy the linear system A\\mathbf{x} = \\mathbf{b} exactly. In other words, it quantifies how much we need to adjust \\mathbf{b} so that \\mathbf{x_a} becomes an exact solution to a slightly modified system. This concept helps us understand the sensitivity of solutions and the reliability of numerical methods.\nThe backward error is defined as:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{r}: The residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the residual."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#overview",
    "title": "Backward Error in Linear Systems",
    "section": "",
    "text": "The backward error is a crucial concept in numerical linear algebra, measuring the smallest perturbation in the right-hand side \\mathbf{b} that would make the approximate solution \\mathbf{x_a} satisfy the linear system A\\mathbf{x} = \\mathbf{b} exactly. In other words, it quantifies how much we need to adjust \\mathbf{b} so that \\mathbf{x_a} becomes an exact solution to a slightly modified system. This concept helps us understand the sensitivity of solutions and the reliability of numerical methods.\nThe backward error is defined as:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{r}: The residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the residual."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#what-backward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#what-backward-error-represents",
    "title": "Backward Error in Linear Systems",
    "section": "What Backward Error Represents",
    "text": "What Backward Error Represents\n\nResidual Perspective:\n\nThe backward error reflects the size of the residual in the infinity norm. A smaller residual indicates that \\mathbf{x_a} nearly satisfies the system A\\mathbf{x} = \\mathbf{b}.\n\nAdjustment to \\mathbf{b}:\n\nIt provides an estimate of the minimal adjustment needed in \\mathbf{b} to make \\mathbf{x_a} an exact solution. Essentially, it tells us how much we need to perturb \\mathbf{b} so that A\\mathbf{x_a} = \\mathbf{b}' holds exactly for some \\mathbf{b}' close to \\mathbf{b}.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then the residual \\mathbf{r} = \\mathbf{0}, and hence:\n\n\\text{BE} = 0"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#why-backward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#why-backward-error-matters",
    "title": "Backward Error in Linear Systems",
    "section": "Why Backward Error Matters",
    "text": "Why Backward Error Matters\n\nAssessing Solution Quality:\n\nThe backward error helps evaluate how good the approximate solution \\mathbf{x_a} is by measuring its exactness for a nearby system.\n\nNumerical Stability:\n\nAlgorithms with small backward errors are considered numerically stable because they produce solutions that are accurate for slightly perturbed inputs.\n\nError Analysis:\n\nUnderstanding the backward error allows us to relate it to the forward error (the difference between \\mathbf{x_a} and the true solution \\mathbf{x}) and to analyze the overall accuracy of numerical methods."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#example",
    "title": "Backward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\times 1 + 1 \\times 1 \\\\ 3 \\times 1 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 - 2 \\\\ 2 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Backward Error\nThe backward error is the infinity norm of the residual:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3\n\nThis means the largest adjustment needed in \\mathbf{b} is 3, making \\mathbf{x_a} an exact solution for the perturbed system A\\mathbf{x_a} = \\mathbf{b}', where \\mathbf{b}' = A\\mathbf{x_a}.\n\n\nStep 4: Interpretation\n\nResidual Components:\n\nThe residual vector \\mathbf{r} has components:\n\n\\mathbf{r} = \\begin{bmatrix} r_1 \\\\ r_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nThe infinity norm is the maximum absolute value among these components:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|r_1|, |r_2|) = \\max(1, 3) = 3\n\nThis highlights that the backward error is dominated by the change in the y-component (r_2 = 3)."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#visualization-of-backward-error",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#visualization-of-backward-error",
    "title": "Backward Error in Linear Systems",
    "section": "Visualization of Backward Error",
    "text": "Visualization of Backward Error\nThe graph below illustrates the backward error:\n\nBlue Vector (\\mathbf{b}): The target vector in the system.\nGreen Vector (A\\mathbf{x_a}): The vector computed by substituting the approximate solution \\mathbf{x_a}.\nRed Vector (\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}): The residual vector, representing the discrepancy.\nResidual Components: Projections of \\mathbf{r} onto the x and y axes, showing r_1 and r_2.\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Example setup for visualizing backward error\nA = np.array([[1, 1], [3, -4]])\nb = np.array([3, 2])\nx_a = np.array([1, 1])  # Approximate solution\n\n# Compute Ax_a and residual\nAx_a = A @ x_a\nr = b - Ax_a\n\n# Improved visualization to prevent label overlap\nplt.figure(figsize=(10, 8))\norigin = np.zeros(2)\n\n# Plot vectors\nplt.quiver(*origin, *b, color='blue', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{b}$')\nplt.quiver(*origin, *Ax_a, color='green', angles='xy', scale_units='xy', scale=1, label=r'$A\\mathbf{x}_a$')\nplt.quiver(*Ax_a, *r, color='red', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_a$')\n\n# Annotate vectors with offset positions to avoid overlap\nplt.annotate(r'$A\\mathbf{x}_a$', (Ax_a[0], Ax_a[1]), textcoords=\"offset points\", xytext=(-60,10), ha='center', color='green', fontsize=12)\nplt.annotate(r'$\\mathbf{b}$', (b[0], b[1]), textcoords=\"offset points\", xytext=(-20,15), ha='center', color='blue', fontsize=12)\nplt.annotate(r'$\\mathbf{r}$', (Ax_a[0] + r[0]/2, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='red', fontsize=12)\n\n# Show vector addition\nplt.plot([Ax_a[0], b[0]], [Ax_a[1], b[1]], 'r--', linewidth=1)\n\n# Plot residual components\nplt.quiver(*Ax_a, r[0], 0, color='orange', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r_1}$')\nplt.quiver(Ax_a[0] + r[0], Ax_a[1], 0, r[1], color='purple', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r_2}$')\n\n# Annotate residual components with adjusted offsets\nplt.annotate(r'$\\mathbf{r_1}$', (Ax_a[0] + r[0]/2, Ax_a[1] - 0.2), textcoords=\"offset points\", xytext=(0,-10), ha='center', color='orange', fontsize=12)\nplt.annotate(r'$\\mathbf{r_2}$', (Ax_a[0] + r[0] + 0.1, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='purple', fontsize=12)\n\n# Highlight the maximum component of the residual\nplt.text(Ax_a[0] + r[0] + 0.1, Ax_a[1] + r[1] -0.3, r'$\\max(|\\mathbf{r_1}|, |\\mathbf{r_2}|) = |\\mathbf{r_2}| = 3$', color='purple', fontsize=10)\n\n# Graph details\nplt.xlim(-1, 5)\nplt.ylim(-2, 5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Backward Error with Residual Components', fontsize=16)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#explanation-of-the-visualization",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#explanation-of-the-visualization",
    "title": "Backward Error in Linear Systems",
    "section": "Explanation of the Visualization",
    "text": "Explanation of the Visualization\n\nVector Addition:\n\nThe residual vector \\mathbf{r} is drawn starting from A\\mathbf{x_a} and pointing towards \\mathbf{b}. This visually demonstrates that:\n\nA\\mathbf{x_a} + \\mathbf{r} = \\mathbf{b}\n\n\nResidual Components:\n\nThe residual vector \\mathbf{r} is decomposed into its x-component r_1 (orange vector) and y-component r_2 (purple vector).\nThe x-component r_1 = 1 and the y-component r_2 = 3.\n\nInfinity Norm Highlighted:\n\nThe maximum absolute component of the residual is |r_2| = 3, which is the backward error \\text{BE} = \\|\\mathbf{r}\\|_\\infty.\nThis is emphasized in the graph with a text annotation.\n\nUnderstanding the Residual Norm:\n\nBy showing the components, we can see that the backward error is determined by the largest adjustment needed in any coordinate.\nIn this case, the adjustment in the y-direction dominates the backward error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#conclusion",
    "title": "Backward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nBackward Error Significance:\n\nThe backward error provides valuable insight into the accuracy of an approximate solution from the perspective of the input data \\mathbf{b}. A small backward error implies that the approximate solution is highly reliable.\n\nVisualization Enhancements:\n\nBy decomposing the residual vector into its components and highlighting the infinity norm, we gain a clearer understanding of how the backward error is calculated.\n\nPractical Implications:\n\nIn applications where data may have inherent uncertainties, a small backward error ensures that the solution is valid for a slightly perturbed problem, which is often acceptable."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html",
    "title": "Relative Backward Error in Linear Systems",
    "section": "",
    "text": "The relative backward error is a normalized version of the backward error. It measures how large the residual vector \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is relative to the size of the right-hand side vector \\mathbf{b}. This normalization ensures the backward error is interpreted in the context of the magnitude of the original problem.\nThe relative backward error is defined as:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty},\n\nwhere:\n\n\\|\\mathbf{r}\\|_\\infty: The infinity norm of the residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\mathbf{b}\\|_\\infty: The infinity norm of the right-hand side vector \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#overview",
    "title": "Relative Backward Error in Linear Systems",
    "section": "",
    "text": "The relative backward error is a normalized version of the backward error. It measures how large the residual vector \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is relative to the size of the right-hand side vector \\mathbf{b}. This normalization ensures the backward error is interpreted in the context of the magnitude of the original problem.\nThe relative backward error is defined as:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty},\n\nwhere:\n\n\\|\\mathbf{r}\\|_\\infty: The infinity norm of the residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\mathbf{b}\\|_\\infty: The infinity norm of the right-hand side vector \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#what-relative-backward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#what-relative-backward-error-represents",
    "title": "Relative Backward Error in Linear Systems",
    "section": "What Relative Backward Error Represents",
    "text": "What Relative Backward Error Represents\n\nScale-Invariant Error:\n\nBy dividing the backward error by \\|\\mathbf{b}\\|_\\infty, the relative backward error accounts for the scale of \\mathbf{b}. This is useful when comparing systems with different magnitudes of \\mathbf{b}.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then the residual \\mathbf{r} = \\mathbf{0}, and:\n\n\\text{RBE} = 0.\n\n\nError Normalization:\n\nA small relative backward error indicates that the residual is negligible compared to the size of \\mathbf{b}, suggesting a high-quality solution."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#why-relative-backward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#why-relative-backward-error-matters",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Why Relative Backward Error Matters",
    "text": "Why Relative Backward Error Matters\n\nAssessing Solution Quality:\n\nThe relative backward error is a scale-invariant metric, making it easier to compare errors across systems of different sizes.\n\nNumerical Stability:\n\nA small relative backward error ensures that the approximate solution \\mathbf{x_a} satisfies a nearby system with respect to the scale of \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#example",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\times 1 + 1 \\times 1 \\\\ 3 \\times 1 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}.\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 - 2 \\\\ 2 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}.\n\n\n\nStep 3: Compute the Relative Backward Error\n\nCompute the infinity norms:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3,\n\\|\\mathbf{b}\\|_\\infty = \\max(|3|, |2|) = 3.\n\nCalculate the relative backward error:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1.\n\n\n\n\nStep 4: Interpretation\n\nA relative backward error of 1 indicates that the residual \\mathbf{r} is as large as the largest component of \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#visualization-of-relative-backward-error",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#visualization-of-relative-backward-error",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Visualization of Relative Backward Error",
    "text": "Visualization of Relative Backward Error\nThe graph below illustrates the residual \\mathbf{r} relative to \\mathbf{b}:\n\nBlue Vector (\\mathbf{b}): The target vector in the system.\nGreen Vector (A\\mathbf{x_a}): The approximate solution mapped back through A.\nRed Vector (\\mathbf{r}): The residual vector, normalized by \\|\\mathbf{b}\\|_\\infty.\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nA = np.array([[1, 1], [3, -4]])\nb = np.array([3, 2])\nx_a = np.array([1, 1])\n\nAx_a = A @ x_a\nr = b - Ax_a\n\nplt.figure(figsize=(10, 8))\norigin = np.zeros(2)\n\nplt.quiver(*origin, *b, color='blue', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{b}$')\nplt.quiver(*origin, *Ax_a, color='green', angles='xy', scale_units='xy', scale=1, label=r'$A\\mathbf{x}_a$')\nplt.quiver(*Ax_a, *r, color='red', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_a$')\n\nplt.annotate(r'$A\\mathbf{x}_a$', (Ax_a[0], Ax_a[1]), textcoords=\"offset points\", xytext=(-60,10), ha='center', color='green', fontsize=12)\nplt.annotate(r'$\\mathbf{b}$', (b[0], b[1]), textcoords=\"offset points\", xytext=(-20,15), ha='center', color='blue', fontsize=12)\nplt.annotate(r'$\\mathbf{r}$', (Ax_a[0] + r[0]/2, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='red', fontsize=12)\n\nplt.plot([Ax_a[0], b[0]], [Ax_a[1], b[1]], 'r--', linewidth=1)\n\nplt.xlim(-1, 5)\nplt.ylim(-2, 5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Relative Backward Error', fontsize=16)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#conclusion",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe relative backward error provides a normalized measure of how much the right-hand side \\mathbf{b} must be perturbed for \\mathbf{x_a} to satisfy the system exactly.\nBy comparing the size of \\mathbf{r} to \\mathbf{b}, the relative backward error allows for consistent error analysis across problems of different scales."
  },
  {
    "objectID": "homework/w02/exercise3-1-1c.html",
    "href": "homework/w02/exercise3-1-1c.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, -2), (2, 1), (4, 4).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, -2)):\n\n\n-2 \\cdot \\frac{(x - 2)(x - 4)}{(0 - 2)(0 - 4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{(-2)(-4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{8} = \\frac{-1}{4}(x - 2)(x - 4)\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 1))\n\n\n1 \\cdot \\frac{(x - 0)(x - 4)}{(2 - 0)(2 - 4)} = 1 \\cdot \\frac{x(x - 4)}{(2)(-2)} = \\frac{x(x - 4)}{-4}\n\n\nThird term (corresponding to (x_3, y_3) = (4, 4)):\n\n\n4 \\cdot \\frac{(x - 0)(x - 2)}{(4 - 0)(4 - 2)} = 4 \\cdot \\frac{x(x - 2)}{(4)(2)} = 4 \\cdot \\frac{x(x - 2)}{8} = \\frac{x(x - 2)}{2}\n\n\n\n\n\nP(x) = \\frac{-1}{4}(x - 2)(x - 4) + \\frac{x(x - 4)}{-4} + \\frac{x(x - 2)}{2}\n\n\n\n\nFirst term:\n\n\\frac{-1}{4}(x - 2)(x - 4) = \\frac{-x^2 + 6x - 8}{4}\n\nSecond term:\n\n\\frac{-x(x - 4)}{4} = \\frac{-x^2 + 4x}{4}\n\nThird term:\n\n\\frac{x(x - 2)}{2} = \\frac{x^2 - 2x}{2}\n\nCombine the terms:\n\nP(x) = \\frac{-x^2 + 6x - 8}{4} + \\frac{-x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nTo combine, first rewrite everything with a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nConvert the second term to have a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{2x^2 - 4x}{4}\n\nNow simplify:\n\nP(x) = \\frac{-2x^2 + 10x - 8}{4} + \\frac{2x^2 - 4x}{4}\n\n\nP(x) = \\frac{6x - 8}{4} = \\frac{3x - 4}{2}\n\nFinal polynomial:\n\nP(x) = \\frac{3x - 4}{2}\n\nThis is the interpolating polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (3 * x - 4) / 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (3x - 4) / 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-1c.html#question",
    "href": "homework/w02/exercise3-1-1c.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, -2), (2, 1), (4, 4).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, -2)):\n\n\n-2 \\cdot \\frac{(x - 2)(x - 4)}{(0 - 2)(0 - 4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{(-2)(-4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{8} = \\frac{-1}{4}(x - 2)(x - 4)\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 1))\n\n\n1 \\cdot \\frac{(x - 0)(x - 4)}{(2 - 0)(2 - 4)} = 1 \\cdot \\frac{x(x - 4)}{(2)(-2)} = \\frac{x(x - 4)}{-4}\n\n\nThird term (corresponding to (x_3, y_3) = (4, 4)):\n\n\n4 \\cdot \\frac{(x - 0)(x - 2)}{(4 - 0)(4 - 2)} = 4 \\cdot \\frac{x(x - 2)}{(4)(2)} = 4 \\cdot \\frac{x(x - 2)}{8} = \\frac{x(x - 2)}{2}\n\n\n\n\n\nP(x) = \\frac{-1}{4}(x - 2)(x - 4) + \\frac{x(x - 4)}{-4} + \\frac{x(x - 2)}{2}\n\n\n\n\nFirst term:\n\n\\frac{-1}{4}(x - 2)(x - 4) = \\frac{-x^2 + 6x - 8}{4}\n\nSecond term:\n\n\\frac{-x(x - 4)}{4} = \\frac{-x^2 + 4x}{4}\n\nThird term:\n\n\\frac{x(x - 2)}{2} = \\frac{x^2 - 2x}{2}\n\nCombine the terms:\n\nP(x) = \\frac{-x^2 + 6x - 8}{4} + \\frac{-x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nTo combine, first rewrite everything with a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nConvert the second term to have a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{2x^2 - 4x}{4}\n\nNow simplify:\n\nP(x) = \\frac{-2x^2 + 10x - 8}{4} + \\frac{2x^2 - 4x}{4}\n\n\nP(x) = \\frac{6x - 8}{4} = \\frac{3x - 4}{2}\n\nFinal polynomial:\n\nP(x) = \\frac{3x - 4}{2}\n\nThis is the interpolating polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (3 * x - 4) / 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (3x - 4) / 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-2c.html",
    "href": "homework/w02/exercise3-1-2c.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n-2\n\n\n\n\n2\n1\n1.5\n\n\n\n4\n4\n1.5\n0\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = -2, \\quad f[x_2] = y_2 = 1, \\quad f[x_3] = y_3 = 4 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-2)}{2 - 0} = \\frac{3}{2} = 1.5 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{4 - 1}{4 - 2} = \\frac{3}{2} = 1.5 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{1.5 - 1.5}{4 - 0} = 0 \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = -2 + 1.5(x - 0) + 0(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = -2 + 1.5x\n\nSo the final polynomial is:\n\nP(x) = 1.5x - 2\n\nThis is the Newton interpolating polynomial for the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return 1.5 * x - 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = 1.5x - 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-2c.html#question",
    "href": "homework/w02/exercise3-1-2c.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n-2\n\n\n\n\n2\n1\n1.5\n\n\n\n4\n4\n1.5\n0\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = -2, \\quad f[x_2] = y_2 = 1, \\quad f[x_3] = y_3 = 4 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-2)}{2 - 0} = \\frac{3}{2} = 1.5 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{4 - 1}{4 - 2} = \\frac{3}{2} = 1.5 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{1.5 - 1.5}{4 - 0} = 0 \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = -2 + 1.5(x - 0) + 0(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = -2 + 1.5x\n\nSo the final polynomial is:\n\nP(x) = 1.5x - 2\n\nThis is the Newton interpolating polynomial for the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return 1.5 * x - 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = 1.5x - 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "reality-checks/rc05/rc05.html",
    "href": "reality-checks/rc05/rc05.html",
    "title": "REALITY CHECK 05",
    "section": "",
    "text": "The use of Adaptive Quadrature is essential for maintaining constant speed along a specific path. This is a requirement in fields like computer-aided manufacturing, robotics and animation. Smooth and controlled movement is crucial for accuracy, but achieving a constant speed along a curved or complex path is challenging. Dividing a path into equal time intervals does not ensure equal-distance segments because the path’s shape influences the distance covered.\nTo address this, numerical methods are employed to divide the path into equal arc-length segments, ensuring consistent movement. The process involves several key steps:\n\nArc Length Measurement: The total length of the path is calculated using parametric equations, accounting for all curves and directional changes. This measurement provides the foundation for precise segmentation.\nMapping Path Position: To locate a point at a given distance s along the path, the corresponding parameter t is determined using numerical methods like Bisection or Newton’s Method. This ensures precise mapping of arc-length positions to their parametric coordinates.\nSegmenting the Path: The path is divided into segments of equal arc length, a process called equipartitioning. This segmentation ensures uniformity in the spacing of points along the path, regardless of its complexity or curvature.\nSmooth Traversal: Animations or simulations often demonstrate the practical effects of this approach. By comparing movement at constant parameter speed with movement along equal arc-length segments, the benefits of consistent, controlled traversal become clear, showcasing smoother and more predictable motion."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#path-equipartitioning-by-arc-length",
    "href": "reality-checks/rc05/rc05.html#path-equipartitioning-by-arc-length",
    "title": "REALITY CHECK 05",
    "section": "Path Equipartitioning by Arc Length",
    "text": "Path Equipartitioning by Arc Length\n\nProblem Statement\nEquipartition the path of Figure 5.6 into n subpaths of equal length, for n = 4 and n = 20. Plot analogues of Figure 5.6, showing the equipartitions.\n\n\n\n\n\n\n\n\n\n\n\nObjective and Approach\nThe objective is to partition the path, defined by parametric equations x(t) and y(t), into segments of equal arc length for a specified n. This method is valuable in fields requiring consistent movement along a path, such as animation or robotics.\nThe approach includes the following steps:\n\nCalculate Total Arc Length: Compute the total arc length from t = 0 to t = 1 using numerical integration, enabling calculation of the length of each segment.\n\n\\text{Segment length} = \\frac{\\text{Total arc length}}{n}\n\nLocate Partition Points Using the Bisection Method: For each segment i, use the Bisection Method to locate the parameter t_i so that the arc length from t = 0 to t = t_i equals i \\times \\text{Segment length}. This ensures equal arc lengths for each segment.\nPlot the Equipartitioned Path: Calculate x(t_i) and y(t_i) at each partition point and plot for both n = 4 and n = 20, visualizing uniform segmentation.\n\n\n\nSolution Code\n\n# Bisection method to find t for a target arc length fraction\ndef bisection_find_t(target_length, tol=1e-8):\n    a, b = 0, 1\n    while (b - a) / 2 &gt; tol:\n        midpoint = (a + b) / 2\n        if compute_arc_length(midpoint) == target_length:\n            return midpoint\n        elif compute_arc_length(midpoint) &lt; target_length:\n            a = midpoint\n        else:\n            b = midpoint\n    return (a + b) / 2\n\n# Equipartition function\ndef equipartition(n):\n    partition_points = [0]\n    total_length = compute_arc_length(1)\n    segment_length = total_length / n\n    for i in range(1, n):\n        target_length = i * segment_length\n        t_i = bisection_find_t(target_length)\n        partition_points.append(t_i)\n    partition_points.append(1)\n    return partition_points\n\n# Plot function for equipartitioned curve\ndef plot_styled_curve(n):\n    plt.figure(figsize=(8, 8), facecolor='white')\n\n    t_vals = np.linspace(0, 1, 500)\n    x_vals = x(t_vals)\n    y_vals = y(t_vals)\n\n    key_points_t = equipartition(n)\n    key_points_x = [x(t) for t in key_points_t]\n    key_points_y = [y(t) for t in key_points_t]\n\n    # Plot the curve with enhanced styling\n    plt.plot(x_vals, y_vals, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    plt.scatter(key_points_x, key_points_y, color=\"#1565C0\", s=40, zorder=4)\n\n    # Add grid with softer appearance\n    plt.grid(True, linestyle='-', alpha=0.2, color='gray')\n    plt.xticks(np.arange(-1, 1.5, 0.5))\n    plt.yticks(np.arange(0, 2.5, 0.5))\n\n\n    # Enhanced axis lines\n    ax = plt.gca()\n    ax.set_xticklabels(['' if x == 0 else str(x) for x in ax.get_xticks()])\n    ax.set_yticklabels(['' if y == 0 else str(y) for y in ax.get_yticks()])\n\n    ax.spines['left'].set_position('zero')\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['left'].set_linewidth(1.5)\n    ax.spines['bottom'].set_linewidth(1.5)\n\n    # Enhance tick appearance\n    plt.tick_params(axis='both', which='major', length=6, width=1, colors='black', direction='out')\n    plt.tick_params(axis='both', which='minor', length=3, width=1, colors='black', direction='out')\n\n    # Label positioning and styling\n    ax.set_ylabel('y', rotation=0, labelpad=15, y=1.02, fontsize=12)\n    ax.set_xlabel('x', x=1.02, fontsize=12)\n\n    plt.xlim(-1.5, 1.5)\n    plt.ylim(-0.5, 2)\n    plt.gca().set_aspect('equal')\n\n    plt.show()\n\n\nEquipartitioned Curve with n = 4\n\n\n\n\n\n\n\n\n\n\n\nEquipartitioned Curve with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of Solution Components\n\nArc Length Calculation: The function compute_arc_length(s) uses numerical integration to compute the arc length from t = 0 to a given t = s.\nBisection Method for Partitioning: bisection_find_t(target_length) finds the parameter t corresponding to a specific arc length, ensuring accurate partition points.\nEquipartition Function: equipartition(n) calculates t-values for partitioning the path into n equal arc-length segments.\nVisualization: plot_styled_curve(n) generates a plot showing the path with points marking each partition.\n\n\n\nResults and Observations\nThe plots for n = 4 and n = 20 illustrate the uniform segmentation of the path into equal-length segments, confirming the effectiveness of the equipartitioning process. This approach achieves constant distances along the path, despite non-uniform parameter spacing.\n\n\nConclusion\nThe solution effectively partitions the path into equal-length segments using numerical integration and the Bisection Method. This method can be further enhanced by employing Newton’s Method for faster convergence or adapting it to three-dimensional paths."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#path-equipartitioning-using-newtons-method",
    "href": "reality-checks/rc05/rc05.html#path-equipartitioning-using-newtons-method",
    "title": "REALITY CHECK 05",
    "section": "Path Equipartitioning Using Newton’s Method",
    "text": "Path Equipartitioning Using Newton’s Method\n\nProblem Statement\nReplace the Bisection Method in Step 2 with Newton’s Method, and repeat Steps 2 and 3. What is the derivative needed? What is a good choice for the initial guess? Is computation time decreased by this replacement?\n\n\nObjective and Approach\n\nObjective: Use Newton’s Method to locate each partition point t_i along the path, ensuring equal arc-length segments for a specified number of partitions n. Newton’s Method is expected to offer faster convergence than the Bisection Method, especially when starting with a good initial guess.\nRequired Derivative: Newton’s Method requires the derivative of the arc length function with respect to t, which is simply the arc length integrand evaluated at t:\n\nf'(t) = \\sqrt{\\left( \\frac{dx}{dt} \\right)^2 + \\left( \\frac{dy}{dt} \\right)^2}\n\nInitial Guess: A reasonable initial guess for each t_i is t_i = \\frac{i}{n}, which provides a uniformly spaced initial estimate along t, aiding the convergence of Newton’s Method.\nPerformance Comparison: To evaluate if Newton’s Method reduces computation time, we will measure the time taken by both the Bisection and Newton’s methods to achieve the same accuracy.\n\n\nWhy t_i = \\frac{i}{n} is a Good Initial Guess\n\nUniform Parameter Distribution: The parameter t varies between 0 and 1 (or the specified range of t), and \\frac{i}{n} provides evenly spaced points within this interval. This ensures that the initial guess is distributed consistently across the parameter space.\nProximity to the True Solution: For smooth and “well-behaved” curves, the true t_i values for equal arc-length segments are often near \\frac{i}{n}. This proximity ensures that Newton’s Method starts “in the ballpark” of the correct value.\nSimplicity and Efficiency: Computing \\frac{i}{n} is computationally trivial and requires no extra effort. This simplicity makes it a practical choice compared to complex initialization schemes.\nImproved Convergence: Starting close to the actual solution allows Newton’s Method to converge quadratically, reducing the number of iterations needed to achieve the desired accuracy.\n\n\n\n\nSolution Code\nThe following Python code implements Newton’s Method to find partition points and compares its performance with the Bisection Method.\n\nimport time\n\n# Newton's Method to find t for a target arc length\ndef newton_find_t(target_length, initial_guess, tol=1e-8, max_iter=100):\n    t = initial_guess\n    for _ in range(max_iter):\n        f_t = compute_arc_length(t) - target_length\n        f_prime_t = integrand(t)\n        if abs(f_t) &lt; tol:\n            return t\n        t -= f_t / f_prime_t  # Update t\n    return t\n\n# Compare performance of Bisection and Newton's methods\ndef compare_performance(target_length):\n    start_time_bisection = time.time()\n    bisection_result = bisection_find_t(target_length)\n    bisection_time = time.time() - start_time_bisection\n\n    start_time_newton = time.time()\n    newton_result = newton_find_t(target_length, initial_guess=0.5)\n    newton_time = time.time() - start_time_newton\n\n    print(f\"Bisection Method Result: {bisection_result:.9f} Time: {bisection_time:.9f} seconds\")\n    print(f\"Newton's Method Result: {newton_result:.9f} Time: {newton_time:.9f} seconds\")\n\n# Example target length (e.g., half the arc length)\ntotal_length = compute_arc_length(1)\ncompare_performance(total_length / 2)\n\nBisection Method Result: 0.800593771 Time: 0.006005764 seconds\nNewton's Method Result: 0.800593767 Time: 0.001000404 seconds\n\n\n\n\nExplanation of Solution Components\n\nNewton’s Method Implementation: The newton_find_t function applies Newton’s Method to locate the parameter t for a given arc length. It iteratively refines t by calculating f(t) and f'(t), adjusting t based on the result.\nPerformance Comparison: The compare_performance function compares the time taken by Bisection and Newton’s methods to find the target t-value. This illustrates the efficiency difference between the two methods.\n\n\n\nResults and Observations\n\nPerformance Gain: Newton’s Method generally converges faster than the Bisection Method due to its quadratic convergence rate.\nAccuracy: With an appropriately chosen initial guess, Newton’s Method efficiently reaches an accurate solution within fewer iterations.\n\n\n\nConclusion\nNewton’s Method provides a more efficient approach for finding the partition points, particularly when an initial guess is available. This reduction in computation time makes it suitable for tasks requiring high precision and quick convergence, such as real-time applications in path traversal and equipartitioning. Future explorations could involve further optimizations by dynamically refining initial guesses based on prior calculations."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#path-animation-at-original-and-constant-speed",
    "href": "reality-checks/rc05/rc05.html#path-animation-at-original-and-constant-speed",
    "title": "REALITY CHECK 05",
    "section": "Path Animation at Original and Constant Speed",
    "text": "Path Animation at Original and Constant Speed\n\nProblem Statement\nUse Python animation commands to demonstrate traveling along the path in two ways:\n\nAt the original speed, based on parameter t for 0 \\leq t \\leq 1, which results in non-uniform speed along the path.\nAt a constant speed using t^*(s) for 0 \\leq s \\leq 1, where the path is re-parameterized to maintain equal arc-length segments.\n\n\n\nObjective and Approach\n\nObjective: To visualize the difference between non-uniform and constant-speed traversal along a path.\n\nOriginal Speed: Animate movement along the path based on evenly spaced t-values, resulting in variable speed.\nConstant Speed: Animate movement along the path with equal arc-length segments by using equipartition points t^*(s).\n\nApproach:\n\nOriginal Speed Animation: Use uniformly spaced t-values from t = 0 to t = 1 to display the natural parameter-based speed.\nConstant Speed Animation: Use the previously calculated equipartition points t^*(s) to animate movement along equal arc-length segments, ensuring a uniform speed.\n\n\n\n\nSolution Code\nThe following Python code generates both animations, showing the path traversal at original and constant speeds.\nimport matplotlib.animation as animation\n\ndef animate_path():\n    fig = plt.figure(figsize=(16, 8), facecolor='white')\n\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n\n    # Generate data for original and constant speed\n    t_values_original_speed = np.linspace(0, 1, 25)\n    x_vals_original_speed = x(t_values_original_speed)\n    y_vals_original_speed = y(t_values_original_speed)\n\n    t_values_constant_speed = equipartition(25)\n    x_vals_constant_speed = [x(t) for t in t_values_constant_speed]\n    y_vals_constant_speed = [y(t) for t in t_values_constant_speed]\n\n    # Enhanced styling function for subplots\n    def style_subplot(ax, title):\n        ax.grid(True, linestyle='-', alpha=0.2, color='gray')\n        ax.set_xlim(-1.5, 1.5)\n        ax.set_ylim(-0.5, 2)\n        ax.set_xticks(np.arange(-1, 1.5, 0.5))\n        ax.set_yticks(np.arange(0, 2.5, 0.5))\n        ax.set_aspect('equal')\n        ax.set_xticklabels(['' if x == 0 else str(x) for x in ax.get_xticks()])\n        ax.set_yticklabels(['' if y == 0 else str(y) for y in ax.get_yticks()])\n\n\n        # Enhanced axis lines\n        ax.spines['left'].set_position('zero')\n        ax.spines['bottom'].set_position('zero')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.spines['left'].set_linewidth(1.5)\n        ax.spines['bottom'].set_linewidth(1.5)\n\n        # Enhanced ticks\n        ax.tick_params(axis='both', which='major', length=6, width=1, colors='black', direction='out')\n        ax.tick_params(axis='both', which='minor', length=3, width=1, colors='black', direction='out')\n\n        ax.set_title(title, pad=20, fontsize=12, fontweight='bold')\n\n    # Configure first subplot\n    ax1.plot(x_vals_original_speed, y_vals_original_speed, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    original_point, = ax1.plot([], [], 'o', color=\"#1565C0\", markersize=8, zorder=4)\n    style_subplot(ax1, \"Original Speed\")\n\n    # Configure second subplot\n    ax2.plot(x_vals_constant_speed, y_vals_constant_speed, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    constant_point, = ax2.plot([], [], 'go',  markersize=8, zorder=4)\n    style_subplot(ax2, \"Constant Speed\")\n\n    plt.tight_layout()\n\n    def update_original(fnum):\n        original_point.set_data(x_vals_original_speed[:fnum], y_vals_original_speed[:fnum])\n        return original_point,\n\n    def update_constant(fnum):\n        constant_point.set_data(x_vals_constant_speed[:fnum], y_vals_constant_speed[:fnum])\n        return constant_point,\n\n    num_frames = len(x_vals_original_speed)\n    ani = animation.FuncAnimation(fig, lambda fnum: update_original(fnum) + update_constant(fnum),\n                                frames=num_frames, interval=200, blit=True)\n    ani.save('combined_animation.mp4', writer='ffmpeg')\n\n  Your browser does not support the video tag. \n\n\n\nExplanation of Solution Components\n\nAnimation Setup:\n\nOriginal Speed: Uses evenly spaced t-values from 0 to 1, resulting in non-uniform movement along the path.\nConstant Speed: Uses equipartition points t^*(s), calculated to ensure each segment has the same arc length, resulting in uniform movement.\n\nAnimation Update Functions: Each animation frame updates the moving point on the respective path for both original and constant speeds.\n\n\n\nResults and Observations\nThe two animations effectively demonstrate the difference between moving at a variable speed (based on t) and moving at a constant speed along equal arc-length segments. By using equipartition points, the constant-speed animation shows smooth, uniform movement, which can be advantageous for applications requiring consistent traversal rates."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#experimenting-with-equipartitioning-on-a-custom-path",
    "href": "reality-checks/rc05/rc05.html#experimenting-with-equipartitioning-on-a-custom-path",
    "title": "REALITY CHECK 05",
    "section": "Experimenting with Equipartitioning on a Custom Path",
    "text": "Experimenting with Equipartitioning on a Custom Path\n\nProblem Statement\nExperiment with equipartitioning a path of your choice. Choose a path defined by parametric equations, partition it into equal arc-length segments, and animate the traversal as demonstrated in Problem 5.\n\nChosen Equation:\n\nx(t) = 0.4 \\sin(3t + \\frac{\\pi}{2}) + 0.5\n\n\ny(t) = 0.3 \\sin(4t) + 0.5\n\n\n\n\nObjective and Approach\n\nObjective: To apply equipartitioning to the specified path, dividing it into segments of equal arc length and visualizing the traversal at constant speed.\nApproach:\n\nPath Definition: Define x(t) and y(t) based on the given equations.\nEquipartitioning: Use numerical integration and Newton’s Method to divide the path into equal-length segments.\nAnimation: Animate the traversal of the path at a constant speed along the equal arc-length segments and compare it with traversal at the original, parameter-based speed.\n\n\n\n\nSolution Code\nThe Python code below calculates the equipartitioned segments and animates traversal along the path:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom scipy.integrate import quad\n\n# Parameters for the curve\nA = 0.4\na = 3\nf = np.pi / 2\nc = 0.5\nB = 0.3\nb = 4\nD = 0.5\n\n# Maximum value of t for one full loop\nt_max = 2 * np.pi\n\n# Define the functions for x(t) and y(t)\ndef x(t):\n    return A * np.sin(a * t + f) + c\n\ndef y(t):\n    return B * np.sin(b * t) + D\n\n# Derivatives of x(t) and y(t) for arc length calculation\ndef dx_dt(t):\n    return A * a * np.cos(a * t + f)\n\ndef dy_dt(t):\n    return B * b * np.cos(b * t)\n\n# Integrand for arc length calculation\ndef integrand(t):\n    return np.sqrt(dx_dt(t)**2 + dy_dt(t)**2)\n\n# Compute arc length using numerical integration\ndef compute_arc_length(s):\n    arc_length, _ = quad(integrand, 0, s)\n    return arc_length\n\n# Equipartition function to divide path into equal arc-length segments\ndef equipartition(n):\n    total_length = compute_arc_length(2 * np.pi)\n    segment_length = total_length / n\n    partition_points = [0]\n    for i in range(1, n):\n        target_length = i * segment_length\n        partition_points.append(find_t_for_length(target_length, partition_points[-1]))\n    partition_points.append(2 * np.pi)\n    return partition_points\n\n# Find parameter t for a given arc length using Newton's Method\ndef find_t_for_length(target_length, initial_guess=0, tol=1e-8, max_iter=100):\n    t = initial_guess\n    for _ in range(max_iter):\n        f_t = compute_arc_length(t) - target_length\n        f_prime_t = integrand(t)\n        if abs(f_t) &lt; tol:\n            return t\n        t -= f_t / f_prime_t\n        t = max(0, min(2 * np.pi, t))\n    return t\n\n\n# Data for animations\nn_points = 200\nt_values_original = np.linspace(0, 2 * np.pi, n_points)\nx_original = x(t_values_original)\ny_original = y(t_values_original)\n\nt_values_constant = equipartition(n_points)\nx_constant = [x(t) for t in t_values_constant]\ny_constant = [y(t) for t in t_values_constant]\n\n# Set up the figure for side-by-side animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Original speed plot\nax1.plot(x_original, y_original, color=\"#2196F3\", linewidth=2)\npoint1, = ax1.plot([], [], 'o', color=\"#1565C0\")\nax1.set_title(\"Original Speed\")\nax1.set_xlim(0, 1)\nax1.set_ylim(0, 1)\nax1.set_aspect('equal')\nax1.grid(True, linestyle='-', alpha=0.2, color='gray')\n\n# Constant speed plot\nax2.plot(x_constant, y_constant, color=\"#2196F3\", linewidth=2)\npoint2, = ax2.plot([], [], 'go')\nax2.set_title(\"Constant Speed\")\nax2.set_xlim(0, 1)\nax2.set_ylim(0, 1)\nax2.set_aspect('equal')\nax2.grid(True, linestyle='-', alpha=0.2, color='gray')\n\n# Update functions for each animation\ndef update_original(frame):\n    point1.set_data(x_original[:frame], y_original[:frame])\n    return point1,\n\ndef update_constant(frame):\n    point2.set_data(x_constant[:frame], y_constant[:frame])\n    return point2,\n\n# Combine animations into one\nnum_frames = len(x_original)\nani = animation.FuncAnimation(\n    fig,\n    lambda frame: update_original(frame) + update_constant(frame),\n    frames=num_frames,\n    interval=100,\n    blit=True\n)\n\n# Save animation as MP4\nani.save(\"custom_path_animation.mp4\", writer=\"ffmpeg\")\n\n  Your browser does not support the video tag. \n\n\n\nExplanation of Solution Components\n\nPath Definition: The parametric equations for x(t) = 0.4 \\sin(3t + \\frac{\\pi}{2}) + 0.5 and y(t) = 0.3 \\sin(4t) + 0.5 define a periodic curve with sinusoidal behavior, creating a visually interesting pattern with symmetric, tight curves.\nArc Length Calculation: The function compute_arc_length integrates the instantaneous speed along the curve (using derivatives dx/dt and dy/dt) over the interval [0, s] to determine the total distance traveled up to a given s.\nEquipartitioning: The equipartition function divides the path into n segments of equal arc length by calculating the target length of each segment and using Newton’s Method to determine t values corresponding to each target segment length. This ensures the segments are evenly spaced along the curve.\nAnimation: The animate_path function generates side-by-side animations of path traversal at original speed (based on parameter t) and constant speed (based on equal arc-length segments).\n\n\n\nResults and Observations\nIn the animation:\n\nOriginal Speed: The left animation shows traversal based on equally spaced t values, resulting in variable speed along the curve. The point moves faster along straighter sections and slows down significantly in tighter curves.\nConstant Speed: The right animation demonstrates traversal at a constant speed along equal arc-length segments. This movement is smoother and consistent, highlighting how equipartitioning ensures a steady traversal rate even along complex paths.\n\n\n\nConclusion\nThis exercise illustrates the benefits of equipartitioning a path into equal arc-length segments for applications that require consistent speed. By reparameterizing the curve to maintain constant speed, we can avoid the variable movement speed that results from a simple, evenly spaced parameter t. This method has potential applications in animation, robotics, and automated manufacturing, where uniform movement along a path with varying curvature is essential."
  }
]