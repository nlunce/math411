[
  {
    "objectID": "reality-checks/rc01/rc01.html",
    "href": "reality-checks/rc01/rc01.html",
    "title": "REALITY CHECK 01",
    "section": "",
    "text": "PROBLEM 1\n\n\nWrite a python function for f(\\theta). The parameters L_1, L_2, L_3, \\gamma, x_1, x_2, y_2 are fixed constants, and the strut lengths p_1, p_2, p_3 will be known for a given pose. To test your code, set the parameters L_1 = 2, L_2 = L_3 = \\sqrt{2}, \\gamma = \\pi/2, and p_1 = p_2 = p_3 = \\sqrt{5}. Then, substituting \\theta = -\\pi/4 or \\theta = \\pi/4, should make f(\\theta) = 0.\nI implemented the Python function f(\\theta) by putting all fixed constants into a Constants object. I initialized the constants with the given values in order to verify that \\theta = -\\pi/4 and \\theta = \\pi/4 were roots.\n\n\nCreate function for f(\\theta)\n\n\nShow Code\n# Define the function f(θ) that calculates based on given constants and angle θ\ndef f(theta, constants):\n    \"\"\"\n    Calculates a value based on the given angle theta and constants object.\n\n    Parameters:\n    theta (float): The angle in radians.\n    constants (Constants): An object containing the necessary constants.\n\n    Returns:\n    float: The calculated result.\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n    p1, p2, p3 = constants.p1, constants.p2, constants.p3\n\n    a2 = l3 * np.cos(theta) - x1\n    b2 = l3 * np.sin(theta)\n    a3 = l2 * np.cos(theta + gamma) - x2\n    b3 = l2 * np.sin(theta + gamma) - y2\n    d = 2 * (a2 * b3 - b2 * a3)\n\n    n1 = b3 * (p2**2 - p1**2 - a2**2 - b2**2) - b2 * (p3**2 - p1**2 - a3**2 - b3**2)\n    n2 = -a3 * (p2**2 - p1**2 - a2**2 - b2**2) + a2 * (p3**2 - p1**2 - a3**2 - b3**2)\n\n    return n1**2 + n2**2 - p1**2 * d**2\n\n\n\n\nTest function f(\\theta)\n\n\nShow Code\n# Define constants and evaluate f(θ) at a specific angle θ = π/4 for testing purposes\nconstants = Constants(\n    l1=2,\n    l2=np.sqrt(2),\n    l3=np.sqrt(2),\n    gamma=np.pi / 2,\n    x1=4,\n    x2=0,\n    y2=4,\n    p1=np.sqrt(5),\n    p2=np.sqrt(5),\n    p3=np.sqrt(5)\n)\n\ntheta = np.pi / 4\n# Evaluate\nresult = f(theta, constants)\nprint(f'f(θ=π/4) = {result}')\n\n\nf(θ=π/4) = -4.547473508864641e-13\n\n\n\n\n\nPROBLEM 2\n\n\nPlot f(\\theta) on [-\\pi, \\pi]\nI plotted the function f(\\theta) over the interval [-π, π] by generating a range of \\theta values and computing f(\\theta) for each. The graph clearly shows the behavior of f(\\theta) and highlights that the the roots identified in Problem 1 are in fact roots.\n\n\nCreate Plot\n\n\nShow Code\n# Generate a range of theta values and compute f(θ) for each value to visualize the function\ntheta_values = np.linspace(-np.pi, np.pi, 400)\nresults = [f(theta, constants) for theta in theta_values]\n\n# Plot f(θ) over the range of theta values\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, results, label=r'$f(\\theta)$', linewidth=2)\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.axvline(-np.pi/4, color='red', linestyle=':', linewidth=2, label=r'$\\theta = -\\pi/4$')\nplt.axvline(np.pi/4, color='red', linestyle=':', linewidth=2, label=r'$\\theta = \\pi/4$')\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 3\n\n\nReproduce Figure 1.15. Plot a red triangle with vertices (u_1, v_1), (u_2, v_2), (u_3, v_3) and place small blue circles at the strut anchor points (0,0), (x_1, 0), (x_2, y_2):\nI utilized several helper functions to efficiently calculate and visualize the Stewart platform’s configuration. The get_x_y() function computes the x and y coordinates based on the given angle \\theta and the fixed constants, determining the position of one vertex of the triangle. The get_points() function then takes these coordinates, along with \\theta and the constants, to calculate the two other vertices of the red triangle. The get_anchor_points() function gets the fixed anchor points (0,0), (x_1, 0), and (x_2, y_2). The plot_triangle() function takes the calculated triangle vertices and anchor points to plot the red triangle and connect the anchor points with blue lines, while also marking the anchor points with blue circles.\n\n\nCreate Helper Functions\n\n\nShow Code\n# Define helper functions for calculating x, y coordinates and plotting the Stewart platform triangle\ndef get_x_y(theta, constants):\n    \"\"\"\n    Returns the coordinates x and y for the given angle theta and constants object.\n\n    Parameters:\n    theta (float): The angle in radians.\n    constants (Constants): An object containing the necessary constants.\n\n    Returns:\n    tuple: The coordinates (x, y).\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n    p1, p2, p3 = constants.p1, constants.p2, constants.p3\n\n    a2 = l3 * np.cos(theta) - x1\n    b2 = l3 * np.sin(theta)\n    a3 = l2 * np.cos(theta + gamma) - x2\n    b3 = l2 * np.sin(theta + gamma) - y2\n\n    d = 2 * (a2 * b3 - b2 * a3)\n    n1 = b3 * (p2**2 - p1**2 - a2**2 - b2**2) - b2 * (p3**2 - p1**2 - a3**2 - b3**2)\n    n2 = -a3 * (p2**2 - p1**2 - a2**2 - b2**2) + a2 * (p3**2 - p1**2 - a3**2 - b3**2)\n\n    x = n1 / d\n    y = n2 / d\n\n    return x, y\n\n\ndef get_points(x, y, theta, constants):\n    \"\"\"\n    Calculate the three points (vertices) of the triangle in the Stewart platform based on x, y, and θ.\n\n    Parameters:\n    x (float): The x-coordinate.\n    y (float): The y-coordinate.\n    theta (float): The angle in radians.\n    constants (Constants): Object containing the necessary constants.\n\n    Returns:\n    list: A list containing the three vertices (l1_point, l2_point, l3_point) of the triangle.\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n\n    # First vertex (base point)\n    l1_point = (x, y)\n\n    # Second vertex of the triangle\n    l2_x = x + (l3 * np.cos(theta))\n    l2_y = y + (l3 * np.sin(theta))\n    l2_point = (np.round(l2_x, 3), np.round(l2_y))  # Rounded to 3 decimal places for clarity\n\n    # Third vertex of the triangle\n    l3_x = x + (l2 * np.cos(theta + gamma))\n    l3_y = y + (l2 * np.sin(theta + gamma))\n    l3_point = (np.round(l3_x), np.round(l3_y))  # Rounded to 3 decimal places for clarity\n\n    return [l1_point, l2_point, l3_point]\n\ndef get_anchor_points(constants):\n    \"\"\"\n    Get the anchor points for the Stewart platform based on the constants.\n\n    Parameters:\n    constants (Constants): Object containing the necessary constants.\n\n    Returns:\n    list: A list of tuples representing the anchor points.\n    \"\"\"\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n\n    return [(0, 0), (x1, 0), (x2, y2)]\n\ndef plot_triangle(ax, points, anchor_points, x_limits=None, y_limits=None, x_step=None, y_step=None):\n    \"\"\"\n    Plots a triangle given the points and anchor points on the provided axis.\n\n    Parameters:\n    ax: The axis on which to plot the triangle.\n    points: The points of the triangle (list of 3 points).\n    anchor_points: The anchor points (list of 2 or more points).\n    x_limits (tuple, optional): Tuple specifying the x-axis limits (x_min, x_max).\n    y_limits (tuple, optional): Tuple specifying the y-axis limits (y_min, y_max).\n    x_step (float, optional): Step size for the x-axis grid.\n    y_step (float, optional): Step size for the y-axis grid.\n\n    Returns:\n    None\n    \"\"\"\n    points = np.array(points)\n    anchor_points = np.array(anchor_points)\n\n    # Extract x and y coordinates for the triangle points\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by appending the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Plot the triangle with red lines\n    ax.plot(x_closed, y_closed, 'r-', linewidth=3.5)\n\n    # Plot blue dots at the triangle vertices\n    ax.plot(x_coords, y_coords, 'bo', markersize=8)\n\n    # Plot lines from anchor points to triangle points\n    for i, anchor in enumerate(anchor_points):\n        if i &lt; len(points):  # Ensure we stay within bounds\n            ax.plot([anchor[0], points[i, 0]], [anchor[1], points[i, 1]], 'b-', linewidth=1.5)\n\n    # Plot blue dots at the anchor points\n    ax.plot(anchor_points[:, 0], anchor_points[:, 1], 'bo', markersize=8)\n\n    # Set axis labels\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\n    # Set x-axis limits if provided\n    if x_limits is not None:\n        ax.set_xlim(x_limits)\n    # Set y-axis limits if provided\n    if y_limits is not None:\n        ax.set_ylim(y_limits)\n    # Set grid step increments if limits are provided\n    if x_step is not None and x_limits is not None:\n        ax.set_xticks(np.arange(x_limits[0], x_limits[1] + x_step, x_step))  # Adjust x-axis ticks\n    if y_step is not None and y_limits is not None:\n        ax.set_yticks(np.arange(y_limits[0], y_limits[1] + y_step, y_step))  # Adjust y-axis ticks\n\n    # Add grid for better visualization\n    ax.grid(True)\n\n\n\n\nCreate Plot\n\n\nShow code\n# Create a plot to visualize the Stewart platform configurations for two different angles\ntheta = np.pi / 4\ntheta_negative = -np.pi / 4\n\n# Calculate the coordinates and points for the triangles\nx, y = get_x_y(theta_negative, constants)\npoints1 = get_points(x, y, theta_negative, constants)\nanchor_points = get_anchor_points(constants)\n\nx, y = get_x_y(theta, constants)\npoints2 = get_points(x, y, theta, constants)\n\n# Create side-by-side subplots to visualize the two triangles\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the triangles on each subplot\nplot_triangle(axes[0], points1, anchor_points, x_limits=(-0.25, 4.25), y_limits=(-0.25, 4.25))\nplot_triangle(axes[1], points2, anchor_points, x_limits=(-0.25, 4.25), y_limits=(-0.25, 4.25))\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 4\n\n\nSolve the forward kinematics problem for the planar Stewart platform specified by x_1 = 5, (x_2, y_2) = (0,6), L_1 = L_3 = 3, L_2 = 3\\sqrt{2}, \\gamma = \\pi / 4, p_1 = p_2 = 5, p_3 = 3. Begin by plotting f(\\theta). Use an equation solver of your choice to find all four poses (roots of f(\\theta)), and plot them. Check your answers by verifying that p_1, p_2, p_3 are the lengths of the struts in your plot.\nI organized all the fixed parameters into a Constants object and plotted the function f(\\theta) over the interval [-π, π] to visualize its behavior. Using the fsolve function with strategically chosen initial guesses, I identified all four roots of f(\\theta), each root representing a unique pose of the Stewart platform. For each detected root, I plotted the corresponding triangle configuration and verified that the strut lengths p_1, p_2, p_3 matched the expected values.\n\n\n4A)\n\n\nShow Code\n# Create new constants object\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=5,\n    p3=3\n)\n\n# Generate an array of θ values between -π and π\ntheta_values = np.linspace(-np.pi, np.pi, 400)\n\n# Plot the function f(θ) over the range of θ values using the given constants\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, f(theta_values, constants), label=r'$f(\\theta)$')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4B)\n\n\nShow Code\n# Function to find roots of f(θ) using fsolve\ndef find_roots(constants, initial_guesses):\n    \"\"\"\n    Finds roots of f(θ) using different initial guesses and the fsolve method.\n\n    Parameters:\n    constants (Constants): Object containing the necessary constants.\n    initial_guesses (list or array): List of initial guesses for fsolve to start from.\n\n    Returns:\n    list: A list of unique roots.\n    \"\"\"\n\n    # Create an empty list to store the roots found\n    roots = []\n    # Iterate over each initial guess and find the root using fsolve\n    for guess in initial_guesses:\n        root = fsolve(f, guess, args=(constants), xtol=1e-12)[0] # Find root for each guess\n        roots.append(root) # Append the found root to the list\n\n    # Return only unique roots to avoid duplicates\n    unique_roots = np.unique(roots)\n    return unique_roots\n\n# Define initial guesses for fsolve to start the root-finding process\ninitial_guesses = [- 1, np.pi / 3, .5, 2]\n\n# Find and print the roots using the initial guesses\nroots = find_roots(constants, initial_guesses)\nprint(f\"The roots of f(θ) in the interval are : {roots}\")\n\n# Function to calculate the length of the struts\ndef calculate_strut_lengths(points, anchor_points):\n    lengths = []\n    # Loop through the 3 points and calculate the Euclidean distance to each corresponding anchor point\n    for i in range(3):\n        length = np.sqrt((points[i][0] - anchor_points[i][0])**2 + (points[i][1] - anchor_points[i][1])**2)\n        lengths.append(length) # Append each calculated length to the list\n    return lengths\n\n\nThe roots of f(θ) in the interval are : [-0.7208492  -0.33100518  1.14368552  2.11590901]\n\n\n\n\nShow Code\n# Create a 2x2 grid of subplots to visualize the four roots and their corresponding triangles\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten() # Flatten the 2D array of subplots into a 1D array for easier access\n\n# Get the anchor points for the Stewart platform\nanchor_points = get_anchor_points(constants)\n\n# Loop through up to four roots and plot the corresponding triangles\nfor i, theta in enumerate(roots[:4]):\n    x, y = get_x_y(theta, constants)\n    points = get_points(x, y, theta, constants)\n\n    # Plot the triangle in the corresponding subplot with custom limits\n    plot_triangle(axes[i], points, anchor_points, x_limits=(-2.5, 7.5), y_limits=(-2, 7), x_step=2.5, y_step=2)\n    axes[i].set_title(rf\"$\\theta$ = {theta}\")\n\n    # Calculate and verify strut lengths\n    lengths = calculate_strut_lengths(points, anchor_points)\n    print(f\"For root {np.round(theta, 3)}, strut lengths are: {np.round(lengths)}\")\n    print(f\"Expected: p1={constants.p1}, p2={constants.p2}, p3={constants.p3}\\n\")\n\n# Turn off any unused subplots if fewer than four roots\nfor j in range(len(roots), 4):\n    axes[j].axis('off')\n\n# Adjust layout\nplt.tight_layout()\n\nplt.show()\n\n\nFor root -0.721, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root -0.331, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root 1.144, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root 2.116, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 5\n\n\nChange strut length to p_2 = 7 and re-solve the problem. For these parameters, there are six poses.\nI updated the strut length p_2 to 7 and re-solved the forward kinematics for the Stewart platform. To do that I modified the Constants object with the new p_2 value and plotted the updated function f(\\theta) over the interval [-π, π] to see its behavior. I made a new set of initial guesses for the find_roots() function and successfully found all six roots corresponding to six possible poses. For each root, I plotted the corresponding triangle configuration and verified that the strut lengths p_1, p_2, p_3 matched the expected values.\n\n\n5A)\n\n\nShow Code\n# Update the constants to reflect the new strut length p2 = 7\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=7,\n    p3=3\n)\n\n# Generate the θ values again to visualize the updated f(θ)\ntheta_values = np.linspace(-np.pi, np.pi, 400)\n\n# Plot f(θ) for the new strut length\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, f(theta_values, constants), label=r'$f(\\theta)$')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5B)\n\n\nShow Code\n# Provide new initial guesses to find six distinct roots for this configuration\ninitial_guesses = [-.7, -.4, .01, .4, .9, 2.5 ]  # Customize this list\n\n# Find and print the roots using the initial guesses\nroots = find_roots(constants, initial_guesses)\nprint(f\"The roots of f(θ) in the interval are : {roots}\")\n\n# Set up the 2x3 grid for plotting the six poses\nfig, axes = plt.subplots(2, 3, figsize=(9, 6))  # Create a 2x3 grid\naxes = axes.flatten()  # Flatten the 2D array of axes for easier access\n\n# Get the anchor points\nanchor_points = get_anchor_points(constants)\n\n# Loop through the six roots and plot each pose\nfor i, theta in enumerate(roots[:6]):\n    x, y = get_x_y(theta, constants)\n    points = get_points(x, y, theta, constants)\n    # Plot the triangle in the corresponding subplot with custom limits\n    plot_triangle(axes[i], points, anchor_points, x_limits=(-5.5, 5.5), y_limits=(-.5, 10), )\n    axes[i].set_title(rf\"$\\theta$ = {theta}\")\n\n    # Calculate and verify strut lengths\n    lengths = calculate_strut_lengths(points, anchor_points)\n    print(f\"For root {np.round(theta, 3)}, strut lengths are: {np.round(lengths)}\")\n    print(f\"Expected: p1={constants.p1}, p2={constants.p2}, p3={constants.p3}\\n\")\n\n# Turn off any unused subplots (though in this case, we should have exactly 6)\nfor j in range(len(roots), 6):\n    axes[j].axis('off')\n\n# Adjust layout\nplt.tight_layout()\n\n\nplt.show()\n\n\nThe roots of f(θ) in the interval are : [-0.67315749 -0.35474027  0.03776676  0.45887818  0.9776729   2.5138528 ]\nFor root -0.673, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root -0.355, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.038, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.459, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.978, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 2.514, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 6\n\n\nFind a strut length p_2, with the rest of the parameters as in Step 4, for which there are only two poses.\nTo identify a strut length for p_2 that results in exactly two poses for the Stewart platform I systematically adjusted p_2 and utilizing the fsolve function to find the corresponding roots of the function f(\\theta). This method enabled me to determine a specific p_2 value that achieves the desired two-pose configuration.\n\n\nShow Code\n# Set a threshold for considering a valid root (how close to zero we want f(theta) to be)\nROOT_THRESHOLD = 1e-6\n\n# Function to find roots for a given p2, and check if they are valid\ndef find_roots_for_p2(p2_value, constants, initial_guesses, ax=None):\n    \"\"\"\n    Adjusts p2 in the constants object, finds the roots, and returns the number of unique roots.\n    Also plots f(theta) for the current p2 value on the provided axis.\n    \"\"\"\n    # Update p2 in constants\n    constants.p2 = p2_value\n\n    # Generate theta values and compute f(theta)\n    theta_values = np.linspace(-np.pi, np.pi, 400)\n    f_values = [f(theta, constants) for theta in theta_values]\n\n    # Plot f(theta) for the current p2 value on the provided axis\n    ax.plot(theta_values, f_values,)\n    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    ax.set_xlabel(r'$\\theta$', fontsize=14)\n    ax.set_ylabel(r'$f(\\theta)$', fontsize=14)\n    ax.set_title(fr'$p_2 = {p2_value:.3f}$')\n    ax.legend(fontsize=10)\n    ax.grid(True)\n\n    # Find the roots for the given p2 value\n    roots = []\n    for guess in initial_guesses:\n        root = fsolve(f, guess, args=(constants))[0]\n\n        # Check if the found root is valid (i.e., f(root) is close to zero)\n        if abs(f(root, constants)) &lt; ROOT_THRESHOLD:\n            roots.append(root)\n\n    # Convert to numpy array and round the roots to avoid precision issues\n    roots = np.round(np.array(roots), decimals=6)\n    unique_roots = np.unique(roots)\n\n    # Print the number of valid roots and the roots themselves\n    print(f\"p2 = {p2_value:.3f}: Found {len(unique_roots)} valid roots: {unique_roots}\")\n\n    return unique_roots\n\n# Function to iterate over possible p2 values and append plots in a grid (wrap after 3)\ndef find_p2_with_two_roots(constants, initial_guesses, p2_start=-1, total_plots=6):\n    \"\"\"\n    Iterates over possible p2 values starting at p2_start, plots f(theta), and prints the number of roots.\n    The plots wrap after 3 per row.\n\n    Parameters:\n    - constants: The Constants object.\n    - initial_guesses: List of initial guesses for root finding.\n    - p2_start: Starting value of p2.\n    - total_plots: Number of plots to show before stopping.\n    \"\"\"\n    p2 = p2_start\n    plot_count = 0\n    max_plots_per_row = 3  # Wrap after 3 plots per row\n\n    # Calculate the number of rows needed (wrap after 3)\n    num_rows = (total_plots + max_plots_per_row - 1) // max_plots_per_row\n\n    # Create a figure with a 3xN grid\n    fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(10, num_rows * 3))\n    axes = axes.flatten()  # Flatten the 2D array of axes for easier access\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)  # Adjust the space between subplots\n\n    # Iterate to plot p2 and find roots\n    while plot_count &lt; total_plots:\n        # Plot for the current p2 value and check the roots\n        unique_roots = find_roots_for_p2(p2, constants, initial_guesses, ax=axes[plot_count])\n\n        if len(unique_roots) == 2:  # Check if there are exactly 2 unique roots\n            print(f\"Found p2={p2} with two distinct roots: {unique_roots}\")\n\n        # Increment p2 and plot the next iteration\n        p2 += 1\n        plot_count += 1\n\n    # Show the final figure with all appended plots\n    plt.tight_layout()\n\n\n    plt.show()\n\n# Example constants (with p2 placeholder)\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=None,  # To be found\n    p3=3\n)\n\n# Initial guesses for root finding\ninitial_guesses = [-np.pi/2, 0, np.pi/2]\n\n# Start p2 at -1 and increment by 1 each time, looking for exactly 2 roots\nfind_p2_with_two_roots(constants, initial_guesses, p2_start=-1, total_plots=6)\n\n\np2 = -1.000: Found 0 valid roots: []\np2 = 0.000: Found 0 valid roots: []\np2 = 1.000: Found 0 valid roots: []\np2 = 2.000: Found 0 valid roots: []\np2 = 3.000: Found 0 valid roots: []\np2 = 4.000: Found 2 valid roots: [1.331642 1.777514]\nFound p2=4 with two distinct roots: [1.331642 1.777514]\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 7\n\n\nCalculate the intervals in p_2, with the rest of the parameters as in Step 4, for which there are 0, 2, 4, and 6 poses, respectively.\nIn transitioning from Problem 6 to Problem 7, I found that using fsolve with predefined initial guesses was too inaccurate for reliably identifying roots. This method often missed valid roots or produced duplicates due to its sensitivity to starting points. To improve accuracy, I switched to detecting sign changes in the function f(\\theta) and used the brentq algorithm, which efficiently locates roots where the function changes from positive to negative or vice versa. This approach greatly improved the precision of root detection.\n\n\nShow Code\ndef count_roots(constants, theta_min=-np.pi, theta_max=np.pi, num_points=1000):\n    \"\"\"\n    Counts roots of f(theta) = 0 within [theta_min, theta_max].\n\n    Parameters:\n    constants (Constants): Stewart platform constants.\n    theta_min (float): Lower bound of theta.\n    theta_max (float): Upper bound of theta.\n    num_points (int): Sampling points.\n\n    Returns:\n    int: Number of unique roots.\n    list: Root values.\n    \"\"\"\n    theta_vals = np.linspace(theta_min, theta_max, num_points)\n\n    # Evaluate f(theta) over the range\n    f_vals = np.array([f(theta, constants) for theta in theta_vals])\n\n    roots = []\n\n    # Detect sign changes indicating roots\n    for i in range(len(theta_vals)-1):\n        if np.sign(f_vals[i]) != np.sign(f_vals[i+1]):\n            try:\n                root = brentq(f, theta_vals[i], theta_vals[i+1], args=(constants,))\n                if theta_min &lt;= root &lt;= theta_max:\n                    roots.append(root)\n            except ValueError:\n                pass  # No root in this interval\n\n    # Eliminate duplicate roots\n    unique_roots = []\n    for r in roots:\n        if not any(np.isclose(r, ur, atol=1e-5) for ur in unique_roots):\n            unique_roots.append(r)\n\n    return len(unique_roots), unique_roots\n\ndef find_p2_intervals(constants, p2_min, p2_max, p2_step):\n    \"\"\"\n    Finds p2 intervals with specific numbers of roots.\n\n    Parameters:\n    constants (Constants): Stewart platform constants.\n    p2_min (float): Starting p2 value.\n    p2_max (float): Ending p2 value.\n    p2_step (float): Increment step for p2.\n\n    Returns:\n    dict: Pose counts as keys and p2 lists as values.\n    \"\"\"\n    p2_values = np.arange(p2_min, p2_max + p2_step, p2_step)\n    root_counts = {0: [], 2: [], 4: [], 6: []}\n\n    for p2 in p2_values:\n        constants.p2 = p2\n        num_roots, _ = count_roots(constants)\n        if num_roots in root_counts:\n            root_counts[num_roots].append(p2)\n\n    return root_counts\n\n#### 4. Implement Problem 7\n\n# Initialize constants\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=5,  # Initial p2; will be varied\n    p3=3\n)\n\n# Set p2 range\np2_min = 0.0\np2_max = 12.98  # Extended to capture p2 &gt;= 9.27\np2_step = 0.01\n\n# Get root counts\nroot_counts = find_p2_intervals(constants, p2_min, p2_max, p2_step)\n\n# Plotting\nplt.figure(figsize=(12, 6))\ncolors = {0: 'blue', 2: 'green', 4: 'orange', 6: 'red'}\n\nfor num_roots, p2_list in root_counts.items():\n    plt.scatter(p2_list, [num_roots]*len(p2_list), label=f'{num_roots} poses', s=10, color=colors.get(num_roots, 'grey'))\n\nplt.xlabel('$p_2$', fontsize=14)\nplt.ylabel('Number of Poses (Roots)', fontsize=14)\nplt.title('Number of Poses vs Length of Strut $p_2$', fontsize=16)\nplt.legend()\nplt.grid(True)\n\nplt.show()\n\n# Identify intervals\nintervals_dict = {0: [], 2: [], 4: [], 6: []}\ntol = 1e-6  # Tolerance for precision\n\nfor num_roots, p2_list in root_counts.items():\n    if p2_list:\n        p2_sorted = np.sort(p2_list)\n        diffs = np.diff(p2_sorted)\n        split_indices = np.where(diffs &gt; (p2_step + tol))[0] + 1\n        intervals = np.split(p2_sorted, split_indices)\n\n        for interval in intervals:\n            p2_start, p2_end = interval[0], interval[-1]\n            intervals_dict[num_roots].append((p2_start, p2_end))\n\n# Print first and last intervals for each pose count\nfor num_roots, intervals in intervals_dict.items():\n    if intervals:\n        print(f\"\\nIntervals with {num_roots} poses:\")\n        if len(intervals) == 1:\n            p2_start, p2_end = intervals[0]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n        else:\n            # First interval\n            p2_start, p2_end = intervals[0]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n\n            # Last interval\n            p2_start, p2_end = intervals[-1]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n\n\n\n\n\n\n\n\n\n\nIntervals with 0 poses:\n  p2 from 0.00 to 3.71\n  p2 from 9.27 to infinity\n\nIntervals with 2 poses:\n  p2 from 3.72 to 4.86\n  p2 from 7.85 to 9.26\n\nIntervals with 4 poses:\n  p2 from 4.87 to 6.96\n  p2 from 7.03 to 7.84\n\nIntervals with 6 poses:\n  p2 from 6.97 to 7.02"
  },
  {
    "objectID": "homework/w03/c0-p2.html",
    "href": "homework/w03/c0-p2.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array–choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n\n\nCreate function and figure\n# Define the function\ndef plot_triangle(points, save_path='triangle_plot.png'):\n    \"\"\"\n    Takes an input of three points (a list of 3 tuples or a 3x2 numpy array)\n    and plots a triangle with small open circles at each of the points.\n    The triangle is rendered with lines connecting each point.\n\n    Parameters:\n    points (list of tuples or numpy array): Points representing the vertices of the triangle.\n    save_path (str): File path to save the plotted figure.\n    \"\"\"\n\n    # Ensure points is a numpy array\n    points = np.array(points)\n\n    # Check if the input is in the correct shape (3x2)\n    if points.shape != (3, 2):\n        raise ValueError(\"Input should be a list of 3 points, each as a pair of (x, y) coordinates.\")\n\n    # Extract the x and y coordinates\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by repeating the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Create the plot\n    plt.figure(figsize=(6, 6))\n\n    # Plot the triangle with open circles at each vertex\n    plt.plot(x_closed, y_closed, 'b-', marker='o', markerfacecolor='none',\n             markeredgecolor='r', markersize=10, label='Triangle')\n\n    # Set labels and title\n    plt.title(\"Triangle with Given Vertices\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    # Set axis limits for better visualization\n    plt.xlim(min(x_closed) - 1, max(x_closed) + 1)\n    plt.ylim(min(y_closed) - 1, max(y_closed) + 1)\n\n    # Add grid for better visualization\n    plt.grid(True)\n\n\n    # Save the figure\n    plt.savefig(save_path, dpi=300)\n\n    # Show the plot\n   #  plt.show()\n\n# Test the function with the points (1, 2), (2, 1), and (2, 3)\ntest_points = [(1, 2), (2, 1), (2, 3)]\nplot_triangle(test_points, save_path='triangle_plot.png')\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Definition:\n\nplot_triangle: This function takes in three points and an optional save_path parameter to specify where to save the plot.\nParameters:\n\npoints: A list of three tuples representing the vertices of the triangle or a 3x2 numpy array.\nsave_path: The file path where the plot image will be saved (default is 'triangle_plot.png').\n\n\nInput Validation:\n\nThe function first converts the input points to a numpy array and checks if it has the correct shape (3, 2). If not, it raises a ValueError.\n\nPlotting:\n\nClosing the Triangle: To draw a complete triangle, the first point is appended to the end of the x_coords and y_coords arrays.\nPlotting Lines and Markers:\n\nThe triangle is plotted with blue lines ('b-') connecting the points.\nSmall open red circles (marker='o', markerfacecolor='none', markeredgecolor='r') are placed at each vertex.\n\nLabels and Grid: The plot includes titles, axis labels, and a grid for better visualization.\n\nSaving and Displaying the Plot:\n\nThe plot is saved as a .png file with a resolution of 300 DPI.\nThe plot window is then displayed using plt.show().\n\n\n\n\n\nAfter running the function with the test points (1, 2), (2, 1), and (2, 3), the resulting triangle will be saved as triangle_plot.png and displayed as shown below:\n\n\n\nTriangle Plot"
  },
  {
    "objectID": "homework/w03/c0-p2.html#question",
    "href": "homework/w03/c0-p2.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array–choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n\n\nCreate function and figure\n# Define the function\ndef plot_triangle(points, save_path='triangle_plot.png'):\n    \"\"\"\n    Takes an input of three points (a list of 3 tuples or a 3x2 numpy array)\n    and plots a triangle with small open circles at each of the points.\n    The triangle is rendered with lines connecting each point.\n\n    Parameters:\n    points (list of tuples or numpy array): Points representing the vertices of the triangle.\n    save_path (str): File path to save the plotted figure.\n    \"\"\"\n\n    # Ensure points is a numpy array\n    points = np.array(points)\n\n    # Check if the input is in the correct shape (3x2)\n    if points.shape != (3, 2):\n        raise ValueError(\"Input should be a list of 3 points, each as a pair of (x, y) coordinates.\")\n\n    # Extract the x and y coordinates\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by repeating the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Create the plot\n    plt.figure(figsize=(6, 6))\n\n    # Plot the triangle with open circles at each vertex\n    plt.plot(x_closed, y_closed, 'b-', marker='o', markerfacecolor='none',\n             markeredgecolor='r', markersize=10, label='Triangle')\n\n    # Set labels and title\n    plt.title(\"Triangle with Given Vertices\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    # Set axis limits for better visualization\n    plt.xlim(min(x_closed) - 1, max(x_closed) + 1)\n    plt.ylim(min(y_closed) - 1, max(y_closed) + 1)\n\n    # Add grid for better visualization\n    plt.grid(True)\n\n\n    # Save the figure\n    plt.savefig(save_path, dpi=300)\n\n    # Show the plot\n   #  plt.show()\n\n# Test the function with the points (1, 2), (2, 1), and (2, 3)\ntest_points = [(1, 2), (2, 1), (2, 3)]\nplot_triangle(test_points, save_path='triangle_plot.png')\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Definition:\n\nplot_triangle: This function takes in three points and an optional save_path parameter to specify where to save the plot.\nParameters:\n\npoints: A list of three tuples representing the vertices of the triangle or a 3x2 numpy array.\nsave_path: The file path where the plot image will be saved (default is 'triangle_plot.png').\n\n\nInput Validation:\n\nThe function first converts the input points to a numpy array and checks if it has the correct shape (3, 2). If not, it raises a ValueError.\n\nPlotting:\n\nClosing the Triangle: To draw a complete triangle, the first point is appended to the end of the x_coords and y_coords arrays.\nPlotting Lines and Markers:\n\nThe triangle is plotted with blue lines ('b-') connecting the points.\nSmall open red circles (marker='o', markerfacecolor='none', markeredgecolor='r') are placed at each vertex.\n\nLabels and Grid: The plot includes titles, axis labels, and a grid for better visualization.\n\nSaving and Displaying the Plot:\n\nThe plot is saved as a .png file with a resolution of 300 DPI.\nThe plot window is then displayed using plt.show().\n\n\n\n\n\nAfter running the function with the test points (1, 2), (2, 1), and (2, 3), the resulting triangle will be saved as triangle_plot.png and displayed as shown below:\n\n\n\nTriangle Plot"
  },
  {
    "objectID": "homework/w02/exercise3-1-2a.html",
    "href": "homework/w02/exercise3-1-2a.html",
    "title": "Exercise 3.1.2a (C3-P2)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\n\n\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n1\n\n\n\n\n2\n3\n1\n\n\n\n3\n0\n-3\n-\\frac{4}{3}\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = 1, \\quad f[x_2] = y_2 = 3, \\quad f[x_3] = y_3 = 0 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{3 - 1}{2 - 0} = \\frac{2}{2} = 1 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{0 - 3}{3 - 2} = \\frac{-3}{1} = -3 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-3 - 1}{3 - 0} = \\frac{-4}{3} \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = 1 + 1(x - 0) + \\left(\\frac{-4}{3}\\right)(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = 1 + x - \\frac{4}{3}(x(x - 2)) = 1 + x - \\frac{4}{3}(x^2 - 2x)\n\n\nP(x) = 1 + x - \\frac{4}{3}x^2 + \\frac{8}{3}x\n\nCombine like terms:\n\nP(x) = 1 + \\left(x + \\frac{8}{3}x\\right) - \\frac{4}{3}x^2 = 1 + \\frac{11x}{3} - \\frac{4x^2}{3}\n\nSo the final polynomial is:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the Newton interpolating polynomial for the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-2a.html#question",
    "href": "homework/w02/exercise3-1-2a.html#question",
    "title": "Exercise 3.1.2a (C3-P2)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\n\n\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n1\n\n\n\n\n2\n3\n1\n\n\n\n3\n0\n-3\n-\\frac{4}{3}\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = 1, \\quad f[x_2] = y_2 = 3, \\quad f[x_3] = y_3 = 0 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{3 - 1}{2 - 0} = \\frac{2}{2} = 1 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{0 - 3}{3 - 2} = \\frac{-3}{1} = -3 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-3 - 1}{3 - 0} = \\frac{-4}{3} \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = 1 + 1(x - 0) + \\left(\\frac{-4}{3}\\right)(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = 1 + x - \\frac{4}{3}(x(x - 2)) = 1 + x - \\frac{4}{3}(x^2 - 2x)\n\n\nP(x) = 1 + x - \\frac{4}{3}x^2 + \\frac{8}{3}x\n\nCombine like terms:\n\nP(x) = 1 + \\left(x + \\frac{8}{3}x\\right) - \\frac{4}{3}x^2 = 1 + \\frac{11x}{3} - \\frac{4x^2}{3}\n\nSo the final polynomial is:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the Newton interpolating polynomial for the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-1a.html",
    "href": "homework/w02/exercise3-1-1a.html",
    "title": "Exercise 3.1.1a (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, 1)):\n\n\n1 \\cdot \\frac{(x - 2)(x - 3)}{(0 - 2)(0 - 3)} = 1 \\cdot \\frac{(x - 2)(x - 3)}{(-2)(-3)} = \\frac{(x - 2)(x - 3)}{6}\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 3))\n\n\n3 \\cdot \\frac{(x - 0)(x - 3)}{(2 - 0)(2 - 3)} = 3 \\cdot \\frac{(x)(x - 3)}{(2)(-1)} = -\\frac{3x(x - 3)}{2}\n\n\nThird term (corresponding to (x_3, y_3) = (3, 0)):\n\n\n0 \\cdot \\frac{(x - 0)(x - 2)}{(3 - 0)(3 - 2)} = 0\n\n\n\n\n\nP(x) = \\frac{(x - 2)(x - 3)}{6} - \\frac{3x(x - 3)}{2}\n\n\n\n\nFirst term:\n\n\\frac{(x - 2)(x - 3)}{6} = \\frac{x^2 - 5x + 6}{6}\n\nSecond term:\n\n-\\frac{3x(x - 3)}{2} = -\\frac{3(x^2 - 3x)}{2} = -\\frac{3x^2}{2} + \\frac{9x}{2}\n\nNow, combine these two terms:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\left(\\frac{3x^2}{2} - \\frac{9x}{2}\\right)\n\nTo combine, first rewrite everything with a denominator of 6:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\frac{9x^2 - 27x}{6}\n\nNow simplify:\n\nP(x) = \\frac{x^2 - 5x + 6 - 9x^2 + 27x}{6}\n\n\nP(x) = \\frac{-8x^2 + 22x + 6}{6}\n\nThis is the final polynomial:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the interpolating polynomial that passes through the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-1a.html#question",
    "href": "homework/w02/exercise3-1-1a.html#question",
    "title": "Exercise 3.1.1a (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, 1)):\n\n\n1 \\cdot \\frac{(x - 2)(x - 3)}{(0 - 2)(0 - 3)} = 1 \\cdot \\frac{(x - 2)(x - 3)}{(-2)(-3)} = \\frac{(x - 2)(x - 3)}{6}\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 3))\n\n\n3 \\cdot \\frac{(x - 0)(x - 3)}{(2 - 0)(2 - 3)} = 3 \\cdot \\frac{(x)(x - 3)}{(2)(-1)} = -\\frac{3x(x - 3)}{2}\n\n\nThird term (corresponding to (x_3, y_3) = (3, 0)):\n\n\n0 \\cdot \\frac{(x - 0)(x - 2)}{(3 - 0)(3 - 2)} = 0\n\n\n\n\n\nP(x) = \\frac{(x - 2)(x - 3)}{6} - \\frac{3x(x - 3)}{2}\n\n\n\n\nFirst term:\n\n\\frac{(x - 2)(x - 3)}{6} = \\frac{x^2 - 5x + 6}{6}\n\nSecond term:\n\n-\\frac{3x(x - 3)}{2} = -\\frac{3(x^2 - 3x)}{2} = -\\frac{3x^2}{2} + \\frac{9x}{2}\n\nNow, combine these two terms:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\left(\\frac{3x^2}{2} - \\frac{9x}{2}\\right)\n\nTo combine, first rewrite everything with a denominator of 6:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\frac{9x^2 - 27x}{6}\n\nNow simplify:\n\nP(x) = \\frac{x^2 - 5x + 6 - 9x^2 + 27x}{6}\n\n\nP(x) = \\frac{-8x^2 + 22x + 6}{6}\n\nThis is the final polynomial:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the interpolating polynomial that passes through the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "",
    "text": "When tackling mathematical problems, especially those involving systems of equations, understanding how errors influence the solutions is paramount. The Error Magnification Factor (EMF) is a crucial tool that allows us to analyze the relationship between different types of errors in numerical computations, helping us assess the reliability and stability of our solutions."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#overview",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "",
    "text": "When tackling mathematical problems, especially those involving systems of equations, understanding how errors influence the solutions is paramount. The Error Magnification Factor (EMF) is a crucial tool that allows us to analyze the relationship between different types of errors in numerical computations, helping us assess the reliability and stability of our solutions."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#what-is-emf",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#what-is-emf",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "What is EMF?",
    "text": "What is EMF?\nThe Error Magnification Factor (EMF) quantifies how an initial error, known as the backward error, is amplified when computing a solution, leading to the forward error. Think of EMF as a magnifying glass that reveals how minor inaccuracies can escalate into significant discrepancies in the final answer.\nMathematically, EMF is defined as:\n\n\\text{EMF} = \\frac{\\text{Relative Forward Error (RFE)}}{\\text{Relative Backward Error (RBE)}}\n\n\nBreaking Down the Components\n\nRelative Forward Error (RFE): Measures the deviation of the computed solution from the true solution relative to the true solution’s magnitude.\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nRelative Backward Error (RBE): Quantifies the adjustment needed in the original problem data to make the computed solution exact, relative to the input data’s magnitude.\n\n\\text{RBE} = \\frac{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\n\nSubstituting these into the EMF formula:\n\n\\text{EMF} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty / \\|\\mathbf{x}\\|_\\infty}{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty / \\|\\mathbf{b}\\|_\\infty} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} \\cdot \\frac{\\|\\mathbf{b}\\|_\\infty}{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty}\n\nWhere:\n\n\\mathbf{x}: True Solution\n\\mathbf{x_a}: Approximate (Computed) Solution\n\\mathbf{b}: Original Input Data\nA: Coefficient Matrix in the system of equations"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#what-emf-represents",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#what-emf-represents",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "What EMF Represents",
    "text": "What EMF Represents\nUnderstanding EMF involves dissecting its components and recognizing what they reveal about the system under analysis.\n\n1. Forward vs. Backward Error\n\nBackward Error (RBE): Think of it as a small mistake in the initial setup of a problem, such as an incorrect measurement or input. It quantifies this initial discrepancy.\nForward Error (RFE): Represents how this initial mistake propagates and affects the final solution, showing the deviation from the intended outcome.\n\nEMF bridges these two by indicating how much an initial error (RBE) influences the final result (RFE). A higher EMF implies that even minor initial errors can lead to significant deviations in the solution.\n\n\n2. Sensitivity of Solutions\n\nHigh EMF: Indicates that the system is sensitive or unstable. Small errors in the input data can cause large errors in the solution.\nLow EMF: Suggests that the system is stable, with errors in input data having minimal impact on the solution.\n\n\n\n3. Well-Conditioned vs. Ill-Conditioned Systems\n\nWell-Conditioned Systems: EMF values close to 1. Errors do not get significantly magnified, making the solutions reliable.\nIll-Conditioned Systems: High EMF values. Small errors can lead to large discrepancies in solutions, rendering them unreliable."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#why-emf-matters",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#why-emf-matters",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "Why EMF Matters",
    "text": "Why EMF Matters\nUnderstanding EMF is essential for several reasons:\n\nStability Analysis: EMF helps determine whether a numerical algorithm will produce reliable results or amplify errors.\nCondition Number Connection: EMF is related to the condition number of matrix A, another measure of sensitivity in systems.\nError Propagation: By analyzing EMF, we can predict how errors in our input data will affect the final solution, enabling us to take corrective measures."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#a-practical-example",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#a-practical-example",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "A Practical Example",
    "text": "A Practical Example\nLet’s delve into a concrete example to illustrate how EMF operates in practice.\n\nThe Problem Setup\nConsider the system of equations:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} is the true solution.\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} is the approximate solution.\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} is the input data.\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} is the coefficient matrix.\n\n\n\nStep 1: Compute the Relative Forward Error (RFE)\nThe RFE measures the deviation of the computed solution from the true solution.\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nCalculations:\n\nDifference Vector:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nInfinity Norm of \\mathbf{e}:\n\n\\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\nInfinity Norm of \\mathbf{x}:\n\n\\|\\mathbf{x}\\|_\\infty = \\max(|2|, |1|) = 2\n\nRFE:\n\n\\text{RFE} = \\frac{2}{2} = 1\n\n\n\n\nStep 2: Compute the Relative Backward Error (RBE)\nThe RBE measures the adjustment needed in the original input data to make the computed solution exact.\n\n\\text{RBE} = \\frac{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nCalculations:\n\nCompute A\\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\cdot 1 + 1 \\cdot (-1) \\\\ 3 \\cdot 1 + (-4) \\cdot (-1) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix}\n\nResidual Vector:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -5 \\end{bmatrix}\n\nInfinity Norm of \\mathbf{r}:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|3|, |5|) = 5\n\nInfinity Norm of \\mathbf{b}:\n\n\\|\\mathbf{b}\\|_\\infty = \\max(|3|, |2|) = 3\n\nRBE:\n\n\\text{RBE} = \\frac{5}{3} \\approx 1.6667\n\n\n\n\nStep 3: Compute the EMF\nUsing the RFE and RBE:\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}} = \\frac{1}{1.6667} \\approx 0.6\n\n\n\nStep 4: Interpretation\nAn EMF of 0.6 implies that the backward error is magnified by a factor of 0.6 in the forward error. This indicates that the system is moderately sensitive but not highly unstable. While there is some error amplification, it isn’t excessively large, suggesting a reasonable level of stability in the solution."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#visualization-of-errors",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#visualization-of-errors",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "Visualization of Errors",
    "text": "Visualization of Errors\nVisual representations can significantly enhance our understanding of EMF. Below is a refined graph that visually illustrates the backward and forward errors in our example, with synchronized domains and ranges for better comparison.\n\nGraph of EMF Components\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.patches import Patch\nimport matplotlib.patheffects as pe\n\ndef create_fancy_arrow(start, end, color, width=2, alpha=1.0, zorder=5):\n    \"\"\"Create a fancy arrow without shadow effect for simplicity\"\"\"\n    arrow = FancyArrowPatch(\n        start, end,\n        arrowstyle='-&gt;',\n        color=color,\n        linewidth=width,\n        alpha=alpha,\n        zorder=zorder\n    )\n    return arrow\n\nx_true = np.array([2, 1])\nx_a = np.array([1, -1])\nb = np.array([3, 2])\nA = np.array([[1, 1], [3, -4]])\n\nAx_a = A @ x_a\nresidual = b - Ax_a\nforward_error = x_true - x_a\n\nall_vectors = [\n    [0, 0], b, Ax_a, x_true, x_a\n]\nmin_vals = np.min(all_vectors, axis=0) - 1\nmax_vals = np.max(all_vectors, axis=0) + 1\nx_min, x_max = min_vals[0], max_vals[0]\ny_min, y_max = min_vals[1], max_vals[1]\n\nplt.style.use('seaborn-v0_8-whitegrid')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n\ncolors = {\n    'b': '#1f77b4',        # blue\n    'Ax_a': '#2ca02c',     # green\n    'residual': '#d62728', # red\n    'x_true': '#000000',   # black\n    'x_a': '#7f7f7f',      # gray\n    'forward': '#ff7f0e'   # orange\n}\n\ninput_vectors = [\n    ((0, 0), b, colors['b'], r'$\\mathbf{b}$'),\n    ((0, 0), Ax_a, colors['Ax_a'], r'$A\\mathbf{x}_a$'),\n    (Ax_a, b, colors['residual'], r'$\\mathbf{r}$')\n]\n\nfor start, end, color, name in input_vectors:\n    ax1.quiver(*start, *(np.array(end) - np.array(start)), color=color, angles='xy', scale_units='xy', scale=1, width=0.01, zorder=5)\n\n    vector = np.array(end) - np.array(start)\n    vector_norm = np.linalg.norm(vector)\n    if vector_norm &gt; 0:\n        direction = vector / vector_norm\n    else:\n        direction = np.array([0, 0])\n\n    perp_direction = np.array([-direction[1], direction[0]])\n    offset = perp_direction * 0.5\n\n    if name == r'$A\\mathbf{x}_a$':\n        offset += np.array([-0.3, 0])\n\n    mid_point = np.array(start) + vector / 2\n    ax1.text(mid_point[0] + offset[0], mid_point[1] + offset[1],\n             name, color=color, fontsize=10, fontweight='bold', zorder=6)\n\nsolution_vectors = [\n    ((0, 0), x_true, colors['x_true'], r'$\\mathbf{x}$'),\n    ((0, 0), x_a, colors['x_a'], r'$\\mathbf{x}_a$'),\n    (x_a, x_true, colors['forward'], r'$\\mathbf{e}$')\n]\n\nfor start, end, color, name in solution_vectors:\n    ax2.quiver(*start, *(np.array(end) - np.array(start)), color=color, angles='xy', scale_units='xy', scale=1, width=0.01, zorder=5)\n\n    vector = np.array(end) - np.array(start)\n    mid_point = np.array(start) + vector / 2\n    offset = np.array([0.3, 0.3])\n\n    if name == r'$\\mathbf{x}_a$':\n        offset += np.array([-0.1, -0.3])\n\n    ax2.text(mid_point[0] + offset[0], mid_point[1] + offset[1],\n             name, color=color, fontsize=10, fontweight='bold', zorder=6)\n\nfor ax, title in zip([ax1, ax2], ['Input Space (Backward Error)', 'Solution Space (Forward Error)']):\n    ax.set_aspect('equal')\n\n    x_padding = (x_max - x_min) * 0.1\n    y_padding = (y_max - y_min) * 0.1\n    ax.set_xlim(x_min - x_padding, x_max + x_padding)\n    ax.set_ylim(y_min - y_padding, y_max + y_padding)\n\n    ax.set_title(title, fontsize=12, pad=15, fontweight='bold')\n\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n\n    ax.grid(color='lightgray', linestyle='--', linewidth=0.7, zorder=0)\n\n    ax.axhline(0, color='black', linewidth=0.8, zorder=1)\n    ax.axvline(0, color='black', linewidth=0.8, zorder=1)\n\nlegend_elements = [\n    Patch(facecolor='none', edgecolor=colors['b'], label=r'$\\mathbf{b}$ (Input)'),\n    Patch(facecolor='none', edgecolor=colors['Ax_a'], label=r'$A\\mathbf{x}_a$ (Computed)'),\n    Patch(facecolor='none', edgecolor=colors['residual'], label=r'$\\mathbf{r}$ (Residual)'),\n    Patch(facecolor='none', edgecolor=colors['x_true'], label=r'$\\mathbf{x}$ (True Solution)'),\n    Patch(facecolor='none', edgecolor=colors['x_a'], label=r'$\\mathbf{x}_a$ (Approximate)'),\n    Patch(facecolor='none', edgecolor=colors['forward'], label=r'$\\mathbf{e}$ (Forward Error)')\n]\n\nfig.legend(handles=legend_elements,\n           loc='upper center',\n           bbox_to_anchor=(0.5, 0.05),\n           ncol=3,\n           fontsize=9)\n\nfig.suptitle('Error Magnification Factor Components', fontsize=14, y=0.98, fontweight='bold')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.87, bottom=0.15, right=0.95, wspace=-0.4)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nLeft Plot: Input Space (Backward Error)\n\n\\mathbf{b} (Input): Represents the original input data vector.\nA\\mathbf{x}_a (Computed): The result of applying the coefficient matrix A to the approximate solution \\mathbf{x_a}.\n\\mathbf{r} (Residual): The difference between the original input \\mathbf{b} and the computed A\\mathbf{x}_a, indicating the backward error.\n\nRight Plot: Solution Space (Forward Error)\n\n\\mathbf{x} (True Solution): The actual solution to the system.\n\\mathbf{x}_a (Approximate): The computed solution.\n\\mathbf{e} (Forward Error): The difference between the true solution and the approximate solution, representing the forward error.\n\n\nThe arrows visually demonstrate how the backward error in the input space relates to the forward error in the solution space, encapsulating the essence of the EMF."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/error-magnification-factor.html#conclusion",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Error Magnification Factor (EMF) is a vital concept in numerical analysis, providing insight into how errors propagate through computational processes. By quantifying the relationship between backward and forward errors, EMF helps in assessing the stability and reliability of numerical solutions. Understanding and calculating EMF enables mathematicians and engineers to design more robust algorithms and make informed decisions when interpreting computational results."
  },
  {
    "objectID": "notes/w06/linear-systems.html",
    "href": "notes/w06/linear-systems.html",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "",
    "text": "In the realm of linear algebra, the equation A\\mathbf{x} = \\mathbf{b} serves as a foundational representation of a system of linear equations. Understanding the components of this equation is crucial for solving linear systems, analyzing their properties, and applying them to real-world problems. This note delves into the inputs of the equation A\\mathbf{x} = \\mathbf{b}, elucidating their roles, characteristics, and significance in the context of linear systems."
  },
  {
    "objectID": "notes/w06/linear-systems.html#overview",
    "href": "notes/w06/linear-systems.html#overview",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "",
    "text": "In the realm of linear algebra, the equation A\\mathbf{x} = \\mathbf{b} serves as a foundational representation of a system of linear equations. Understanding the components of this equation is crucial for solving linear systems, analyzing their properties, and applying them to real-world problems. This note delves into the inputs of the equation A\\mathbf{x} = \\mathbf{b}, elucidating their roles, characteristics, and significance in the context of linear systems."
  },
  {
    "objectID": "notes/w06/linear-systems.html#components-of-amathbfx-mathbfb",
    "href": "notes/w06/linear-systems.html#components-of-amathbfx-mathbfb",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Components of A\\mathbf{x} = \\mathbf{b}",
    "text": "Components of A\\mathbf{x} = \\mathbf{b}\nTo comprehensively grasp the inputs of the equation A\\mathbf{x} = \\mathbf{b}, it’s essential to break down each component:\n\nMatrix A (Coefficient Matrix):\n\nDefinition: A rectangular array of numbers arranged in rows and columns.\nRole: Encapsulates the coefficients of the variables in the system of equations.\nDimensions: If A is an m \\times n matrix, there are m equations and n variables.\nExample: \nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}\n\n\nVector \\mathbf{b} (Right-Hand Side Vector):\n\nDefinition: A column vector representing the constants from the right side of each equation.\nRole: Contains the target values that the linear combination of columns of A (scaled by \\mathbf{x}) should equal.\nDimensions: An m \\times 1 vector, corresponding to the number of equations.\nExample: \n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\n\nVector \\mathbf{x} (Solution Vector):\n\nDefinition: A column vector containing the variables of the system.\nRole: Represents the values of the variables that satisfy all equations in the system.\nDimensions: An n \\times 1 vector, where n is the number of variables.\nExample: \n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}"
  },
  {
    "objectID": "notes/w06/linear-systems.html#identifying-the-inputs",
    "href": "notes/w06/linear-systems.html#identifying-the-inputs",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Identifying the Inputs",
    "text": "Identifying the Inputs\nIn the equation A\\mathbf{x} = \\mathbf{b}:\n\nInputs: A and \\mathbf{b}\nOutput (Solution): \\mathbf{x}\n\n\n1. Matrix A as an Input\n\nPurpose: Defines the relationships between the variables in the system. Each row of A corresponds to an equation, and each column corresponds to a variable.\nExample Interpretation:\nConsider the matrix:\n\nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}\n\nThis represents the system of equations:\n\n\\begin{cases}\n1x_1 + 2x_2 = b_1 \\\\\n3x_1 + 4x_2 = b_2 \\\\\n\\end{cases}\n\n\n\n\n2. Vector \\mathbf{b} as an Input\n\nPurpose: Provides the constants against which the linear combinations of variables (as defined by A) are measured. Essentially, \\mathbf{b} represents the desired outcomes or targets for each equation.\nExample Interpretation:\nGiven:\n\n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\nThe system becomes:\n\n\\begin{cases}\n1x_1 + 2x_2 = 5 \\\\\n3x_1 + 4x_2 = 11 \\\\\n\\end{cases}"
  },
  {
    "objectID": "notes/w06/linear-systems.html#the-role-of-inputs-in-solving-amathbfx-mathbfb",
    "href": "notes/w06/linear-systems.html#the-role-of-inputs-in-solving-amathbfx-mathbfb",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "The Role of Inputs in Solving A\\mathbf{x} = \\mathbf{b}",
    "text": "The Role of Inputs in Solving A\\mathbf{x} = \\mathbf{b}\nThe inputs A and \\mathbf{b} determine the nature of the system and influence the methods used to find the solution \\mathbf{x}. Here’s how:\n\n1. System Properties Influenced by A and \\mathbf{b}\n\nUniqueness of Solution:\n\nIf A is invertible (i.e., \\det(A) \\neq 0 for square matrices), the system has a unique solution.\nIf A is singular (i.e., \\det(A) = 0), the system may have infinitely many solutions or no solution.\n\nConsistency:\n\nThe system is consistent if at least one solution exists.\nIt is inconsistent if no solution satisfies all equations simultaneously.\n\n\n\n\n2. Influence on Solution Methods\n\nDirect Methods:\n\nApplicable when A is well-conditioned and invertible.\nExamples include Gaussian elimination and matrix inversion.\n\nIterative Methods:\n\nUseful for large or sparse systems where direct methods are computationally expensive.\nExamples include Jacobi, Gauss-Seidel, and Conjugate Gradient methods.\n\nLeast Squares:\n\nEmployed when the system is overdetermined (more equations than variables) and no exact solution exists.\nSeeks to minimize the residual \\|\\mathbf{b} - A\\mathbf{x}\\|."
  },
  {
    "objectID": "notes/w06/linear-systems.html#a-practical-example",
    "href": "notes/w06/linear-systems.html#a-practical-example",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "A Practical Example",
    "text": "A Practical Example\nTo solidify the understanding of inputs in A\\mathbf{x} = \\mathbf{b}, let’s walk through a concrete example.\n\nThe Problem Setup\nConsider the following system of linear equations:\n\n\\begin{cases}\n1x_1 + 2x_2 = 5 \\\\\n3x_1 + 4x_2 = 11 \\\\\n\\end{cases}\n\nThis can be written in matrix form as:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\n\n\nStep 1: Define the Inputs\n\nCoefficient Matrix A:\n\nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}\n\nRight-Hand Side Vector \\mathbf{b}:\n\n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\n\n\n\nStep 2: Analyze the Inputs\n\nInvertibility of A:\nCompute the determinant of A:\n\n\\det(A) = (1)(4) - (2)(3) = 4 - 6 = -2 \\neq 0\n\nSince \\det(A) \\neq 0, A is invertible, and the system has a unique solution.\n\n\n\nStep 3: Solve for \\mathbf{x}\nUsing the inverse of A:\n\n\\mathbf{x} = A^{-1}\\mathbf{b}\n\nFirst, compute A^{-1}:\n\nA^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1 \\\\\n\\end{bmatrix} = \\frac{1}{-2} \\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n-2 & 1 \\\\\n1.5 & -0.5 \\\\\n\\end{bmatrix}\n\nNow, multiply A^{-1} by \\mathbf{b}:\n\n\\mathbf{x} = \\begin{bmatrix}\n-2 & 1 \\\\\n1.5 & -0.5 \\\\\n\\end{bmatrix} \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n(-2)(5) + (1)(11) \\\\\n(1.5)(5) + (-0.5)(11) \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n-10 + 11 \\\\\n7.5 - 5.5 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n\\end{bmatrix}\n\nSolution:\n\n\\mathbf{x} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n\\end{bmatrix} \\implies x_1 = 1, \\quad x_2 = 2\n\n\n\nStep 4: Interpretation of Inputs and Solution\n\nInputs A and \\mathbf{b}:\n\nDefine the system’s structure and target outcomes.\nDetermine the method used for solving (e.g., direct inversion due to invertibility).\n\nSolution \\mathbf{x}:\n\nRepresents the values that satisfy both equations simultaneously.\nIn this case, x_1 = 1 and x_2 = 2 are the unique values that make both equations true."
  },
  {
    "objectID": "notes/w06/linear-systems.html#geometric-interpretation",
    "href": "notes/w06/linear-systems.html#geometric-interpretation",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nVisualizing A\\mathbf{x} = \\mathbf{b} can enhance comprehension, especially in two dimensions.\n\nExample Visualization\nConsider the system:\n\n\\begin{cases}\n1x_1 + 2x_2 = 5 \\\\\n3x_1 + 4x_2 = 11 \\\\\n\\end{cases}\n\nGraphically, each equation represents a line in the x_1-x_2 plane.\n\nFirst Equation: 1x_1 + 2x_2 = 5\n\nSlope: -\\frac{1}{2}\nIntercepts: (5, 0) and (0, 2.5)\n\nSecond Equation: 3x_1 + 4x_2 = 11\n\nSlope: -\\frac{3}{4}\nIntercepts: (\\frac{11}{3}, 0) and (0, \\frac{11}{4})\n\n\nThe unique solution (1, 2) is the intersection point of these two lines.\n\n\nVisual Representation\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(-1, 4, 400)\n\nx2_eq1 = (5 - x1) / 2\n\nx2_eq2 = (11 - 3*x1) / 4\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x1, x2_eq1, label=r'$1x_1 + 2x_2 = 5$', color='blue')\nplt.plot(x1, x2_eq2, label=r'$3x_1 + 4x_2 = 11$', color='green')\n\nplt.plot(1, 2, 'ro', label=r'Solution $(1, 2)$')\n\nplt.xlim(-1, 4)\nplt.ylim(-1, 4)\n\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.xlabel(r'$x_1$', fontsize=12)\nplt.ylabel(r'$x_2$', fontsize=12)\nplt.title('Geometric Interpretation of $A\\mathbf{x} = \\mathbf{b}$', fontsize=14)\nplt.legend()\nplt.axhline(0, color='black', linewidth=1)\nplt.axvline(0, color='black', linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nBlue Line: Represents the equation 1x_1 + 2x_2 = 5.\nGreen Line: Represents the equation 3x_1 + 4x_2 = 11.\nRed Dot: Marks the unique solution (1, 2), where the two lines intersect.\n\nThis visualization underscores how the inputs A and \\mathbf{b} define the system’s structure and determine the solution’s existence and uniqueness."
  },
  {
    "objectID": "notes/w06/linear-systems.html#importance-of-understanding-inputs",
    "href": "notes/w06/linear-systems.html#importance-of-understanding-inputs",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Importance of Understanding Inputs",
    "text": "Importance of Understanding Inputs\nGrasping the role and nature of inputs in A\\mathbf{x} = \\mathbf{b} is pivotal for several reasons:\n\nSolution Strategy:\n\nKnowing whether A is invertible informs the choice of solution methods (e.g., using the inverse matrix, LU decomposition, etc.).\n\nNumerical Stability:\n\nThe properties of A (such as its condition number) influence the sensitivity of the solution to perturbations in A or \\mathbf{b}.\n\nApplicability to Real-World Problems:\n\nIn fields like engineering, economics, and computer science, accurately modeling problems as linear systems depends on correctly identifying A and \\mathbf{b}.\n\nPerformance Optimization:\n\nUnderstanding the size and sparsity of A can guide the selection of efficient algorithms for large-scale systems."
  },
  {
    "objectID": "notes/w06/linear-systems.html#conclusion",
    "href": "notes/w06/linear-systems.html#conclusion",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Conclusion",
    "text": "Conclusion\nIn the linear system A\\mathbf{x} = \\mathbf{b}:\n\nInputs:\n\nMatrix A: Defines the coefficients and relationships between variables.\nVector \\mathbf{b}: Specifies the target outcomes or constants for each equation.\n\nOutput:\n\nVector \\mathbf{x}: The solution that satisfies all equations in the system.\n\n\nRecognizing and comprehending the roles of A and \\mathbf{b} are fundamental for effectively solving linear systems, analyzing their properties, and applying them to diverse practical scenarios. Mastery of these concepts equips mathematicians, engineers, and scientists with the tools necessary to tackle complex problems with confidence and precision."
  },
  {
    "objectID": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html",
    "href": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html",
    "title": "Jacobi Method - Convergence Proof",
    "section": "",
    "text": "Theorem 2.10 (p. 107)\nIf the n \\times n matrix A is strictly diagonally dominant, then:\n\nA is a nonsingular matrix.\nFor every vector b and every starting guess, the Jacobi Method applied to A \\mathbf{x} = \\mathbf{b} converges to the (unique) solution."
  },
  {
    "objectID": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#diagonal-dominance-convergence-theorem",
    "href": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#diagonal-dominance-convergence-theorem",
    "title": "Jacobi Method - Convergence Proof",
    "section": "",
    "text": "Theorem 2.10 (p. 107)\nIf the n \\times n matrix A is strictly diagonally dominant, then:\n\nA is a nonsingular matrix.\nFor every vector b and every starting guess, the Jacobi Method applied to A \\mathbf{x} = \\mathbf{b} converges to the (unique) solution."
  },
  {
    "objectID": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#spectral-radius-convergence-theorem",
    "href": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#spectral-radius-convergence-theorem",
    "title": "Jacobi Method - Convergence Proof",
    "section": "Spectral Radius Convergence Theorem",
    "text": "Spectral Radius Convergence Theorem\nTheorem A.7 (p. 588)\nIf the n \\times n matrix A has spectral radius \\rho(A) &lt; 1, and \\mathbf{b} is arbitrary, then, for any vector \\mathbf{x}_0, the iteration \\mathbf{x}_{k+1} = A \\mathbf{x}_k + \\mathbf{b} converges. In fact, there exists a unique \\mathbf{x}, such that \\lim_{k \\to \\infty} \\mathbf{x}_k = \\mathbf{x}, and \\mathbf{x} = A \\mathbf{x} + \\mathbf{b}."
  },
  {
    "objectID": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#definitions",
    "href": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#definitions",
    "title": "Jacobi Method - Convergence Proof",
    "section": "Definitions",
    "text": "Definitions\n\nSpectral radius:\nThe spectral radius \\rho(A) of a square matrix A is the maximum magnitude of its eigenvalues.\nInfinity or max norm:\nFor a vector \\mathbf{x} \\in \\mathbb{R}^n, the infinity norm is \\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|."
  },
  {
    "objectID": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#proof",
    "href": "worksheets/jacobi-method-convergence/jacobi-method-convergence.html#proof",
    "title": "Jacobi Method - Convergence Proof",
    "section": "Proof",
    "text": "Proof\nRecall that the Jacobi Method for solving A \\mathbf{x} = \\mathbf{b} is\n\n\\mathbf{x}_{k+1} = -D^{-1}(L + U) \\mathbf{x}_k + D^{-1} \\mathbf{b},\n\nwhere\n\nA = L + D + U,\n\nL is the lower triangular part of A, D is the diagonal part of A, and U is the upper triangular part of A.\nWe will apply Theorem A.7 by showing that the spectral radius of -D^{-1}(L + U) is less than 1:\n\n\\rho(D^{-1}(L + U)) &lt; 1\n\nFor notational convenience, let R = L + U denote the non-diagonal part of the matrix A. Then we must show that \\rho(D^{-1}R) &lt; 1.\n\n\n\n\n\n\n1. Scaled Vector \\mathbf{v}:\n\n\n\nGiven any vector \\mathbf{x}, we can create a scaled version of \\mathbf{x}, say \\mathbf{v}, as \\mathbf{v} = \\frac{\\mathbf{x}}{c}. What value of c will guarantee that \\|\\mathbf{v}\\|_\\infty = 1?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo ensure \\|\\mathbf{v}\\|_\\infty = 1, define \\mathbf{v} = \\frac{\\mathbf{x}}{c}, where c is a scalar.\nThe infinity norm of \\mathbf{v} is:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} \\left| \\frac{x_i}{c} \\right|\n\nSet \\|\\mathbf{v}\\|_\\infty = 1, so:\n\n\\frac{\\max_{1 \\leq i \\leq n} |x_i|}{c} = 1\n\nSolve for c:\n\nc = \\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|\n\nThus, scaling \\mathbf{x} by c = \\|\\mathbf{x}\\|_\\infty guarantees \\|\\mathbf{v}\\|_\\infty = 1.\n\n\n\n\n\n\n\n\n\n2. Eigenvalue Analysis:\n\n\n\nLet \\lambda represent an arbitrary eigenvalue of D^{-1}R with corresponding eigenvector \\mathbf{v}. Then D^{-1}R \\mathbf{v} = \\lambda \\mathbf{v}, or R \\mathbf{v} = \\lambda D \\mathbf{v}.\nWhy?\nWe’ll look at each side of this equation in turn. Suppose we scale the eigenvector \\mathbf{v} such that \\|\\mathbf{v}\\|_\\infty = 1. Then |v_i| \\leq 1 for every index i, 1 \\leq i \\leq n, and |v_m| = 1 for at least one index m, 1 \\leq m \\leq n.\nUsing this index m, explain why the absolute value of the m-th row of R \\mathbf{v} is:\n\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo analyze the absolute value of the m-th row of R \\mathbf{v}, start with the eigenvalue equation:\n\nD^{-1} R \\mathbf{v} = \\lambda \\mathbf{v}\n\nMultiply through by D to rewrite it as:\n\nR \\mathbf{v} = \\lambda D \\mathbf{v}\n\nHere:\n\nR = L + U, where L is the strictly lower triangular part of A and U is the strictly upper triangular part of A.\nD is the diagonal part of A.\n\\mathbf{v} is an eigenvector scaled such that \\|\\mathbf{v}\\|_\\infty = 1, meaning |v_i| \\leq 1 for all i, and |v_m| = 1 for at least one m.\n\nThe m-th row of R is:\n\n\\begin{bmatrix}\nr_{m,1} & r_{m,2} & \\cdots & r_{m,m-1} & 0 & r_{m,m+1} & \\cdots & r_{m,n}\n\\end{bmatrix}\n\nMultiplying this row by the vector \\mathbf{v}, the m-th entry of R \\mathbf{v} is:\n\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1}^n r_{m,i} v_i\n\nSince r_{m,m} = 0 (as R = L + U excludes the diagonal), this simplifies to:\n\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1, i \\neq m}^n r_{m,i} v_i\n\n\n\n\n\n\n\n\n\n\n3. Scaling with D:\n\n\n\nNow, explain why the absolute value of the m-th row of \\lambda D \\mathbf{v} is |\\lambda| |d_{m,m}|.\n\n\n\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "reality-checks/index.html",
    "href": "reality-checks/index.html",
    "title": "REALITY CHECKS",
    "section": "",
    "text": "Reality Check 01\n\nStewart Platform Kinematics\n\n\n\nReality Check 05\n\nMotion Control in Computer-Aided Modeling"
  },
  {
    "objectID": "notes/w08/least-squares.html",
    "href": "notes/w08/least-squares.html",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "",
    "text": "The least squares solution minimizes the distance between the vector \\mathbf{b} and the column space of A. The column space of A, denoted \\mathrm{Col}(A), is equivalent to the span of the columns of A, written as \\mathrm{span}(A_1, A_2, \\dots, A_n), where A_1, A_2, \\dots, A_n are the columns of A.\nThis distance corresponds to the residual \\|\\mathbf{b} - \\mathbf{\\hat{b}}\\|, where \\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}} is the projection of \\mathbf{b} onto \\mathrm{Col}(A)."
  },
  {
    "objectID": "notes/w08/least-squares.html#overview",
    "href": "notes/w08/least-squares.html#overview",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "",
    "text": "The least squares solution minimizes the distance between the vector \\mathbf{b} and the column space of A. The column space of A, denoted \\mathrm{Col}(A), is equivalent to the span of the columns of A, written as \\mathrm{span}(A_1, A_2, \\dots, A_n), where A_1, A_2, \\dots, A_n are the columns of A.\nThis distance corresponds to the residual \\|\\mathbf{b} - \\mathbf{\\hat{b}}\\|, where \\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}} is the projection of \\mathbf{b} onto \\mathrm{Col}(A)."
  },
  {
    "objectID": "notes/w08/least-squares.html#visualizing-the-least-squares-solution",
    "href": "notes/w08/least-squares.html#visualizing-the-least-squares-solution",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "Visualizing the Least Squares Solution",
    "text": "Visualizing the Least Squares Solution\nThe figure below illustrates how the least squares solution \\mathbf{\\hat{x}} projects \\mathbf{b} onto \\mathrm{Col}(A), which is spanned by the columns of A. The vector \\mathbf{r} = \\mathbf{b} - \\mathbf{\\hat{b}} is orthogonal to \\mathrm{Col}(A).\n\n\n\\mathbf{b}: The target vector outside \\mathrm{Col}(A).\n\\mathbf{\\hat{b}}: The projection of \\mathbf{b} onto \\mathrm{Col}(A), calculated as \\mathbf{\\hat{b}} = A(A^\\top A)^{-1}A^\\top \\mathbf{b}.\n\\mathbf{r}: The residual vector, orthogonal to \\mathrm{Col}(A).\nA_1, A_2, \\dots, A_n: Basis vectors spanning \\mathrm{Col}(A)."
  },
  {
    "objectID": "notes/w08/least-squares.html#intuitive-walkthrough-of-the-proof",
    "href": "notes/w08/least-squares.html#intuitive-walkthrough-of-the-proof",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "Intuitive Walkthrough of the Proof",
    "text": "Intuitive Walkthrough of the Proof\n\n1. The Problem\nGiven a matrix A with columns A_1, A_2, \\dots, A_n and a vector \\mathbf{b}, the system A\\mathbf{x} = \\mathbf{b} is inconsistent if \\mathbf{b} does not lie in \\mathrm{Col}(A). The objective is to find \\mathbf{\\hat{x}}, which minimizes the distance between \\mathbf{b} and \\mathrm{Col}(A).\n\n\n2. Projection and Residual\nMinimizing the distance involves projecting \\mathbf{b} onto \\mathrm{Col}(A). The projection is denoted \\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}}. The residual \\mathbf{r} = \\mathbf{b} - \\mathbf{\\hat{b}} represents the difference, and the goal is to minimize \\|\\mathbf{r}\\|^2, which can be written as:\n\n\\|\\mathbf{r}\\|^2 = \\|\\mathbf{b} - A\\mathbf{\\hat{x}}\\|^2.\n\n\n\n3. Orthogonality Condition\nThe residual \\mathbf{r} is orthogonal to \\mathrm{Col}(A) when \\mathbf{\\hat{b}} is the best approximation of \\mathbf{b}. This condition is expressed as:\n\nA^\\top \\mathbf{r} = 0 \\quad \\text{or equivalently} \\quad A^\\top (\\mathbf{b} - A\\mathbf{\\hat{x}}) = 0.\n\n\n\n4. Normal Equations\nExpanding the orthogonality condition yields:\n\nA^\\top \\mathbf{b} - A^\\top A\\mathbf{\\hat{x}} = 0 \\quad \\Rightarrow \\quad A^\\top A\\mathbf{\\hat{x}} = A^\\top \\mathbf{b}.\n\nThis is the normal equation. Solving it provides \\mathbf{\\hat{x}}, the least squares solution.\n\n\n5. Computing \\mathbf{\\hat{x}}\nIf A^\\top A is invertible, the solution for \\mathbf{\\hat{x}} is:\n\n\\mathbf{\\hat{x}} = (A^\\top A)^{-1}A^\\top \\mathbf{b}.\n\n\n\n6. Geometric Interpretation\nThe projection \\mathbf{\\hat{b}} lies in \\mathrm{Col}(A), and the residual \\mathbf{r} = \\mathbf{b} - \\mathbf{\\hat{b}} is orthogonal to this space. This makes \\mathbf{\\hat{b}} the closest vector in \\mathrm{Col}(A) to \\mathbf{b}."
  },
  {
    "objectID": "notes/w08/least-squares.html#summary",
    "href": "notes/w08/least-squares.html#summary",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "Summary",
    "text": "Summary\nThe least squares method projects \\mathbf{b} onto \\mathrm{Col}(A), solving the equation:\n\nA^\\top A\\mathbf{\\hat{x}} = A^\\top \\mathbf{b}.\n\nThe solution is:\n\n\\mathbf{\\hat{x}} = (A^\\top A)^{-1}A^\\top \\mathbf{b}.\n\nThis projection minimizes the residual \\|\\mathbf{b} - \\mathbf{\\hat{b}}\\|, making it the best approximation of \\mathbf{b} within \\mathrm{Col}(A). The accompanying diagram visually demonstrates this projection and the orthogonal residual."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html",
    "href": "notes/w07/norms/taxicab-vector-norm.html",
    "title": "Taxicab Norm",
    "section": "",
    "text": "The Taxicab norm (also known as the Manhattan norm or \\ell_1-norm) is a way of measuring the “distance” of a vector in a grid-like space. Unlike the Euclidean norm, which measures the straight-line distance, the Taxicab norm sums the absolute values of a vector’s components. It is often used in scenarios where movement is constrained to axes, like navigating a city grid."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#overview",
    "href": "notes/w07/norms/taxicab-vector-norm.html#overview",
    "title": "Taxicab Norm",
    "section": "",
    "text": "The Taxicab norm (also known as the Manhattan norm or \\ell_1-norm) is a way of measuring the “distance” of a vector in a grid-like space. Unlike the Euclidean norm, which measures the straight-line distance, the Taxicab norm sums the absolute values of a vector’s components. It is often used in scenarios where movement is constrained to axes, like navigating a city grid."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#definition",
    "href": "notes/w07/norms/taxicab-vector-norm.html#definition",
    "title": "Taxicab Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the Taxicab norm is defined as:\n\n\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^n |v_i|\n\nThis measures the total “travel distance” required along the axes to reach the endpoint of the vector."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#properties",
    "href": "notes/w07/norms/taxicab-vector-norm.html#properties",
    "title": "Taxicab Norm",
    "section": "Properties",
    "text": "Properties\nThe Taxicab norm satisfies the following properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_1 \\geq 0, and \\|\\mathbf{v}\\|_1 = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_1 = |c| \\|\\mathbf{v}\\|_1.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_1 \\leq \\|\\mathbf{u}\\|_1 + \\|\\mathbf{v}\\|_1."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#examples",
    "href": "notes/w07/norms/taxicab-vector-norm.html#examples",
    "title": "Taxicab Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Example\nFor \\mathbf{v} = [3, -4]:\n\n\\|\\mathbf{v}\\|_1 = |3| + |-4| = 3 + 4 = 7\n\n\n\n2. 3D Example\nFor \\mathbf{v} = [1, -2, 3]:\n\n\\|\\mathbf{v}\\|_1 = |1| + |-2| + |3| = 1 + 2 + 3 = 6\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|"
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#applications",
    "href": "notes/w07/norms/taxicab-vector-norm.html#applications",
    "title": "Taxicab Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Urban Navigation\nThe Taxicab norm models distance in grid-based systems, such as city streets, where movement is restricted to horizontal and vertical directions.\n\n\n2. Machine Learning\nIn machine learning, the \\ell_1-norm is used as a regularization term (Lasso regression) to encourage sparsity in models.\n\n\n3. Optimization\nThe Taxicab norm is used in linear programming and optimization problems where constraints align with the \\ell_1-metric.\n\n\n4. Signal Processing\nThe \\ell_1-norm is used in compressed sensing and sparse recovery to find solutions with minimal non-zero components."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#visualization",
    "href": "notes/w07/norms/taxicab-vector-norm.html#visualization",
    "title": "Taxicab Norm",
    "section": "Visualization",
    "text": "Visualization\nIn 2D, the set of points at a fixed Taxicab norm distance from the origin forms a diamond shape (rotated square). For example, all points satisfying \\|\\mathbf{v}\\|_1 = 3 in \\mathbb{R}^2 would form the square:\n\n|x| + |y| = 3"
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#example-problem",
    "href": "notes/w07/norms/taxicab-vector-norm.html#example-problem",
    "title": "Taxicab Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the Taxicab norm of \\mathbf{v} = [-3, 4, -5].\n\nSolution:\n\nTake the absolute values of the components: |-3| = 3, |4| = 4, |-5| = 5.\nSum them: \\|\\mathbf{v}\\|_1 = 3 + 4 + 5 = 12."
  },
  {
    "objectID": "notes/w07/norms/taxicab-vector-norm.html#conclusion",
    "href": "notes/w07/norms/taxicab-vector-norm.html#conclusion",
    "title": "Taxicab Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe Taxicab norm is a useful measure of distance in grid-based systems and finds applications in various fields such as geometry, optimization, and machine learning. Its simplicity and intuitive interpretation make it a valuable tool for analyzing vector magnitudes under \\ell_1-metrics."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html",
    "href": "notes/w07/norms/infinity-norm-matrices.html",
    "title": "Infinity Norm for Matrices",
    "section": "",
    "text": "The infinity norm (or maximum row sum norm) of a matrix is a measure of its size, calculated as the maximum absolute row sum. It is often used to analyze the stability of numerical algorithms and the behavior of matrices in linear transformations. The infinity norm provides an upper bound on the effect a matrix can have on a vector in terms of row-wise contributions."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#overview",
    "href": "notes/w07/norms/infinity-norm-matrices.html#overview",
    "title": "Infinity Norm for Matrices",
    "section": "",
    "text": "The infinity norm (or maximum row sum norm) of a matrix is a measure of its size, calculated as the maximum absolute row sum. It is often used to analyze the stability of numerical algorithms and the behavior of matrices in linear transformations. The infinity norm provides an upper bound on the effect a matrix can have on a vector in terms of row-wise contributions."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#definition",
    "href": "notes/w07/norms/infinity-norm-matrices.html#definition",
    "title": "Infinity Norm for Matrices",
    "section": "Definition",
    "text": "Definition\nFor a matrix A = [a_{ij}] of size m \\times n, the infinity norm is defined as:\n\n\\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^n |a_{ij}|\n\nThis is equivalent to finding the largest sum of absolute values of the elements in any row of the matrix.\n\nGeometric Interpretation\nThe infinity norm quantifies the maximum influence of a row of the matrix when applied to a vector. It answers the question: “What is the largest total contribution of any single row?”"
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#properties",
    "href": "notes/w07/norms/infinity-norm-matrices.html#properties",
    "title": "Infinity Norm for Matrices",
    "section": "Properties",
    "text": "Properties\nThe infinity norm satisfies the following key properties:\n\nNon-negativity: \\|A\\|_\\infty \\geq 0, and \\|A\\|_\\infty = 0 if and only if A = 0 (a zero matrix).\nHomogeneity: For any scalar c, \\|cA\\|_\\infty = |c| \\|A\\|_\\infty.\nTriangle Inequality: \\|A + B\\|_\\infty \\leq \\|A\\|_\\infty + \\|B\\|_\\infty."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#example",
    "href": "notes/w07/norms/infinity-norm-matrices.html#example",
    "title": "Infinity Norm for Matrices",
    "section": "Example",
    "text": "Example\nGiven a matrix A:\n\nA = \\begin{pmatrix}\n1 & -2 & 3 \\\\\n-4 & 5 & -6 \\\\\n7 & -8 & 9\n\\end{pmatrix}\n\n\nCompute the row sums:\n\nRow 1: |1| + |-2| + |3| = 1 + 2 + 3 = 6,\nRow 2: |-4| + |5| + |-6| = 4 + 5 + 6 = 15,\nRow 3: |7| + |-8| + |9| = 7 + 8 + 9 = 24.\n\nFind the maximum row sum: \n\\|A\\|_\\infty = \\max(6, 15, 24) = 24\n\n\nThus, the infinity norm of A is 24."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#applications",
    "href": "notes/w07/norms/infinity-norm-matrices.html#applications",
    "title": "Infinity Norm for Matrices",
    "section": "Applications",
    "text": "Applications\n\nNumerical Stability:\n\nThe infinity norm is often used to measure the sensitivity of solutions to changes in the matrix.\n\nCondition Numbers:\n\nIt contributes to the calculation of matrix condition numbers, which measure how close a matrix is to being singular.\n\nError Analysis:\n\nThe infinity norm is used to bound errors in iterative algorithms and numerical solutions to matrix equations.\n\nOptimization:\n\nIt appears in optimization problems where constraints are defined by row-wise contributions."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#visualization",
    "href": "notes/w07/norms/infinity-norm-matrices.html#visualization",
    "title": "Infinity Norm for Matrices",
    "section": "Visualization",
    "text": "Visualization\nFor a matrix, the infinity norm represents the largest “row weight” when summing all the absolute values of the row’s entries. It gives insight into the matrix’s effect along the rows."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#example-problem",
    "href": "notes/w07/norms/infinity-norm-matrices.html#example-problem",
    "title": "Infinity Norm for Matrices",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the infinity norm of:\n\nA = \\begin{pmatrix}\n2 & -1 \\\\\n-3 & 4\n\\end{pmatrix}\n\n\nSolution:\n\nCompute the row sums:\n\nRow 1: |2| + |-1| = 2 + 1 = 3,\nRow 2: |-3| + |4| = 3 + 4 = 7.\n\nFind the maximum row sum: \n\\|A\\|_\\infty = \\max(3, 7) = 7\n\n\nThus, \\|A\\|_\\infty = 7."
  },
  {
    "objectID": "notes/w07/norms/infinity-norm-matrices.html#conclusion",
    "href": "notes/w07/norms/infinity-norm-matrices.html#conclusion",
    "title": "Infinity Norm for Matrices",
    "section": "Conclusion",
    "text": "Conclusion\nThe infinity norm provides a simple yet powerful way to measure the row-wise magnitude of a matrix. Its computational simplicity and relevance to numerical analysis make it an essential tool in matrix computations."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html",
    "href": "notes/w07/norms/euclidean-vector-norm.html",
    "title": "Euclidean Vector Norm",
    "section": "",
    "text": "The Euclidean vector norm (or 2-norm) is a measure of the magnitude (or length) of a vector in Euclidean space. It is widely used in mathematics, physics, and engineering for calculating distances, measuring error, and normalizing vectors. This note covers the definition, properties, computation, and applications of the Euclidean norm."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#overview",
    "href": "notes/w07/norms/euclidean-vector-norm.html#overview",
    "title": "Euclidean Vector Norm",
    "section": "",
    "text": "The Euclidean vector norm (or 2-norm) is a measure of the magnitude (or length) of a vector in Euclidean space. It is widely used in mathematics, physics, and engineering for calculating distances, measuring error, and normalizing vectors. This note covers the definition, properties, computation, and applications of the Euclidean norm."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#definition",
    "href": "notes/w07/norms/euclidean-vector-norm.html#definition",
    "title": "Euclidean Vector Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the Euclidean norm is defined as:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}\n\nThis formula calculates the straight-line distance from the origin to the point represented by \\mathbf{v} in n-dimensional space."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#properties",
    "href": "notes/w07/norms/euclidean-vector-norm.html#properties",
    "title": "Euclidean Vector Norm",
    "section": "Properties",
    "text": "Properties\nThe Euclidean norm satisfies several key properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_2 \\geq 0, and \\|\\mathbf{v}\\|_2 = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_2 = |c| \\|\\mathbf{v}\\|_2.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_2 \\leq \\|\\mathbf{u}\\|_2 + \\|\\mathbf{v}\\|_2."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#examples",
    "href": "notes/w07/norms/euclidean-vector-norm.html#examples",
    "title": "Euclidean Vector Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Vector\nFor \\mathbf{v} = [3, 4]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5\n\n\n\n2. 3D Vector\nFor \\mathbf{v} = [1, 2, 3]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}"
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#applications",
    "href": "notes/w07/norms/euclidean-vector-norm.html#applications",
    "title": "Euclidean Vector Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Measuring Distance\nThe Euclidean norm is used to calculate the distance between two points \\mathbf{u} and \\mathbf{v} in space:\n\n\\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}\n\n\n\n2. Normalizing Vectors\nTo convert a vector to unit length, divide it by its Euclidean norm:\n\n\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2}\n\n\n\n3. Error Measurement\nIn numerical analysis, the Euclidean norm measures the error or residual of a solution.\n\n\n4. Machine Learning\nThe Euclidean norm underpins metrics like the Euclidean distance, used in clustering and regression."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#visualization",
    "href": "notes/w07/norms/euclidean-vector-norm.html#visualization",
    "title": "Euclidean Vector Norm",
    "section": "Visualization",
    "text": "Visualization\nIn two-dimensional space, the Euclidean norm corresponds to the length of the hypotenuse of a right triangle formed by the vector’s components. This is equivalent to the straight-line distance from the origin to the point represented by the vector."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#example-problem",
    "href": "notes/w07/norms/euclidean-vector-norm.html#example-problem",
    "title": "Euclidean Vector Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the Euclidean norm of \\mathbf{v} = [2, -3, 6].\n\nSolution:\n\nSquare each component: 2^2 = 4, (-3)^2 = 9, 6^2 = 36.\nSum the squares: 4 + 9 + 36 = 49.\nTake the square root: \\|\\mathbf{v}\\|_2 = \\sqrt{49} = 7."
  },
  {
    "objectID": "notes/w07/norms/euclidean-vector-norm.html#conclusion",
    "href": "notes/w07/norms/euclidean-vector-norm.html#conclusion",
    "title": "Euclidean Vector Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe Euclidean vector norm is an essential concept in mathematics and its applications, providing a straightforward measure of vector magnitude. Its simplicity and intuitive geometric interpretation make it a fundamental tool across diverse fields such as physics, engineering, and machine learning."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "",
    "text": "In the realm of numerical computations, precision and accuracy are paramount. However, various phenomena can undermine these qualities, leading to erroneous results. One such phenomenon is swamping, a term that describes the masking or overwhelming of smaller errors by larger ones, making it difficult to detect or mitigate the underlying issues. Understanding swamping is essential for developing robust numerical algorithms and ensuring the reliability of computational results."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html#overview",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "",
    "text": "In the realm of numerical computations, precision and accuracy are paramount. However, various phenomena can undermine these qualities, leading to erroneous results. One such phenomenon is swamping, a term that describes the masking or overwhelming of smaller errors by larger ones, making it difficult to detect or mitigate the underlying issues. Understanding swamping is essential for developing robust numerical algorithms and ensuring the reliability of computational results."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html#what-is-swamping",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html#what-is-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "What is Swamping?",
    "text": "What is Swamping?\nSwamping refers to the scenario where smaller errors or perturbations in a numerical computation are obscured or dominated by larger errors. This can occur in various contexts, such as solving linear systems, eigenvalue computations, or iterative algorithms. When swamping happens, it becomes challenging to identify and correct minor inaccuracies, potentially leading to significant deviations in the final outcome.\nMathematically, consider a computation where multiple sources of errors are present. If one error component is significantly larger than others, it can overshadow the smaller ones, effectively “swamping” them. This makes it difficult to assess the cumulative effect of all errors accurately."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html#causes-of-swamping",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html#causes-of-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "Causes of Swamping",
    "text": "Causes of Swamping\nSwamping can arise from several factors in numerical computations:\n\nFinite Precision Arithmetic:\n\nRound-Off Errors: In floating-point computations, numbers are represented with limited precision. Repeated arithmetic operations can accumulate round-off errors, where smaller errors become overshadowed by larger ones.\nCancellation Errors: Subtracting nearly equal numbers can result in significant loss of precision, amplifying existing errors.\n\nIll-Conditioned Systems:\n\nSystems with a high condition number are sensitive to perturbations. Small errors in input data or intermediate computations can lead to large errors in the solution, causing swamping.\n\nAlgorithmic Instabilities:\n\nCertain numerical algorithms may amplify specific error components due to their inherent design, leading to swamping of other errors.\n\nData Noise:\n\nIn data-driven computations, high levels of noise can mask underlying signals, making it difficult to detect subtle patterns or trends."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html#implications-of-swamping",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html#implications-of-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "Implications of Swamping",
    "text": "Implications of Swamping\nSwamping has several critical implications for numerical computations:\n\nReduced Accuracy: The dominance of larger errors can significantly reduce the overall accuracy of the computation.\nError Propagation: Swamping can lead to uncontrolled error propagation, where inaccuracies at one stage of computation affect subsequent stages.\nAlgorithm Reliability: Algorithms susceptible to swamping may produce unreliable results, undermining their applicability in sensitive applications.\nDifficulty in Debugging: Identifying and isolating the sources of errors becomes challenging when swamping occurs, complicating the debugging process."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html#detecting-and-mitigating-swamping",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html#detecting-and-mitigating-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "Detecting and Mitigating Swamping",
    "text": "Detecting and Mitigating Swamping\nEffective detection and mitigation strategies are essential to manage swamping in numerical computations:\n\n1. Error Analysis:\n\nRelative and Absolute Errors: Monitor both relative and absolute errors to identify when smaller errors are being overshadowed.\nResidual Analysis: In solving linear systems, analyze the residual \\mathbf{r} = \\mathbf{b} - A\\mathbf{x} to assess the accuracy of the solution.\n\n\n\n2. Condition Number Assessment:\n\nCompute Condition Numbers: Evaluate the condition number of matrices involved using appropriate norms (e.g., Infinity Norm) to gauge sensitivity.\nWell-Conditioned vs. Ill-Conditioned: Prefer algorithms and formulations that minimize the condition number to reduce susceptibility to swamping.\n\n\n\n3. Algorithm Selection and Improvement:\n\nStable Algorithms: Choose numerical methods known for their stability and resistance to error amplification (e.g., using QR decomposition over Gaussian elimination in certain cases).\nPivoting Techniques: Implement pivoting strategies in matrix factorizations to enhance numerical stability.\n\n\n\n4. Precision Management:\n\nHigher Precision Arithmetic: Utilize higher precision data types (e.g., double-precision instead of single-precision) to minimize round-off and cancellation errors.\nAdaptive Precision: Dynamically adjust the precision based on the sensitivity of the computation stages.\n\n\n\n5. Regularization Techniques:\n\nTikhonov Regularization: Introduce regularization terms to stabilize solutions, especially in ill-posed problems.\nNoise Filtering: Apply filtering techniques to data to reduce noise levels and prevent swamping of subtle signals."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/swamping.html#a-practical-example",
    "href": "notes/w07/errors-analysis-linear-systems/swamping.html#a-practical-example",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "A Practical Example",
    "text": "A Practical Example\nTo illustrate swamping, let’s consider the problem of solving a linear system using Gaussian elimination without pivoting, which can be susceptible to error amplification in ill-conditioned systems.\n\nThe Problem Setup\nConsider the system:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\nA = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 1.0001 \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix}\n2 \\\\\n2.0001 \\\\\n\\end{bmatrix}\n\nThe true solution is:\n\n\\mathbf{x} = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}\n\n\n\nStep 1: Compute the Condition Number \\kappa_\\infty(A)\nUsing the Infinity Norm:\n\n\\|A\\|_\\infty = \\max \\left\\{ |1| + |1|, \\ |1| + |1.0001| \\right\\} = \\max \\{2, 2.0001\\} = 2.0001\n\nCompute A^{-1}:\n\n\\det(A) = (1)(1.0001) - (1)(1) = 0.0001\n\n\nA^{-1} = \\frac{1}{0.0001} \\begin{bmatrix}\n1.0001 & -1 \\\\\n-1 & 1 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n10001 & -10000 \\\\\n-10000 & 10000 \\\\\n\\end{bmatrix}\n\n\n\\|A^{-1}\\|_\\infty = \\max \\left\\{ |10001| + |-10000|, \\ |-10000| + |10000| \\right\\} = \\max \\{20001, 20000\\} = 20001\n\nThus,\n\n\\kappa_\\infty(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 2.0001 \\times 20001 \\approx 40004\n\nA condition number of approximately 40004 indicates that the matrix A is ill-conditioned, making the system highly sensitive to perturbations.\n\n\nStep 2: Solve the System Using Gaussian Elimination Without Pivoting\nPerforming Gaussian elimination:\n\nFirst Pivot: The element a_{11} = 1 is used to eliminate the first entry in the second row.\nElimination Step:\n\\text{Multiplier} = \\frac{a_{21}}{a_{11}} = \\frac{1}{1} = 1\nUpdate the second row:\na_{22}' = a_{22} - \\text{Multiplier} \\times a_{12} = 1.0001 - 1 \\times 1 = 0.0001\nb_2' = b_2 - \\text{Multiplier} \\times b_1 = 2.0001 - 1 \\times 2 = 0.0001\nBack Substitution:\nx_2 = \\frac{b_2'}{a_{22}'} = \\frac{0.0001}{0.0001} = 1\nx_1 = \\frac{b_1 - a_{12}x_2}{a_{11}} = \\frac{2 - 1 \\times 1}{1} = 1\n\nThe computed solution is \\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, which matches the true solution. However, this accuracy is contingent on the precision of computations and the absence of rounding errors.\n\n\nStep 3: Introduce Perturbations to Simulate Swamping\nNow, let’s introduce a more realistic perturbation that simulates how a computer might round off numbers during computations.\n\nAnalogy: Computer Rounding Leading to Swamping\nComputers represent numbers with finite precision, typically using floating-point arithmetic. Suppose during calculations, the computer rounds the number 2.0001 to 2 due to limited precision. This seemingly minor adjustment can have a drastic impact on the solution.\n\nPerturbed Equation 2:\nx + 1.0001y = 2\n(Notice the right-hand side changed from 2.0001 to 2 due to rounding)\n\nThis small change is akin to a computer rounding 2.0001 down to 2, illustrating how precision limitations can lead to significant deviations.\n\n\n\nStep 4: Solving the Perturbed System\nWith the perturbed equation, the system becomes:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\n\\mathbf{b} = \\begin{bmatrix}\n2 \\\\\n2 \\\\\n\\end{bmatrix}\n\nReapplying Gaussian elimination:\n\nElimination Step:\n\\text{Multiplier} = \\frac{a_{21}}{a_{11}} = \\frac{1}{1} = 1\nUpdate the second row:\na_{22}' = a_{22} - \\text{Multiplier} \\times a_{12} = 1.0001 - 1 \\times 1 = 0.0001\nb_2' = b_2 - \\text{Multiplier} \\times b_1 = 2 - 1 \\times 2 = 0\nBack Substitution:\nx_2 = \\frac{b_2'}{a_{22}'} = \\frac{0}{0.0001} = 0\nx_1 = \\frac{b_1 - a_{12}x_2}{a_{11}} = \\frac{2 - 1 \\times 0}{1} = 2\n\nThe computed solution is \\mathbf{x} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}, which deviates significantly from the true solution \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n\n\nStep 5: Analyzing the Impact\n\nOriginal Solution: (x, y) = (1, 1)\nAfter Rounding Error: (x, y) = (2, 0)\n\nObservation:\nA minuscule rounding error in the constant term of the second equation (from 2.0001 to 2) caused the solution to shift dramatically from (1, 1) to (2, 0).\n\n\nStep 6: Interpretation\nThe small perturbation in \\mathbf{b} led to a substantial error in the solution, illustrating how swamping can occur in ill-conditioned systems. The large condition number amplified the minor change in \\mathbf{b}, resulting in a significant deviation in \\mathbf{x}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html",
    "title": "Relative Forward Error in Linear Systems",
    "section": "",
    "text": "The relative forward error measures the ratio of the forward error to the norm of the true solution \\mathbf{x}. This concept provides a normalized metric for quantifying the error relative to the magnitude of the true solution, making it particularly useful for comparing errors across solutions of different scales.\nThe relative forward error is defined as:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of a vector."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#overview",
    "title": "Relative Forward Error in Linear Systems",
    "section": "",
    "text": "The relative forward error measures the ratio of the forward error to the norm of the true solution \\mathbf{x}. This concept provides a normalized metric for quantifying the error relative to the magnitude of the true solution, making it particularly useful for comparing errors across solutions of different scales.\nThe relative forward error is defined as:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of a vector."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#what-relative-forward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#what-relative-forward-error-represents",
    "title": "Relative Forward Error in Linear Systems",
    "section": "What Relative Forward Error Represents",
    "text": "What Relative Forward Error Represents\n\nNormalized Accuracy:\n\nThe relative forward error quantifies the error in the approximate solution relative to the size of the true solution.\n\nScale Invariance:\n\nBy dividing the forward error by \\|\\mathbf{x}\\|_\\infty, the metric becomes independent of the magnitude of \\mathbf{x}, allowing comparisons across systems of varying scales.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then:\n\n\\text{RFE} = 0"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#why-relative-forward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#why-relative-forward-error-matters",
    "title": "Relative Forward Error in Linear Systems",
    "section": "Why Relative Forward Error Matters",
    "text": "Why Relative Forward Error Matters\n\nComparing Errors Across Scales:\n\nFor large-scale systems, the absolute forward error may not be meaningful. The relative forward error provides a better sense of the proportionate error.\n\nAssessing Numerical Stability:\n\nA small relative forward error suggests that the numerical method produces a solution that is proportionately close to the true solution.\n\nImproving Computation Reliability:\n\nBy minimizing the relative forward error, algorithms can be optimized for consistent accuracy."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#example",
    "title": "Relative Forward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\nStep 1: Verify the True Solution\nCheck that \\mathbf{x} satisfies A\\mathbf{x} = \\mathbf{b}:\n\nA\\mathbf{x} = \\begin{bmatrix} 1 \\times 2 + 1 \\times 1 \\\\ 3 \\times 2 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\mathbf{b}\n\n\n\nStep 2: Compute the Forward Error\nCompute the difference between \\mathbf{x} and \\mathbf{x_a}:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 - 1 \\\\ 1 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nThe forward error is the infinity norm of \\mathbf{e}:\n\n\\text{FE} = \\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\n\n\nStep 3: Compute the Relative Forward Error\nCompute the norm of the true solution \\mathbf{x}:\n\n\\|\\mathbf{x}\\|_\\infty = \\max(|2|, |1|) = 2\n\nThe relative forward error is:\n\n\\text{RFE} = \\frac{\\|\\mathbf{e}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} = \\frac{2}{2} = 1\n\n\n\nInterpretation\n\nThe relative forward error indicates that the forward error is equal to the size of the largest component of the true solution."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/relative-forward-error.html#conclusion",
    "title": "Relative Forward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nRelative Forward Error Significance:\n\nThe relative forward error normalizes the forward error, making it meaningful regardless of the scale of the true solution.\n\nPractical Implications:\n\nA small relative forward error indicates that the numerical method is proportionately accurate, ensuring reliability across systems of varying magnitudes."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/index.html",
    "href": "notes/w07/errors-analysis-linear-systems/index.html",
    "title": "LINEAR SYSTEM ERROR ANALYSIS",
    "section": "",
    "text": "Residual\nBackward Error\nRelative Backward Error\nForward Error\nRelative Forward Error\nError Magnification Factor (EMF)\nCondition Number\nSwamping"
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "",
    "text": "The Jacobi Method is a type of fixed-point iteration specifically designed for solving systems of linear equations. The approach involves rewriting each equation to solve for one variable at a time and iterating over these equations to approximate the solution.\nFor a system of equations A\\mathbf{x} = \\mathbf{b}, the Jacobi Method works by isolating the i-th variable in the i-th equation, updating its value based on the other variables’ values from the previous iteration."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html#overview",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html#overview",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "",
    "text": "The Jacobi Method is a type of fixed-point iteration specifically designed for solving systems of linear equations. The approach involves rewriting each equation to solve for one variable at a time and iterating over these equations to approximate the solution.\nFor a system of equations A\\mathbf{x} = \\mathbf{b}, the Jacobi Method works by isolating the i-th variable in the i-th equation, updating its value based on the other variables’ values from the previous iteration."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html#the-jacobi-method",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html#the-jacobi-method",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "The Jacobi Method",
    "text": "The Jacobi Method\nConsider the system of equations:\n\nA\\mathbf{x} = \\mathbf{b}\n\nLet A be decomposed into three parts:\n\nD: The diagonal components of A.\nL: The strictly lower triangular components of A.\nU: The strictly upper triangular components of A.\n\nThus,\n\nA = D + L + U\n\nRewriting A\\mathbf{x} = \\mathbf{b}, we have:\n\nD\\mathbf{x} = \\mathbf{b} - (L + U)\\mathbf{x}\n\nSolving for \\mathbf{x}:\n\n\\mathbf{x} = D^{-1} \\left( \\mathbf{b} - (L + U)\\mathbf{x} \\right)\n\nThe Jacobi Method then iteratively updates the values of \\mathbf{x} as:\n\n\\mathbf{x}^{(k+1)} = D^{-1} \\left( \\mathbf{b} - (L + U)\\mathbf{x}^{(k)} \\right)\n\nwhere k denotes the iteration step."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html#example",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html#example",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Example",
    "text": "Example\n\nSystem of Equations\nConsider the system:\n\n3u + v = 5, \\quad u + 2v = 5\n\n\n\nStep 1: Rearrange Equations\nRewrite each equation to isolate the variables:\n\nu = \\frac{5 - v}{3}, \\quad v = \\frac{5 - u}{2}\n\n\n\nStep 2: Iterative Updates\nStart with an initial guess \\mathbf{x}^{(0)} = \\begin{bmatrix} u_0 \\\\ v_0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}. The iterations proceed as follows:\n\nIteration 1 (k = 1):\n\nu_1 = \\frac{5 - v_0}{3} = \\frac{5 - 0}{3} = \\frac{5}{3}, \\quad v_1 = \\frac{5 - u_0}{2} = \\frac{5 - 0}{2} = \\frac{5}{2}\n\nThus, \\mathbf{x}^{(1)} = \\begin{bmatrix} \\frac{5}{3} \\\\ \\frac{5}{2} \\end{bmatrix}.\nIteration 2 (k = 2):\n\nu_2 = \\frac{5 - v_1}{3} = \\frac{5 - \\frac{5}{2}}{3} = \\frac{5}{6}, \\quad v_2 = \\frac{5 - u_1}{2} = \\frac{5 - \\frac{5}{3}}{2} = \\frac{5}{3}\n\nThus, \\mathbf{x}^{(2)} = \\begin{bmatrix} \\frac{5}{6} \\\\ \\frac{5}{3} \\end{bmatrix}.\nIteration 3 (k = 3):\n\nu_3 = \\frac{5 - v_2}{3} = \\frac{5 - \\frac{5}{3}}{3} = \\frac{10}{9}, \\quad v_3 = \\frac{5 - u_2}{2} = \\frac{5 - \\frac{5}{6}}{2} = \\frac{25}{12}\n\nThus, \\mathbf{x}^{(3)} = \\begin{bmatrix} \\frac{10}{9} \\\\ \\frac{25}{12} \\end{bmatrix}.\n\n\n\nConvergence\nThe iterative updates show convergence to the solution \\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html#convergence-conditions",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html#convergence-conditions",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Convergence Conditions",
    "text": "Convergence Conditions\nThe Jacobi Method converges if the matrix A is strictly diagonally dominant, meaning:\n\n|a_{ii}| &gt; \\sum_{j \\neq i} |a_{ij}|\n\nfor all rows i. Alternatively, if A is symmetric positive definite, the Jacobi Method is also guaranteed to converge."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html#limitations",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html#limitations",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Limitations",
    "text": "Limitations\nIn cases where A is not strictly diagonally dominant or symmetric positive definite, the Jacobi Method may fail to converge.\n\nExample of Divergence\nConsider the system:\n\nu + 2v = 5, \\quad 3u + v = 5\n\nRewriting:\n\nu = 5 - 2v, \\quad v = 5 - 3u\n\nStarting with \\mathbf{x}^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, the iterations diverge:\n\nIteration 1 (k = 1):\n\nu_1 = 5 - 2v_0 = 5, \\quad v_1 = 5 - 3u_0 = 5\n\n\\mathbf{x}^{(1)} = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix}.\nIteration 2 (k = 2): \nu_2 = 5 - 2v_1 = -5, \\quad v_2 = 5 - 3u_1 = -10\n \\mathbf{x}^{(2)} = \\begin{bmatrix} -5 \\\\ -10 \\end{bmatrix}\n\nThe divergence occurs because the matrix is not strictly diagonally dominant."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/jacobi-method.html#summary",
    "href": "notes/w07/ax-b-iterative-methods/jacobi-method.html#summary",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Summary",
    "text": "Summary\nThe Jacobi Method is a simple and effective iterative technique for solving linear systems, particularly when the system matrix is strictly diagonally dominant or symmetric positive definite. However, it is sensitive to the properties of the matrix A and may not converge for all systems."
  },
  {
    "objectID": "notes/w07/2-3.html",
    "href": "notes/w07/2-3.html",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "",
    "text": "When solving linear systems of the form A\\mathbf{x} = \\mathbf{b}, two significant issues may arise:\n\nControllable Errors: Errors due to computational methods, which we can manage or minimize.\nUncontrollable Errors: Errors inherent to the problem’s nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving A\\mathbf{x} = \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/2-3.html#overview",
    "href": "notes/w07/2-3.html#overview",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "",
    "text": "When solving linear systems of the form A\\mathbf{x} = \\mathbf{b}, two significant issues may arise:\n\nControllable Errors: Errors due to computational methods, which we can manage or minimize.\nUncontrollable Errors: Errors inherent to the problem’s nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving A\\mathbf{x} = \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/2-3.html#vector-norms",
    "href": "notes/w07/2-3.html#vector-norms",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Vector Norms",
    "text": "Vector Norms\nA norm is a function that assigns a non-negative length or size to vectors in a vector space. Norms help measure the magnitude of vectors, which is essential in analyzing algorithms and numerical stability.\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n]^T, common norms include:\n\n1. \\ell_2-Norm (Euclidean Norm)\nMeasures the straight-line distance from the origin to the point \\mathbf{v}:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n\n\n\n2. \\ell_1-Norm (Taxicab or Manhattan Norm)\nSums the absolute values of the vector components:\n\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\dots + |v_n|\n\n\n\n3. \\ell_\\infty-Norm (Maximum or Infinity Norm)\nTakes the maximum absolute value among the components:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|\n\nAlso expressed as:\n\n\\|\\mathbf{v}\\|_\\infty = \\lim_{p \\to \\infty} \\left( \\sum_{i=1}^n |v_i|^p \\right)^{1/p}."
  },
  {
    "objectID": "notes/w07/2-3.html#matrix-norms",
    "href": "notes/w07/2-3.html#matrix-norms",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Matrix Norms",
    "text": "Matrix Norms\nMatrix norms measure the “size” of matrices and help understand how matrices affect vector norms when used as linear transformations.\nCommon matrix norms include:\n\n1. Spectral Norm (\\|A\\|_2)\nInvolves the largest singular value of A:\n\n\\|A\\|_2 = \\sqrt{\\lambda_{\\text{max}}(A^TA)},\n\nwhere \\lambda_{\\text{max}} is the largest eigenvalue of A^TA.\n\n\n2. Maximum Column Sum Norm (\\|A\\|_1)\nThe largest sum of absolute values in any column:\n\n\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|\n\n\n\n3. Maximum Row Sum Norm (\\|A\\|_\\infty)\nThe largest sum of absolute values in any row:\n\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|"
  },
  {
    "objectID": "notes/w07/2-3.html#the-matrix-infinity-norm-and-row-sums",
    "href": "notes/w07/2-3.html#the-matrix-infinity-norm-and-row-sums",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "The Matrix Infinity Norm and Row Sums",
    "text": "The Matrix Infinity Norm and Row Sums\n\nDefinition of \\|A\\|_\\infty\nFormally defined as:\n\n\\|A\\|_\\infty = \\sup_{\\mathbf{x} \\neq 0} \\frac{\\|A\\mathbf{x}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} = \\sup_{\\|\\mathbf{x}\\|_\\infty = 1} \\|A\\mathbf{x}\\|_\\infty\n\n\n\nKey Insight\n\nThe infinity norm of A is equal to the maximum row sum of A.\nIt represents the maximum effect A can have on any vector \\mathbf{x} with \\|\\mathbf{x}\\|_\\infty = 1."
  },
  {
    "objectID": "notes/w07/2-3.html#proof-a_infty-maximum-row-sum-of-a",
    "href": "notes/w07/2-3.html#proof-a_infty-maximum-row-sum-of-a",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Proof: \\|A\\|_\\infty = Maximum Row Sum of A",
    "text": "Proof: \\|A\\|_\\infty = Maximum Row Sum of A\nFor matrix A:\n\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|.\n\n\nExample\nConsider:\n\nA = \\begin{bmatrix}\n-1 & -2 & 3 \\\\\n3 & -4 & -5 \\\\\n-2 & 3 & -4\n\\end{bmatrix}\n\n\nCompute Row Sums\n\nRow 1: |-1| + |-2| + |3| = 6\nRow 2: |3| + |-4| + |-5| = 12\nRow 3: |-2| + |3| + |-4| = 9\n\nMaximum Row Sum: 12 (Row 2)\nTherefore, \\|A\\|_\\infty = 12.\n\n\n\nChoosing \\mathbf{\\hat{x}}\nTo achieve \\|A\\mathbf{\\hat{x}}\\|_\\infty = \\|A\\|_\\infty, select \\mathbf{\\hat{x}} with \\|\\mathbf{\\hat{x}}\\|_\\infty = 1 that aligns with the signs of Row 2:\n\n\\mathbf{\\hat{x}} = \\begin{bmatrix}\n1 \\\\\n-1 \\\\\n-1\n\\end{bmatrix}\n\n\nCompute A\\mathbf{\\hat{x}}:\n\nA\\mathbf{\\hat{x}} = \\begin{bmatrix}\n-1(1) + -2(-1) + 3(-1) \\\\\n3(1) + -4(-1) + -5(-1) \\\\\n-2(1) + 3(-1) + -4(-1)\n\\end{bmatrix} = \\begin{bmatrix}\n-2 \\\\\n12 \\\\\n-1\n\\end{bmatrix}\n\n\n\nCompute \\|A\\mathbf{\\hat{x}}\\|_\\infty:\n\n\\|A\\mathbf{\\hat{x}}\\|_\\infty = \\max(|-2|, |12|, |-1|) = 12 = \\|A\\|_\\infty\n\n\n\n\nConclusion\nIt is not possible to find \\mathbf{\\hat{x}} such that \\|A\\mathbf{\\hat{x}}\\|_\\infty &gt; \\|A\\|_\\infty when \\|\\mathbf{\\hat{x}}\\|_\\infty = 1, because \\|A\\|_\\infty is the supremum of \\|A\\mathbf{x}\\|_\\infty over all such \\mathbf{x}."
  },
  {
    "objectID": "notes/w07/2-3.html#error-analysis-and-condition-numbers",
    "href": "notes/w07/2-3.html#error-analysis-and-condition-numbers",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Error Analysis and Condition Numbers",
    "text": "Error Analysis and Condition Numbers\n\nWhy Care About Errors in A\\mathbf{x} = \\mathbf{b}?\nWhen solving A\\mathbf{x} = \\mathbf{b}, understanding errors helps improve numerical accuracy and stability.\nLet \\mathbf{x_a} be an approximate solution to A\\mathbf{x} = \\mathbf{b}, meaning:\n\nA\\mathbf{x_a} \\neq \\mathbf{b}\n\nWe quantify errors to assess the accuracy of \\mathbf{x_a} and evaluate the impact of approximations.\n\n\nDefinitions\n\nResidual: The difference between \\mathbf{b} and A\\mathbf{x_a}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nBackward Error (BE): Measures the infinity norm of the residual:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nRelative Backward Error (RBE): Normalizes the backward error relative to \\mathbf{b}:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nForward Error (FE): Measures the difference between the true solution \\mathbf{x} and the approximate solution \\mathbf{x_a}:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nRelative Forward Error (RFE): Normalizes the forward error relative to \\mathbf{x}:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|x\\|_\\infty}"
  },
  {
    "objectID": "notes/w07/2-3.html#error-magnification-factor-emf",
    "href": "notes/w07/2-3.html#error-magnification-factor-emf",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Error Magnification Factor (EMF)",
    "text": "Error Magnification Factor (EMF)\nThe error magnification factor (EMF) relates the relative forward error (RFE) to the relative backward error (RBE):\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n\nThis quantifies how much the backward error is amplified when reflected in the forward error."
  },
  {
    "objectID": "notes/w07/2-3.html#condition-number-of-a-matrix",
    "href": "notes/w07/2-3.html#condition-number-of-a-matrix",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Condition Number of a Matrix",
    "text": "Condition Number of a Matrix\nThe condition number of a matrix A measures the sensitivity of the solution \\mathbf{x} to changes in \\mathbf{b}. It is defined as:\n\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty\n\n\nInterpretation:\n\nA low condition number (close to 1) indicates a well-conditioned matrix.\nA high condition number suggests an ill-conditioned matrix, meaning small changes in \\mathbf{b} can result in large changes in \\mathbf{x}."
  },
  {
    "objectID": "notes/w07/2-3.html#example-1",
    "href": "notes/w07/2-3.html#example-1",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Example",
    "text": "Example\nGiven:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\n\n\nStep 1: Compute Errors\n\nForward Error (FE):\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty = \\left\\| \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\|_\\infty = 1\n\nRelative Forward Error (RFE):\n\n\\text{RFE} = \\frac{\\text{FE}}{\\|\\mathbf{x}\\|_\\infty} = \\frac{1}{2} = 0.5\n\nResidual:\n\nr = b - Ax_a = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nBackward Error (BE):\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = 3\n\nRelative Backward Error (RBE):\n\n\\text{RBE} = \\frac{\\text{BE}}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1\n\n\n\n\nStep 2: Compute EMF\nUsing:\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n\nwe find:\n\n\\text{EMF} = \\frac{0.5}{1} = 0.5\n\n\n\nStep 3: Compute Condition Number\n\nCompute \\|A\\|_\\infty:\n\n\\|A\\|_\\infty = \\max\\left( |1| + |1|, |3| + |-4| \\right) = \\max(2, 7) = 7\n\nCompute \\|A^{-1}\\|_\\infty:\nFrom A^{-1}, we find:\n\n\\|A^{-1}\\|_\\infty = \\max\\left( \\frac{4}{7} + \\frac{1}{7}, \\frac{3}{7} + \\frac{1}{7} \\right) = \\frac{5}{7}\n\nCondition Number:\n\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 7 \\cdot \\frac{5}{7} = 5"
  },
  {
    "objectID": "notes/w07/2-3.html#next-steps-pa-lu-decomposition-partial-pivoting",
    "href": "notes/w07/2-3.html#next-steps-pa-lu-decomposition-partial-pivoting",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Next Steps: PA = LU Decomposition (Partial Pivoting)",
    "text": "Next Steps: PA = LU Decomposition (Partial Pivoting)\n\nWhat is Partial Pivoting?\nPartial pivoting rearranges rows of A during LU decomposition to place the largest available pivot element on the diagonal. This ensures numerical stability by reducing rounding errors.\n\n\nConsequences of Partial Pivoting:\n\nControlled Multipliers: Ensures all multipliers satisfy |m_{ij}| \\leq 1.\nPrevents Swamping: Avoids large numerical errors caused by small pivot elements.\n\nUnderstanding norms, errors, and condition numbers is foundational for solving A\\mathbf{x} = \\mathbf{b} efficiently and accurately."
  },
  {
    "objectID": "notes/w04/trapezoidal-rule.html",
    "href": "notes/w04/trapezoidal-rule.html",
    "title": "Trapezoidal Rule",
    "section": "",
    "text": "Overview\nThe trapezoidal rule is a numerical method for approximating definite integrals. It works by dividing the interval of integration into sub-intervals and approximating the area under the curve as a series of trapezoids. This method is particularly useful for complex functions or when an analytical solution is not feasible.\n\n\nGeneral Formula\nGiven the integral:\n\\[\n\\int_a^b f(x) \\, dx\n\\]\nThe trapezoidal rule estimates the integral by approximating the region under the curve with trapezoids.\n\n\nEqual Sub-Intervals (Uniform Spacing)\nWhen the interval \\([a, b]\\) is divided into \\(n\\) equal sub-intervals of width:\n\\[\n\\Delta x = \\frac{b - a}{n}\n\\]\nThe composite trapezoidal rule can be expressed as:\n\\[\nT_n = \\frac{\\Delta x}{2} \\left( y_0 + 2 y_1 + 2 y_2 + \\dots + 2 y_{n-1} + y_n \\right)\n\\]\nwhere:\n\n\\(y_0 = f(a)\\) and \\(y_n = f(b)\\) are the function values at the endpoints.\n\\(y_1, y_2, \\dots, y_{n-1}\\) are the function values at the interior points, each multiplied by 2 because they are shared by two adjacent trapezoids.\n\n\nStep-by-Step Formula in Function Terms\nUsing function values at each point \\(x_i = a + i \\Delta x\\), the formula becomes:\n\\[\nT_n = \\frac{\\Delta x}{2} \\left( f(a) + 2 f(a + \\Delta x) + 2 f(a + 2 \\Delta x) + \\dots + 2 f(a + (n - 1) \\Delta x) + f(b) \\right)\n\\]\nThis formula simplifies integration when the sub-intervals are uniformly spaced.\n\n\nSummation Notation for Uniform Spacing\nAlternatively, the composite trapezoidal rule for uniform spacing can be written as:\n\\[\nT_n = \\frac{\\Delta x}{2} \\left( f(x_0) + 2 \\sum_{i=1}^{n-1} f(x_i) + f(x_n) \\right)\n\\]\nwhere:\n\n\\(x_0 = a\\), \\(x_n = b\\), and \\(x_i = a + i \\Delta x\\).\nThe summation accounts for the contributions of interior points.\n\n\n\n\nUnequal Sub-Intervals (Non-Uniform Spacing)\nWhen the sub-intervals are not equally spaced, the formula adjusts to account for varying widths \\(\\Delta x_i\\) between points. The non-uniform trapezoidal rule becomes:\n\\[\nT = \\sum_{i=1}^{n} \\frac{\\Delta x_i}{2} \\left( f(x_{i-1}) + f(x_i) \\right)\n\\]\nwhere:\n\nEach term computes the area of a trapezoid over the individual sub-interval \\([x_{i-1}, x_i]\\).\nThe width of each sub-interval is \\(\\Delta x_i = x_i - x_{i-1}\\), which may vary.\n\n\n\nComparison: Equal vs. Unequal Sub-Intervals\n\n\n\n\n\n\n\n\nProperty\nEqual Sub-Intervals\nUnequal Sub-Intervals\n\n\n\n\nWidth of Sub-Intervals\n\\(\\Delta x = \\frac{b - a}{n}\\) (constant)\n\\(\\Delta x_i = x_i - x_{i-1}\\) (varies)\n\n\nFormula\n\\(\\frac{\\Delta x}{2} \\left( y_0 + 2 y_1 + \\dots + 2 y_{n-1} + y_n \\right)\\)\n\\(\\sum \\frac{\\Delta x_i}{2} \\left( f(x_{i-1}) + f(x_i) \\right)\\)\n\n\nComputational Effort\nEasier to compute with uniform spacing\nMore complicated due to variable widths"
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html",
    "href": "notes/w03/chebyshev-interpolation.html",
    "title": "Chebyshev Interpolation",
    "section": "",
    "text": "Chebyshev Interpolation is a powerful technique in numerical analysis used to approximate functions with polynomials, particularly minimizing errors and avoiding issues like Runge’s phenomenon. This note provides an overview of Chebyshev Interpolation, including its definition, properties, advantages, and practical implementation."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#introduction",
    "href": "notes/w03/chebyshev-interpolation.html#introduction",
    "title": "Chebyshev Interpolation",
    "section": "Introduction",
    "text": "Introduction\nInterpolation involves approximating a function f(x) using a polynomial P_n(x) that passes through a set of points (nodes). While polynomial interpolation is straightforward, selecting appropriate nodes is crucial to ensure accuracy and stability. Chebyshev Interpolation leverages Chebyshev nodes to construct interpolating polynomials that minimize the maximum error across the interpolation interval."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#chebyshev-polynomials",
    "href": "notes/w03/chebyshev-interpolation.html#chebyshev-polynomials",
    "title": "Chebyshev Interpolation",
    "section": "Chebyshev Polynomials",
    "text": "Chebyshev Polynomials\nChebyshev polynomials are a sequence of orthogonal polynomials that arise in various approximation and interpolation problems. They are defined recursively and have properties that make them ideal for minimizing interpolation errors.\n\nDefinition\nThe Chebyshev polynomials of the first kind, T_n(x), are defined by:\n\nT_n(x) = \\cos(n \\arccos x), \\quad \\text{for } x \\in [-1, 1]\n\n\n\nProperties\n\nOrthogonality: Chebyshev polynomials are orthogonal with respect to the weight w(x) = \\frac{1}{\\sqrt{1 - x^2}} on the interval [-1, 1].\nExtremal Property: Among all polynomials of degree n with leading coefficient 2^{n-1}, T_n(x) has the smallest maximum deviation from zero on [-1, 1].\nRoots and Extremes: The roots of T_n(x) are given by:\n\nx_k = \\cos\\left( \\frac{2k - 1}{2n} \\pi \\right), \\quad k = 1, 2, \\dots, n\n\nThe extrema (maximum and minimum points) of T_n(x) occur at:\n\nx_k = \\cos\\left( \\frac{k}{n} \\pi \\right), \\quad k = 0, 1, \\dots, n"
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#chebyshev-nodes",
    "href": "notes/w03/chebyshev-interpolation.html#chebyshev-nodes",
    "title": "Chebyshev Interpolation",
    "section": "Chebyshev Nodes",
    "text": "Chebyshev Nodes\nChebyshev nodes are specific points in the interval [-1, 1] used as interpolation nodes to minimize the interpolation error.\n\nDefinition\nFor n+1 Chebyshev nodes, the k-th node x_k is given by:\n\nx_k = \\cos\\left( \\frac{2k + 1}{2n + 2} \\pi \\right), \\quad k = 0, 1, 2, \\dots, n\n\nAlternatively, they can be expressed as:\n\nx_k = \\cos\\left( \\frac{(2k + 1)\\pi}{2(n + 1)} \\right), \\quad k = 0, 1, 2, \\dots, n\n\n\n\nMapping to Arbitrary Intervals\nFor an interval [a, b], Chebyshev nodes are mapped as:\n\nx_k = \\frac{a + b}{2} + \\frac{b - a}{2} \\cos\\left( \\frac{(2k + 1)\\pi}{2(n + 1)} \\right), \\quad k = 0, 1, 2, \\dots, n\n\n\n\nImportance\nUsing Chebyshev nodes instead of equally spaced nodes helps in minimizing the oscillatory behavior (Runge’s phenomenon) and ensures better convergence properties of the interpolating polynomial."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#interpolation-process",
    "href": "notes/w03/chebyshev-interpolation.html#interpolation-process",
    "title": "Chebyshev Interpolation",
    "section": "Interpolation Process",
    "text": "Interpolation Process\nThe Chebyshev Interpolation process involves the following steps:\n\nSelect Chebyshev Nodes: Determine the n+1 Chebyshev nodes x_0, x_1, \\dots, x_n in the interval [-1, 1].\nEvaluate the Function: Compute the function values f(x_0), f(x_1), \\dots, f(x_n).\nConstruct the Interpolating Polynomial: Use methods such as the Chebyshev series expansion or the barycentric interpolation formula to construct the interpolating polynomial Q_n(x).\nApproximate the Function: Use Q_n(x) to approximate f(x) within the interval.\n\n\nBarycentric Interpolation Formula\nOne efficient method to compute the interpolating polynomial is the barycentric interpolation formula:\n\nQ_n(x) = \\frac{\\sum_{k=0}^n \\frac{w_k f(x_k)}{x - x_k}}{\\sum_{k=0}^n \\frac{w_k}{x - x_k}}\n\nwhere w_k are the barycentric weights defined as:\n\nw_k = (-1)^k \\sin\\left( \\frac{(2k + 1)\\pi}{2n + 2} \\right)"
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#advantages-of-chebyshev-interpolation",
    "href": "notes/w03/chebyshev-interpolation.html#advantages-of-chebyshev-interpolation",
    "title": "Chebyshev Interpolation",
    "section": "Advantages of Chebyshev Interpolation",
    "text": "Advantages of Chebyshev Interpolation\n\nMinimized Error: Chebyshev nodes minimize the maximum error (uniform convergence) of the interpolating polynomial.\nReduced Oscillations: Avoids Runge’s phenomenon, where high-degree polynomial interpolations at equally spaced nodes exhibit large oscillations near the interval endpoints.\nEfficient Computation: The barycentric interpolation formula allows for efficient and stable computation of interpolating polynomials.\nOrthogonality: Leveraging the orthogonality of Chebyshev polynomials aids in various approximation and numerical integration techniques."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#error-analysis",
    "href": "notes/w03/chebyshev-interpolation.html#error-analysis",
    "title": "Chebyshev Interpolation",
    "section": "Error Analysis",
    "text": "Error Analysis\nUnderstanding the error associated with Chebyshev Interpolation is crucial for assessing the approximation’s reliability.\n\nInterpolation Error Formula\nFor a function f(x) sufficiently smooth on [-1, 1], the error of the Chebyshev interpolating polynomial Q_n(x) of degree n is given by:\n\n|f(x) - Q_n(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\frac{1}{2^{n+1}}\n\nwhere:\n\nM is an upper bound on the (n+1)-th derivative of f(x) on [-1, 1].\n\n\n\nWorst-Case Error Estimate\nFor example, if f(x) = e^x, all derivatives are f^{(k)}(x) = e^x. On [-1, 1], e^x \\leq e, so M = e.\nFor a fifth-degree polynomial (n = 5):\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{2^5} = \\frac{e}{720 \\times 32} \\approx 0.000118\n\n\n\nImplications\nAn error bound of approximately 1.18 \\times 10^{-4} implies that at least three decimal digits of the approximation Q_5(x) are accurate across the interval [-1, 1]."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#example",
    "href": "notes/w03/chebyshev-interpolation.html#example",
    "title": "Chebyshev Interpolation",
    "section": "Example",
    "text": "Example\nProblem: Approximate f(x) = e^x on [-1, 1] using a fifth-degree Chebyshev interpolating polynomial Q_5(x). Estimate the worst-case error and determine the number of correct decimal digits in the approximation.\nSolution:\n\nDetermine M:\n\nf^{(6)}(x) = e^x, so M = e.\n\nCompute the Error Bound:\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{2^5} = \\frac{2.71828}{720 \\times 32} \\approx 0.000118\n\nInterpret the Error:\n\nThe approximation Q_5(x) deviates from e^x by less than 1.18 \\times 10^{-4}.\nAt least three decimal digits of Q_5(x) are accurate.\n\n\nConclusion: The fifth-degree Chebyshev interpolating polynomial Q_5(x) approximates e^x with an error less than 0.00012, ensuring at least three correct decimal digits across [-1, 1]."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#applications",
    "href": "notes/w03/chebyshev-interpolation.html#applications",
    "title": "Chebyshev Interpolation",
    "section": "Applications",
    "text": "Applications\nChebyshev Interpolation is widely used in various fields due to its robustness and efficiency:\n\nNumerical Integration: Chebyshev polynomials are used in Gaussian quadrature methods for approximating integrals.\nApproximation Theory: Provides optimal polynomial approximations for continuous functions.\nSignal Processing: Used in filter design and spectral analysis.\nComputer Graphics: Facilitates curve and surface modeling with minimal errors.\nScientific Computing: Enhances the accuracy of simulations and numerical solutions to differential equations."
  },
  {
    "objectID": "notes/w03/chebyshev-interpolation.html#conclusion",
    "href": "notes/w03/chebyshev-interpolation.html#conclusion",
    "title": "Chebyshev Interpolation",
    "section": "Conclusion",
    "text": "Conclusion\nChebyshev Interpolation offers a reliable method for polynomial approximation, leveraging Chebyshev nodes to minimize interpolation errors and avoid common pitfalls like Runge’s phenomenon. Its mathematical foundations, combined with practical computational techniques, make it an essential tool in numerical analysis and various applied disciplines."
  },
  {
    "objectID": "notes/w02/lagrange-interpolation.html",
    "href": "notes/w02/lagrange-interpolation.html",
    "title": "Lagrange Interpolation",
    "section": "",
    "text": "Lagrange Interpolation is a method of constructing a polynomial that passes through a given set of points. It is particularly useful when you have a small number of data points and want to determine the polynomial function that exactly fits those points."
  },
  {
    "objectID": "notes/w02/lagrange-interpolation.html#the-lagrange-interpolating-polynomial",
    "href": "notes/w02/lagrange-interpolation.html#the-lagrange-interpolating-polynomial",
    "title": "Lagrange Interpolation",
    "section": "The Lagrange Interpolating Polynomial",
    "text": "The Lagrange Interpolating Polynomial\nGiven n distinct data points (x_1, y_1), (x_2, y_2), ..., (x_n, y_n), the Lagrange interpolating polynomial is the polynomial P(x) of degree at most n-1 that passes through all the points, meaning:\n\nP(x_i) = y_i \\quad \\text{for each} \\ i = 1, 2, ..., n\n\nThe Lagrange form of the polynomial is given by:\n\nP(x) = \\sum_{i=1}^{n} y_i L_i(x)\n\nwhere L_i(x) are the Lagrange basis polynomials, defined as:\n\nL_i(x) = \\prod_{j=1, j \\neq i}^{n} \\frac{x - x_j}{x_i - x_j}\n\nHere, the product is taken over all j \\neq i, ensuring that L_i(x_j) = 0 for j \\neq i and L_i(x_i) = 1. This ensures that P(x_i) = y_i for each i.\n\nStep-by-Step Calculation\nSuppose we are given a set of three points: (x_1, y_1), (x_2, y_2), (x_3, y_3). The Lagrange interpolating polynomial is:\n\nP(x) = y_1 L_1(x) + y_2 L_2(x) + y_3 L_3(x)\n\nwhere the Lagrange basis polynomials are:\n\nL_1(x) = \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)}\n\n\nL_2(x) = \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)}\n\n\nL_3(x) = \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\n\nBy evaluating these expressions and plugging in the values of y_1, y_2, and y_3, we obtain the polynomial P(x) that passes through all the given points.\n\n\nExample\nLet’s go through an example where we are given three points: (1, 2), (2, 3), and (3, 5).\n\nPoints:\n(x_1, y_1) = (1, 2)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 5)\nLagrange basis polynomials:\n\n\nL_1(x) = \\frac{(x - 2)(x - 3)}{(1 - 2)(1 - 3)} = \\frac{(x - 2)(x - 3)}{2}\n\n\nL_2(x) = \\frac{(x - 1)(x - 3)}{(2 - 1)(2 - 3)} = \\frac{(x - 1)(x - 3)}{-1}\n\n\nL_3(x) = \\frac{(x - 1)(x - 2)}{(3 - 1)(3 - 2)} = \\frac{(x - 1)(x - 2)}{2}\n\n\nLagrange interpolating polynomial:\n\n\nP(x) = 2 \\cdot L_1(x) + 3 \\cdot L_2(x) + 5 \\cdot L_3(x)\n\nSubstituting the values for L_1(x), L_2(x), and L_3(x):\n\nP(x) = 2 \\cdot \\frac{(x - 2)(x - 3)}{2} + 3 \\cdot \\frac{(x - 1)(x - 3)}{-1} + 5 \\cdot \\frac{(x - 1)(x - 2)}{2}\n\nSimplifying this expression will give you the final polynomial P(x) that passes through all three points.\n\n\nGeneral Properties of Lagrange Interpolation\n\nUniqueness: There is exactly one polynomial of degree n-1 that interpolates n points. This is guaranteed by the fundamental theorem of algebra, which states that a polynomial of degree n-1 is uniquely determined by n distinct points.\nEfficiency: Lagrange interpolation is not the most computationally efficient method for large datasets, because each term depends on all the data points, making the calculation costly for large n. Methods like Newton’s divided differences are generally preferred for interpolation with larger datasets.\nAccuracy: Interpolation works well if the points are well-distributed and the function is smooth. However, for unevenly spaced points or functions with high curvature, the interpolation polynomial may oscillate significantly, a phenomenon known as Runge’s phenomenon.\n\n\n\nApplications of Lagrange Interpolation\n\nCurve Fitting: Lagrange interpolation can be used to construct a polynomial that exactly fits a given set of data points.\nNumerical Integration: The interpolating polynomial can be used to approximate integrals through methods such as Newton-Cotes formulas.\nGraphics and Animation: In computer graphics, Lagrange interpolation is used to smoothly interpolate between keyframes in animations.\nSignal Processing: It is used in digital signal processing for reconstructing missing samples from a set of known data points.\n\n\n\nDrawbacks\n\nRunge’s Phenomenon: Lagrange interpolation can lead to significant oscillation, especially when interpolating over large intervals with a high degree polynomial.\nNot Easily Updateable: If a new point is added, the entire Lagrange polynomial must be recalculated. In contrast, methods like Newton’s divided differences allow for easier updates when new points are added.\n\n\n\nConclusion\nLagrange interpolation is a powerful tool for constructing polynomials that pass through a set of points, but it can suffer from inefficiencies and oscillations for large datasets. It’s important to understand its benefits and limitations to use it effectively in applications."
  },
  {
    "objectID": "notes/w01/newtons-method.html",
    "href": "notes/w01/newtons-method.html",
    "title": "Newton’s Method",
    "section": "",
    "text": "Newton’s Method, also known as the Newton-Raphson Method, is a widely used numerical method for finding successively better approximations to the roots (or zeros) of a real-valued function. It is particularly efficient when the initial guess is close to the actual root and when the function is well-behaved (smooth and differentiable)."
  },
  {
    "objectID": "notes/w01/newtons-method.html#the-newtons-method-formula",
    "href": "notes/w01/newtons-method.html#the-newtons-method-formula",
    "title": "Newton’s Method",
    "section": "The Newton’s Method Formula",
    "text": "The Newton’s Method Formula\nNewton’s Method is based on using the tangent line at an approximation of the root to generate a better approximation. The formula for generating the next approximation x_{k+1} from the current approximation x_k is given by:\n\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n\nwhere:\n\nf(x) is the function whose root we are trying to find.\nf'(x) is the derivative of f(x).\nx_k is the current approximation, and x_{k+1} is the next approximation.\n\n\nGeometrical Interpretation\nNewton’s Method can be interpreted geometrically: given an approximation x_k, the tangent line to the curve y = f(x) at the point $ (x*k, f(x_k)) $ is used to estimate where the curve crosses the x-axis, which provides the next approximation x*{k+1}.\n\n\nConvergence Criteria\nNewton’s Method converges quadratically under certain conditions, which means that the number of correct digits roughly doubles with each iteration. However, this fast convergence occurs only if:\n\nThe function f(x) is continuous and differentiable in the vicinity of the root.\nThe derivative f'(x) is non-zero at the root.\nThe initial guess is sufficiently close to the actual root.\n\nIf the initial guess is too far from the root, Newton’s Method may fail to converge or may converge very slowly.\n\n\nStep-by-Step Procedure\n\nInitial Guess: Choose an initial approximation x_0.\nIteration Formula: Compute successive approximations using the formula:\n\n\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n\n\nRepeat: Continue iterating until |x_{k+1} - x_k| &lt; \\epsilon, where \\epsilon is a small tolerance value, or until |f(x_k)| &lt; \\epsilon.\n\n\n\nExample\nLet’s solve the equation f(x) = x^2 - 2 = 0 using Newton’s Method, which has a root at x = \\sqrt{2}.\n\nFunction and Derivative:\n\nf(x) = x^2 - 2\n\n\nf'(x) = 2x\n\nInitial Guess: Let x_0 = 1.5.\nFirst Iteration:\n\nx_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} = 1.5 - \\frac{1.5^2 - 2}{2(1.5)} = 1.4167\n\nSecond Iteration:\n\nx_2 = x_1 - \\frac{f(x_1)}{f'(x_1)} = 1.4167 - \\frac{1.4167^2 - 2}{2(1.4167)} = 1.4142\n\nFurther Iterations: Repeat until the difference between successive approximations is less than a specified tolerance (e.g., \\epsilon = 10^{-5}).\n\nIn this case, after just a few iterations, we have a highly accurate approximation of \\sqrt{2}.\n\n\nGeneral Properties of Newton’s Method\n\nQuadratic Convergence: When close to the root, Newton’s Method converges quadratically, meaning that the error decreases roughly as the square of the previous error.\nRequires Derivatives: Unlike the Bisection Method, Newton’s Method requires that the derivative f'(x) be known and be non-zero at the root.\nSensitive to Initial Guess: The method is sensitive to the initial guess, and poor choices of x_0 can lead to divergence or slow convergence.\n\n\n\nApplications of Newton’s Method\n\nRoot Finding: Newton’s Method is widely used to find roots of non-linear equations in mathematics, physics, engineering, and economics.\nOptimization: Newton’s Method is the basis of Newton’s optimization method, which is used to find local minima or maxima of differentiable functions by solving f'(x) = 0.\nEngineering and Modeling: It is used to solve non-linear models and systems, especially in fields like structural engineering, fluid dynamics, and electrical circuit analysis.\n\n\n\nAdvantages of Newton’s Method\n\nFast Convergence: When it converges, Newton’s Method is extremely fast due to its quadratic convergence rate.\nSimple Iterative Formula: The iteration formula is straightforward and easy to implement.\nFew Iterations: For well-behaved functions and good initial guesses, only a few iterations are required to obtain a highly accurate result.\n\n\n\nLimitations of Newton’s Method\n\nRequires Derivatives: The method requires that f(x) is differentiable, and that the derivative f'(x) can be computed analytically or numerically.\nRisk of Divergence: If the initial guess is too far from the root or if f'(x) is zero or near zero, the method may diverge or fail to converge.\nSlow or No Convergence: For functions with inflection points or flat regions near the root, the method may converge very slowly or not at all. In these cases, alternative methods like the Secant Method or Bisection Method may be better suited.\n\n\n\nConclusion\nNewton’s Method is a powerful and efficient tool for finding roots of equations, especially when the initial guess is close to the solution. While it requires the calculation of derivatives, its fast convergence makes it a preferred method when applicable. However, care must be taken with the choice of initial guess to avoid issues with divergence or slow convergence."
  },
  {
    "objectID": "notes/w01/bisection-method.html",
    "href": "notes/w01/bisection-method.html",
    "title": "Bisection Method",
    "section": "",
    "text": "Bisection Method is one of the simplest and most reliable numerical methods for finding a root of a continuous function f(x) = 0 over a closed interval [a, b]. The method works by repeatedly bisecting the interval and then selecting the subinterval in which the function changes sign, ensuring that a root lies within that subinterval."
  },
  {
    "objectID": "notes/w01/bisection-method.html#the-bisection-method-formula",
    "href": "notes/w01/bisection-method.html#the-bisection-method-formula",
    "title": "Bisection Method",
    "section": "The Bisection Method Formula",
    "text": "The Bisection Method Formula\nThe Bisection Method requires that the function f(x) be continuous over the interval [a, b], and that the function has opposite signs at the endpoints a and b, i.e., f(a) \\cdot f(b) &lt; 0. This guarantees that there is at least one root in the interval by the Intermediate Value Theorem.\nThe basic idea of the method is to repeatedly bisect the interval and check the sign of f(x) at the midpoint to determine the subinterval containing the root.\n\nAlgorithm\n\nInitial Guess: Choose an interval [a_0, b_0] such that f(a_0) \\cdot f(b_0) &lt; 0.\nMidpoint Calculation: Compute the midpoint c_k = \\frac{a_k + b_k}{2} of the interval [a_k, b_k].\nCheck the Sign: Evaluate f(c_k).\n\nIf f(c_k) = 0, then c_k is the root.\nIf f(a_k) \\cdot f(c_k) &lt; 0, set b_{k+1} = c_k, and the root lies in [a_k, c_k].\nIf f(c_k) \\cdot f(b_k) &lt; 0, set a_{k+1} = c_k, and the root lies in [c_k, b_k].\n\nRepeat: Continue bisecting the interval until the length of the interval is smaller than a specified tolerance \\epsilon, or until |f(c_k)| &lt; \\epsilon.\n\nThe approximate root will be:\n\nx^* = \\frac{a_k + b_k}{2}\n\n\n\nConvergence\nThe Bisection Method converges linearly. The length of the interval halves at each iteration, ensuring that the method always converges to a solution (if one exists) within the interval.\nThe number of iterations n required to achieve an accuracy of \\epsilon can be estimated by:\n\nn \\geq \\frac{\\log \\left( \\frac{b_0 - a_0}{\\epsilon} \\right)}{\\log 2}\n\n\n\nExample\nLet’s solve the equation f(x) = x^3 - x - 2 = 0 in the interval [1, 2].\n\nInitial Interval:\nf(1) = 1^3 - 1 - 2 = -2\nf(2) = 2^3 - 2 - 2 = 4\nSince f(1) \\cdot f(2) &lt; 0, there is a root in [1, 2].\nFirst Iteration:\nMidpoint: c_1 = \\frac{1 + 2}{2} = 1.5\nf(1.5) = 1.5^3 - 1.5 - 2 = -0.125\nSince f(1) \\cdot f(1.5) &lt; 0, the root lies in [1, 1.5].\nSecond Iteration:\nMidpoint: c_2 = \\frac{1 + 1.5}{2} = 1.25\nf(1.25) = 1.25^3 - 1.25 - 2 = -1.796875\nSince f(1) \\cdot f(1.25) &lt; 0, the root lies in [1, 1.25].\nFurther Iterations:\nRepeat the process until the interval width is smaller than the desired tolerance \\epsilon.\n\n\n\nGeneral Properties of the Bisection Method\n\nGuaranteed Convergence: The method is guaranteed to converge to a root if f(a) \\cdot f(b) &lt; 0 and f(x) is continuous on [a, b].\nRate of Convergence: The Bisection Method has linear convergence, meaning the error decreases by a constant factor with each iteration. This makes the method slower than other methods like Newton’s Method, but much more reliable.\nRobustness: The method is very robust as it does not require the derivative of the function and is insensitive to the initial guesses, provided the condition f(a) \\cdot f(b) &lt; 0 holds.\n\n\n\nApplications of the Bisection Method\n\nRoot Finding: The Bisection Method is used in various fields, including physics, engineering, and mathematics, to find roots of non-linear equations.\nModeling and Simulation: It is used when precise solutions are needed and the function is known to be continuous over the interval.\nInitial Root Estimates: The Bisection Method is often used to find a good initial approximation for more efficient methods like Newton’s Method or the Secant Method.\n\n\n\nAdvantages of the Bisection Method\n\nGuaranteed Convergence: The method always converges if the initial interval contains a root.\nNo Derivatives Needed: Unlike Newton’s Method, the Bisection Method does not require the computation of derivatives.\nSimplicity: The method is easy to understand and implement.\n\n\n\nLimitations of the Bisection Method\n\nSlow Convergence: The method converges linearly, which makes it slower compared to methods like Newton’s Method, which has quadratic convergence.\nOnly One Root: The Bisection Method only finds one root in the interval. If multiple roots exist, it cannot find them all without running the method on different intervals.\nInitial Interval Requirement: The method requires an initial interval [a, b] where the function changes sign, which may not always be easy to determine.\n\n\n\nConclusion\nThe Bisection Method is a reliable and simple method for finding roots of continuous functions, especially when no derivative information is available. While slower than other root-finding algorithms, its guaranteed convergence and robustness make it a valuable tool in numerical analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FALL 2024 - SCHEDULE",
    "section": "",
    "text": "Week\n\n\nMonday\n\n\nTuesday\n\n\nWednesday\n\n\nThursday\n\n\nFriday\n\n\n\n\n1\n\n\n16Chapter 1\n\n\n17\n\n\n18Chapter 1\n\n\n19\n\n\n20Chapter 1\n\n\n\n\n2\n\n\n23Add/DropChapter 1\n\n\n24\n\n\n25Chapter 1Quiz 1\n\n\n26\n\n\n27Chapter 3\n\n\n\n\n3\n\n\n30Chapter 3\n\n\n1\n\n\n2Chapter 3Quiz 2\n\n\n3\n\n\n4Chapter 3\n\n\n\n\n4\n\n\n7Chapter 3Reality Check 1\n\n\n8No W drop date\n\n\n9Chapter 5Quiz 3\n\n\n10\n\n\n11Chapter 5\n\n\n\n\n5\n\n\n14Chapter 5\n\n\n15\n\n\n16Chapter 5Quiz 4\n\n\n17\n\n\n18Chapter 5\n\n\n\n\n6\n\n\n21Exam 1 ReviewExam 1-2 Opens\n\n\n22\n\n\n23Exam 1-1In Class\n\n\n24Exam 1-2 Closes\n\n\n25Chapter 2\n\n\n\n\n7\n\n\n28Chapter 2\n\n\n29\n\n\n30Chapter 2\n\n\n31\n\n\n1Chapter 2\n\n\n\n\n8\n\n\n4Chapter 2\n\n\n5\n\n\n6Chapter 4Quiz 5\n\n\n7\n\n\n8Chapter 4\n\n\n\n\n9\n\n\n11Drop w/W dateChapter 42nd Reality Check\n\n\n12\n\n\n13Chapter 4Quiz 6\n\n\n14\n\n\n15Chapter 4\n\n\n\n\n10\n\n\n18Chapter 4\n\n\n19\n\n\n20Chapter 4\n\n\n21\n\n\n22Exam 2 ReviewExam 2-2 Opens\n\n\n\n\n11\n\n\n25Exam 2-1In Class\n\n\n26Exam 2-2 Closes\n\n\n27No Classes\n\n\n28Holiday\n\n\n29Holiday\n\n\n\n\n12\n\n\n2Chapter 10\n\n\n3Discontinuance\n\n\n4Chapter 10\n\n\n5\n\n\n6Chapter 10Quiz 7 (?)\n\n\n\n\n13\n\n\n9Chapter 10\n\n\n10\n\n\n11Chapter 10\n\n\n12\n\n\n13Chapter 10Exam 3-2 Opens\n\n\n\n\n14\n\n\n16Last Day of Class\n\n\n1710:30 Exam 3-1Exam 3-2 Closes3rd Reality Check\n\n\n18\n\n\n19Commencement\n\n\n20"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html",
    "href": "homework/w09/exercise4-3-1d.html",
    "title": "Exercise 4.3.1d",
    "section": "",
    "text": "4.3.1d\n\n\n\nApply classical Gram–Schmidt orthogonalization to find the full QR factorization of the matrix:\n\n\\mathbf{A} = \\begin{bmatrix} 4 & 8 & 1 \\\\ 0 & 2 & -2 \\\\ 3 & 6 & 7 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#define-the-columns-of-matrix-mathbfa",
    "href": "homework/w09/exercise4-3-1d.html#define-the-columns-of-matrix-mathbfa",
    "title": "Exercise 4.3.1d",
    "section": "Define the Columns of Matrix \\mathbf{A}",
    "text": "Define the Columns of Matrix \\mathbf{A}\nRepresent the columns of \\mathbf{A} as vectors:\n\n\\mathbf{a}_1 = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix}, \\quad \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#finding-mathbfq_1",
    "href": "homework/w09/exercise4-3-1d.html#finding-mathbfq_1",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_1",
    "text": "Finding \\mathbf{q}_1\nTo find the first orthonormal vector \\mathbf{q}_1, normalize \\mathbf{a}_1.\n\nCalculate the norm of \\mathbf{a}_1:\n\n\\|\\mathbf{a}_1\\| = \\sqrt{4^2 + 0^2 + 3^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\n\nNormalize \\mathbf{a}_1:\n\n\\mathbf{q}_1 = \\frac{\\mathbf{a}_1}{\\|\\mathbf{a}_1\\|} = \\frac{1}{5} \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#finding-mathbfq_2",
    "href": "homework/w09/exercise4-3-1d.html#finding-mathbfq_2",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_2",
    "text": "Finding \\mathbf{q}_2\nTo find \\mathbf{q}_2, project \\mathbf{a}_2 onto \\mathbf{q}_1 and then subtract this projection from \\mathbf{a}_2.\n\nCalculate the projection of \\mathbf{a}_2 onto \\mathbf{q}_1:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\left( \\mathbf{a}_2 \\cdot \\mathbf{q}_1 \\right) \\mathbf{q}_1\n\nSince \\mathbf{q}_1 is a unit vector, \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\nCompute \\mathbf{a}_2 \\cdot \\mathbf{q}_1:\n\n\\mathbf{a}_2 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{32}{5} + 0 + \\frac{18}{5} = \\frac{50}{5} = 10\n\nCalculate \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = 10 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix}\n\nCalculate \\mathbf{u}_2:\nSubtracting the projection from \\mathbf{a}_2 gives \\mathbf{u}_2:\n\n\\mathbf{u}_2 = \\mathbf{a}_2 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix}\n\nNormalize \\mathbf{u}_2 to obtain \\mathbf{q}_2:\nThe norm of \\mathbf{u}_2 is:\n\n\\|\\mathbf{u}_2\\| = \\sqrt{0^2 + 2^2 + 0^2} = 2\n\nThus,\n\n\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{\\|\\mathbf{u}_2\\|} = \\frac{1}{2} \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#finding-mathbfq_3",
    "href": "homework/w09/exercise4-3-1d.html#finding-mathbfq_3",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_3",
    "text": "Finding \\mathbf{q}_3\nTo find \\mathbf{q}_3, project \\mathbf{a}_3 onto both \\mathbf{q}_1 and \\mathbf{q}_2, and then subtract these projections from \\mathbf{a}_3.\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_1:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{4}{5} + 0 + \\frac{21}{5} = \\frac{25}{5} = 5\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 = 5 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_2:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_2 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = -2\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = -2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix}\n\nCalculate \\mathbf{u}_3:\nSubtracting both projections from \\mathbf{a}_3 yields \\mathbf{u}_3:\n\n\\mathbf{u}_3 = \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} - \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix}\n\nNormalize \\mathbf{u}_3 to obtain \\mathbf{q}_3:\nThe norm of \\mathbf{u}_3 is:\n\n\\|\\mathbf{u}_3\\| = \\sqrt{(-3)^2 + 0^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\nThus,\n\n\\mathbf{q}_3 = \\frac{\\mathbf{u}_3}{\\|\\mathbf{u}_3\\|} = \\frac{1}{5} \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ 0 \\\\ \\frac{4}{5} \\end{bmatrix}"
  },
  {
    "objectID": "homework/w09/exercise4-3-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "href": "homework/w09/exercise4-3-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "title": "Exercise 4.3.1d",
    "section": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices",
    "text": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices\n\nOrthogonal Matrix \\mathbf{Q}\nCombine \\mathbf{q}_1, \\mathbf{q}\\_2, and \\mathbf{q}_3 as columns to form \\mathbf{Q}:\n\n\\mathbf{Q} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix}\n\n\n\nUpper Triangular Matrix \\mathbf{R}\nThe entries of \\mathbf{R} are calculated as the dot products of the original columns of \\mathbf{A} with the orthonormal vectors \\mathbf{q}_1, \\mathbf{q}_2, and \\mathbf{q}_3:\n\n\\mathbf{R} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{q}_1 & \\mathbf{a}_2 \\cdot \\mathbf{q}_1 & \\mathbf{a}_3 \\cdot \\mathbf{q}_1 \\\\ 0 & \\mathbf{a}_2 \\cdot \\mathbf{q}_2 & \\mathbf{a}_3 \\cdot \\mathbf{q}_2 \\\\ 0 & 0 & \\mathbf{a}_3 \\cdot \\mathbf{q}_3 \\end{bmatrix} = \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe QR factorization of \\mathbf{A} is:\n\n\\mathbf{A} = \\mathbf{Q} \\mathbf{R} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix} \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}"
  },
  {
    "objectID": "homework/w07/exercise2-4-6.html",
    "href": "homework/w07/exercise2-4-6.html",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "",
    "text": "(a) Write down the 4 \\times 4 matrix P such that multiplying a matrix on the left by P causes the second and fourth rows of the matrix to be exchanged.\n(b) What is the effect of multiplying on the right by P? Demonstrate with an example."
  },
  {
    "objectID": "homework/w07/exercise2-4-6.html#problem",
    "href": "homework/w07/exercise2-4-6.html#problem",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "",
    "text": "(a) Write down the 4 \\times 4 matrix P such that multiplying a matrix on the left by P causes the second and fourth rows of the matrix to be exchanged.\n(b) What is the effect of multiplying on the right by P? Demonstrate with an example."
  },
  {
    "objectID": "homework/w07/exercise2-4-6.html#solution",
    "href": "homework/w07/exercise2-4-6.html#solution",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "Solution:",
    "text": "Solution:\n\n(a) Constructing the Permutation Matrix P\nTo create a permutation matrix P that exchanges the second and fourth rows when multiplied on the left, start with the 4 \\times 4 identity matrix I_4 and swap its second and fourth rows.\nIdentity Matrix I_4:\n\nI_4 = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\nPermutation Matrix P (after swapping rows 2 and 4):\n\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\nWhen P multiplies any 4 \\times n matrix A on the left (PA), it exchanges the second and fourth rows of A.\n\n\n(b) Effect of Multiplying on the Right by P\nMultiplying a matrix on the right by P rearranges its columns (instead of rows) according to the pattern defined by P. In this case, it specifically swaps the matrix’s second and fourth columns.\nDemonstration with an Example:\nLet’s consider a 4 \\times 4 matrix A:\n\nA = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44} \\\\\n\\end{pmatrix}\n\nCompute AP:\n\nAP = A \\times P = A \\times \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\nResulting Matrix AP:\n\nAP = \\begin{pmatrix}\na_{11} & a_{14} & a_{13} & a_{12} \\\\\na_{21} & a_{24} & a_{23} & a_{22} \\\\\na_{31} & a_{34} & a_{33} & a_{32} \\\\\na_{41} & a_{44} & a_{43} & a_{42} \\\\\n\\end{pmatrix}\n\nNumerical Example:\n\nA = \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12 \\\\\n13 & 14 & 15 & 16 \\\\\n\\end{pmatrix}\n\nCompute AP:\n\nAP = A \\times \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 4 & 3 & 2 \\\\\n5 & 8 & 7 & 6 \\\\\n9 & 12 & 11 & 10 \\\\\n13 & 16 & 15 & 14 \\\\\n\\end{pmatrix}\n\nFinal Answer\n(a) The 4 \\times 4 permutation matrix P that exchanges the second and fourth rows when multiplied on the left is:\n\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\n(b) Multiplying on the right by P exchanges the second and fourth columns of a matrix. For example, for the matrix:\n\nA = \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12 \\\\\n13 & 14 & 15 & 16 \\\\\n\\end{pmatrix}\n\nMultiplying on the right by P yields:\n\nAP = \\begin{pmatrix}\n1 & 4 & 3 & 2 \\\\\n5 & 8 & 7 & 6 \\\\\n9 & 12 & 11 & 10 \\\\\n13 & 16 & 15 & 14 \\\\\n\\end{pmatrix}\n\nwhich is A with its second and fourth columns exchanged."
  },
  {
    "objectID": "homework/w06/exercise2-1-2a.html",
    "href": "homework/w06/exercise2-1-2a.html",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "",
    "text": "2.1.2a\n\n\n\nSolve the following system of equations using Gaussian elimination:\n\n\\begin{aligned}\n2x - 2y - z &= -2, \\\\\n4x + y - 2z &= 1, \\\\\n-2x + y - z &= -3.\n\\end{aligned}"
  },
  {
    "objectID": "homework/w06/exercise2-1-2a.html#problem",
    "href": "homework/w06/exercise2-1-2a.html#problem",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "",
    "text": "2.1.2a\n\n\n\nSolve the following system of equations using Gaussian elimination:\n\n\\begin{aligned}\n2x - 2y - z &= -2, \\\\\n4x + y - 2z &= 1, \\\\\n-2x + y - z &= -3.\n\\end{aligned}"
  },
  {
    "objectID": "homework/w06/exercise2-1-2a.html#solution",
    "href": "homework/w06/exercise2-1-2a.html#solution",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "Solution:",
    "text": "Solution:\n\nRepresenting the System in Augmented Matrix Form\nThe system can be written in augmented matrix form as:\n\n\\begin{bmatrix}\n2 & -2 & -1 & -2 \\\\\n4 & 1 & -2 & 1 \\\\\n-2 & 1 & -1 & -3\n\\end{bmatrix}.\n\n\n\nRow Operations for Gaussian Elimination\n\nNormalize Row 1\nDivide Row 1 by 2:\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n4 & 1 & -2 & 1 \\\\\n-2 & 1 & -1 & -3\n\\end{bmatrix}.\n\n\n\nEliminate the First Column in Rows 2 and 3\n\nR_2 \\to R_2 - 4 \\cdot R_1\nR_3 \\to R_3 + 2 \\cdot R_1\n\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n0 & 5 & -\\frac{6}{2} & 5 \\\\\n0 & -1 & -2 & -5\n\\end{bmatrix}.\n\n\n\nNormalize Row 2\nDivide Row 2 by 5:\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & -1 & -2 & -5\n\\end{bmatrix}.\n\n\n\nEliminate the Second Column in Rows 1 and 3\n\nR_1 \\to R_1 + R_2\nR_3 \\to R_3 + R_2\n\n\n\\begin{bmatrix}\n1 & 0 & -\\frac{7}{10} & 0 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & 0 & -\\frac{13}{5} & -4\n\\end{bmatrix}.\n\n\n\nNormalize Row 3\nDivide Row 3 by -\\frac{13}{5}:\n\n\\begin{bmatrix}\n1 & 0 & -\\frac{7}{10} & 0 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & 0 & 1 & \\frac{20}{13}\n\\end{bmatrix}.\n\n\n\nBack Substitution\n\nUpdate R_2: R_2 \\to R_2 + \\frac{3}{5} \\cdot R_3\nUpdate R_1: R_1 \\to R_1 + \\frac{7}{10} \\cdot R_3\n\n\n\\begin{bmatrix}\n1 & 0 & 0 & \\frac{14}{13} \\\\\n0 & 1 & 0 & \\frac{19}{13} \\\\\n0 & 0 & 1 & \\frac{20}{13}\n\\end{bmatrix}.\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\n\nx = \\frac{14}{13}, \\quad y = \\frac{19}{13}, \\quad z = \\frac{20}{13}."
  },
  {
    "objectID": "homework/w04/exercise5-2-1a.html",
    "href": "homework/w04/exercise5-2-1a.html",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "",
    "text": "Apply the composite Trapezoid Rule with m = 1, 2, and 4 panels to approximate the integral. Compute the error by comparing with the exact value from calculus.\n\n\\int_0^1 x^2 \\, dx\n\n\n\n\n\n\nThe formula for the composite trapezoidal rule is:\n\nT_m = \\frac{h}{2} \\left( f(a) + 2 \\sum_{i=1}^{m-1} f(x_i) + f(b) \\right)\n\nwhere:\n\nh = \\frac{b - a}{m}\nx_i = a + i \\cdot h for i = 1, 2, \\ldots, m-1"
  },
  {
    "objectID": "homework/w04/exercise5-2-1a.html#problem",
    "href": "homework/w04/exercise5-2-1a.html#problem",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "",
    "text": "Apply the composite Trapezoid Rule with m = 1, 2, and 4 panels to approximate the integral. Compute the error by comparing with the exact value from calculus.\n\n\\int_0^1 x^2 \\, dx\n\n\n\n\n\n\nThe formula for the composite trapezoidal rule is:\n\nT_m = \\frac{h}{2} \\left( f(a) + 2 \\sum_{i=1}^{m-1} f(x_i) + f(b) \\right)\n\nwhere:\n\nh = \\frac{b - a}{m}\nx_i = a + i \\cdot h for i = 1, 2, \\ldots, m-1"
  },
  {
    "objectID": "homework/w04/exercise5-2-1a.html#solution",
    "href": "homework/w04/exercise5-2-1a.html#solution",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "Solution:",
    "text": "Solution:\n\n1. Exact Value of the Integral\nWe first compute the exact value of the integral:\n\n\\int_0^1 x^2 \\, dx = \\left[ \\frac{x^3}{3} \\right]_0^1 = \\frac{1^3}{3} - \\frac{0^3}{3} = \\frac{1}{3} \\approx 0.3333\n\n\n\n2. Approximations Using the Composite Trapezoidal Rule\n\nCase m = 1:\n\nh = \\frac{1 - 0}{1} = 1\nPoints: x_0 = 0, x_1 = 1\nApproximation:\n\n\nT_1 = \\frac{1}{2} \\left( f(0) + f(1) \\right) = \\frac{1}{2} \\left( 0^2 + 1^2 \\right) = \\frac{1}{2} \\cdot 1 = 0.5\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.5 \\right| = 0.1667\n\n\n\nCase m = 2:\n\nh = \\frac{1 - 0}{2} = 0.5\nPoints: x_0 = 0, x_1 = 0.5, x_2 = 1\nApproximation:\n\n\nT_2 = \\frac{0.5}{2} \\left( f(0) + 2 \\cdot f(0.5) + f(1) \\right) = \\frac{0.5}{2} \\left( 0^2 + 2 \\cdot 0.5^2 + 1^2 \\right)\n\n\nT_2 = \\frac{0.5}{2} \\cdot (0 + 2 \\cdot 0.25 + 1) = \\frac{0.5}{2} \\cdot 1.5 = 0.375\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.375 \\right| = 0.0417\n\n\n\nCase m = 4:\n\nh = \\frac{1 - 0}{4} = 0.25\nPoints: x_0 = 0, x_1 = 0.25, x_2 = 0.5, x_3 = 0.75, x_4 = 1\nApproximation:\n\n\nT_4 = \\frac{0.25}{2} \\left( f(0) + 2 \\cdot \\left( f(0.25) + f(0.5) + f(0.75) \\right) + f(1) \\right)\n\n\nT_4 = \\frac{0.25}{2} \\left( 0^2 + 2 \\cdot (0.25^2 + 0.5^2 + 0.75^2) + 1^2 \\right)\n\n\nT_4 = \\frac{0.25}{2} \\cdot (0 + 2 \\cdot (0.0625 + 0.25 + 0.5625) + 1) = \\frac{0.25}{2} \\cdot 2.75 = 0.34375\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.34375 \\right| = 0.0104\n\n\n\n\n3. Summary of Results\n\n\n\nm\nApproximation\nError\n\n\n\n\n1\n0.5000\n0.1667\n\n\n2\n0.3750\n0.0417\n\n\n4\n0.3438\n0.0104\n\n\n\n\n\n4. Conclusion\nAs the number of panels m increases, the approximation becomes more accurate, and the error decreases. This demonstrates that the composite trapezoidal rule converges to the exact value as the number of panels increases."
  },
  {
    "objectID": "homework/w03/exercise3-2-6.html",
    "href": "homework/w03/exercise3-2-6.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree."
  },
  {
    "objectID": "homework/w03/exercise3-2-6.html#problem",
    "href": "homework/w03/exercise3-2-6.html#problem",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree."
  },
  {
    "objectID": "homework/w03/exercise3-2-6.html#solution",
    "href": "homework/w03/exercise3-2-6.html#solution",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "Solution:",
    "text": "Solution:\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\nFinal Degree 5 Polynomial\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\nExample\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\nDoes the Value of c Matter?\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "homework/w03/exercise3-2-2.html",
    "href": "homework/w03/exercise3-2-2.html",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "",
    "text": "Given the data points (1, 0), (2, \\ln 2), and (4, \\ln 4), find the degree 2 interpolating polynomial.\n\nUse the result of (a) to approximate \\ln 3.\n\nUse Theorem 3.3 to give an error bound for the approximation in part (b).\n\nCompare the actual error to your error bound."
  },
  {
    "objectID": "homework/w03/exercise3-2-2.html#problem",
    "href": "homework/w03/exercise3-2-2.html#problem",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "",
    "text": "Given the data points (1, 0), (2, \\ln 2), and (4, \\ln 4), find the degree 2 interpolating polynomial.\n\nUse the result of (a) to approximate \\ln 3.\n\nUse Theorem 3.3 to give an error bound for the approximation in part (b).\n\nCompare the actual error to your error bound."
  },
  {
    "objectID": "homework/w03/exercise3-2-2.html#key-concepts",
    "href": "homework/w03/exercise3-2-2.html#key-concepts",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nError in Polynomial Interpolation:\nThe error between the actual function f(x) and the interpolated polynomial P_{n-1}(x) is given by the following bound:\n\n|f(x) - P_{n-1}(x)| \\leq \\left| \\frac{(x - x_0)(x - x_1) \\dots (x - x_n)}{n!} f^{(n)}(c) \\right|\n\nwhere:\n\nn is the number of interpolation points.\nx_0, x_1, \\dots, x_n are the known data points.\nf^{(n)}(c) is the n-th derivative of the actual function f(x), evaluated at some point c in the interval [x_0, x_n]."
  },
  {
    "objectID": "homework/w03/exercise3-2-2.html#solution",
    "href": "homework/w03/exercise3-2-2.html#solution",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "Solution",
    "text": "Solution\n\n(a) Finding the Degree 2 Interpolating Polynomial\nGiven the points (1, 0), (2, \\ln 2), and (4, \\ln 4), we apply the Lagrange interpolation formula.\n\nLagrange Basis Polynomials:\n\nL_0(x):\n\nL_0(x) = \\frac{(x - 2)(x - 4)}{(1 - 2)(1 - 4)} = \\frac{(x - 2)(x - 4)}{3}\n\nL_1(x):\n\nL_1(x) = \\frac{(x - 1)(x - 4)}{(2 - 1)(2 - 4)} = -\\frac{(x - 1)(x - 4)}{2}\n\nL_2(x): \nL_2(x) = \\frac{(x - 1)(x - 2)}{(4 - 1)(4 - 2)} = \\frac{(x - 1)(x - 2)}{6}\n\n\n\n\nPolynomial Construction:\nNow, the interpolating polynomial becomes:\n\nP(x) = y_0 L_0(x) + y_1 L_1(x) + y_2 L_2(x)\n\nSince y_0 = 0, we have:\n\nP(x) = \\ln 2 \\cdot \\left(-\\frac{(x - 1)(x - 4)}{2}\\right) + \\ln 4 \\cdot \\frac{(x - 1)(x - 2)}{6}\n\nWe know that \\ln 4 = 2 \\ln 2, so the polynomial simplifies to:\n\nP(x) = \\ln 2 \\cdot \\left(-\\frac{(x - 1)(x - 4)}{2} + \\frac{(x - 1)(x - 2)}{3}\\right)\n\n\n\n\n(b) Approximation of \\ln 3\nTo approximate \\ln 3, substitute x = 3 into the polynomial:\n\nP(3) = \\ln 2 \\cdot \\left(-\\frac{(3 - 1)(3 - 4)}{2} + \\frac{(3 - 1)(3 - 2)}{3}\\right)\n\nSimplifying:\n\nP(3) = \\ln 2 \\cdot \\left(1 + \\frac{2}{3}\\right) = \\ln 2 \\cdot \\frac{5}{3}\n\nSince \\ln 2 \\approx 0.6931, we have:\n\nP(3) \\approx \\frac{5}{3} \\cdot 0.6931 \\approx 1.1552\n\nThus, the approximation for \\ln 3 is:\n\nP(3) \\approx 1.1552\n\n\n\n(c) Error Bound using Theorem 3.3\nThe error formula for degree 2 interpolation is:\n\n|f(x) - P(x)| \\leq \\left| \\frac{(x - x_0)(x - x_1)(x - x_2)}{3!} f^{(3)}(c) \\right|\n\nwhere f(x) = \\ln(x) and f^{(3)}(x) = \\frac{2}{x^3}. The maximum of f^{(3)}(x) occurs at x = 1, giving:\n\nf^{(3)}(1) = 2\n\nSubstituting into the error formula for x = 3:\n\nE(3) = \\frac{(3 - 1)(3 - 2)(3 - 4)}{6} \\cdot 2 = \\frac{(2)(1)(-1)}{6} \\cdot 2 = -\\frac{4}{6} = -\\frac{2}{3}\n\nTherefore, the error bound is approximately -\\frac{2}{3} \\approx -0.6667.\n\n\n(d) Comparison of Actual Error and Error Bound\nThe actual value of \\ln 3 is approximately:\n\n\\ln 3 \\approx 1.0986\n\nOur approximation was P(3) \\approx 1.1552. Therefore, the actual error is:\n\n\\text{Actual error} = |1.0986 - 1.1552| \\approx 0.0566\n\nThe error bound is approximately 0.6667, which is larger than the actual error. This confirms that the actual error is well within the error bound, as expected."
  },
  {
    "objectID": "homework/math411-suggested-homework.html",
    "href": "homework/math411-suggested-homework.html",
    "title": "MATH411 Suggested Homework - Fall 2024",
    "section": "",
    "text": "Week 1\n\n(C0-P1) Read/work through the Getting Started with Python notebook (see Modules &gt; Homework &gt; Getting_Started_with_Python.ipynb). Then complete the following exercises in a separate notebook or .py file:\n\nWhat is the difference between the outputs generated by the following two lines of code?\n\nnp.array([i for i in range(10)])\nnp.linspace(0, 9, 10, endpoint=True)\n\nUse both np.linspace() and np.arange() to create an array containing floating point numbers starting at 1.0, ending at 4.0, equally spaced with separation 0.2. In other words, the array should contain 1.0, 1.2, 1.4, …, 3.8, 4.0.\nCreate an array consisting of the floats 1.0, 2.0, 3.0, 4.0, and 5.0. Create a second array containing the square root of each of these numbers. Then, use a for loop to compute the sum of the squared differences between the two arrays:\n\n\n\\sum_{i=1}^n \\left(x_i - \\sqrt{x_i}\\right)^2\n\nExtra Challenge: Can you do this without a loop?\n\nStarting with x = 1, use a while loop to divide by 2 until x &lt; 10^{-4}. Display (print) the list 1.0, 0.5, 0.25, …, and report the number of divisions by 2 needed such that the (k-1)th division produces x &gt; 10^{-4} and the kth division produces x &lt; 10^{-4}.\nWrite code to create a function to compute f(x) = e^{-x} \\cos x, where x is a vector (array) of one or more numbers. Then evaluate f(x) at the points 0, 0.1, 0.2, …, 1.0.\nWrite code to plot the function h(x) = e^{x} \\cos^2 x - 2 on the interval -0.5 to 5.5 and visually estimate the roots of h(x) on that interval.\n\n(C1-P1) Exercise 1.1.4ab\n(C1-P2) Exercise 1.2.2\n(C1-P3) Computer Problem 1.2.2ab. For each equation, find an initial point x_0 and a function g(x) such that the fixed-point iteration x_{k+1} = g(x_k) converges to x, where g(x) = x. If this is not possible, explain why.\n(C1-P4) Exercise 1.2.14\n(C1-P5) Exercise 1.4.1\n(C1-P6) Exercise 1.4.3\n\n\nWeek 2\n\n(C1-P7) Exercise 1.4.6\n(C1-P8) Exercise 1.4.8\n(C1-P9) Computer Problem 1.4.7\n(C1-P10) Exercise 1.5.1\n(C1-P11) Use Python to compare results obtained using the Bisection Method, Newton’s Method, and the Secant Method to solve the equation \\ln x + x^2 = 3. Note that Python code for these methods is available on I-Learn.\n\nSolve the problem using each of the three methods. Report starting values and the number of iterations required to obtain 6 correct decimal places of accuracy. Hint: a graph of the function may help with starting values.\nOn the same axes, plot \\log(\\epsilon_{i+1}) vs. \\log(\\epsilon_i) for the three methods. Explain your plot. How is it related to the rate or order of convergence? Use the errors to determine if your results are consistent with theory. How would you compute the error if you didn’t have an exact value for the root?\n\n(C3-P1) Exercise 3.1.1ac\n(C3-P2) Exercise 3.1.2ac\n(C3-P3) Exercise 3.1.6\n\n\nWeek 3\n\n(C0-P2) Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array—choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n(C3-P4) Exercise 3.2.2\n(C3-P5) Exercise 3.2.5\n(C3-P6) Exercise 3.2.6 Note: the two additional points in the next-to-last sentence should be (x_7, y_7) = (0.1, f(0.1)) and (x_8, y_8) = (0.5, f(0.5)).\n(C3-P7) Computer Problem 3.1.3. To demonstrate that your function works, interpolate \\sin(x) on the interval [-\\pi, \\pi] using nodes -\\pi, -\\frac{\\pi}{2}, 0, \\frac{\\pi}{2}, \\pi. Plot your interpolating polynomial. Plot \\sin(x) on the same graph, and use the numpy functions polyfit and polyval to plot an interpolating polynomial from Python on the same graph. Use a legend to make clear which curve is yours and which one came from Python. Hint: the code on p. 146 should help. Python versions of newtdd and nest (p. 3) are available in Canvas.\n(C3-P8) Exercise 3.3.2ac\n(C3-P9) Exercise 3.3.2ac\n(C3-P10) Exercise 3.3.3\n\n\nWeek 4\n\n(C5-P1) Exercise 5.2.1ab\n(C5-P2) Exercise 5.2.2ab\n(C5-P3) Exercise 5.2.3ab\n(C5-P4) Exercise 5.2.10\n(C5-P5) Exercise 5.2.12\n(C5-P6) Computer Problem 5.2.1ac\n(C5-P7) Computer Problem 5.2.2de\n(C5-P8) Computer Problem 5.2.9bf\n\n\nWeek 5\n\n(C5-P9) Exercise 5.5.1ab\n(C5-P10) Exercise 5.5.4cd\n(C5-P11) Computer Problem 5.4.1acd\n(C5-P12) Computer Problem 5.4.2\n(C5-P13) Computer Problem 5.4.3acd\n(C5-P14) Exercise 5.5.5cd\n(C5-P15) Exercise 5.5.7\n\n\nWeek 6\n\n(C2-P1) Exercise 2.1.2ac\n(C2-P2) Computer Problem 2.1.2ac\n(C2-P3) Exercise 2.2.1ab\n(C2-P4) Exercise 2.2.2ab\n\n\nWeek 7\n\n(C2-P5) Exercise 2.2.4\n(C2-P6) Computer Problem 2.2.1ab\n(C2-P7) Exercise 2.4.1ab\n(C2-P8) Exercise 2.4.2ab\n(C2-P9) Exercise 2.4.4a\n(C2-P10) Exercise 2.4.6\n\n\nWeek 8\n\n(C2-P11) Exercise 2.5.2ab\n(C2-P12) Computer Problem 2.5.2 (solve using both Jacobi and Gauss-Seidel, compare results)\n(C4-P1) Exercise 4.1.2\n(C4-P2) Computer Problem 4.1.5 (also use a quadratic fit and compare)\n(C4-P3) Exercise 4.3.2\n\n\nWeek 9\n\n(C4-P4) Exercise 4.3.4 (use the matrix from Exercise 4.3.1d)\n(C4-P5) Exercise 4.3.7 (you can use your QR factorizations from Exercise 4.3.2)\n(C4-P6) Computer Problem 4.3.4 Additional instructions: Write a classical Gram-Schmidt code only. Use the matrices in Exercise 4.3.2 to check your code. If you use the code I provided, you must comment it (explain what every line does).\n\n\nWeek 10\n\n(C4-P7) Exercise 4.4.2\n(C4-P8) Exercise 4.4.3\n(C4-P9) Computer Problem 4.4.2 (find a preconditioned GMRES Python code and use it)\n\n\nWeek 11\n\nNone\nExam 2 in class on Monday Week 11\nAttempt the first several Week 12 problems before Monday Week 12\n\n\nWeek 12\n\n(C10-P1) Exercise 10.1.1ad (also, find the inverse DFT of your result, compare to the original vector)\n(C10-P2) Exercise 10.1.8\n(C10-P3) Exercise 10.2.1ab\n(C10-P4) Exercise 10.2.3ab\n(C10-P5) Exercise 10.2.3 (plot data and function to show your interpolating function does interpolate the data)\n(C10-P6) Computer Problem 10.2.4\n(C10-P7) Exercise 10.3.2ab\n\n\nWeek 13\n\n(C10-P8) Computer Problem 10.3.2cd\n(C10-P9) Exercise 10.3.5 (Complete the \\sum_{j=0}^{n-1} \\cos \\frac{2 \\pi j k}{n} \\cos \\frac{2 \\pi j l}{n} result only)\n(C10-P10) Exercise 10.1.6"
  },
  {
    "objectID": "homework/index.html",
    "href": "homework/index.html",
    "title": "HOMEWORK",
    "section": "",
    "text": "Suggested Homework\n\nWEEK 01\n\n(C0-P1)\n(C1-P1) Exercise 1.1.4ab\n(C1-P2) Exercise 1.2.2\n(C1-P3)\n(C1-P4) Exercise 1.2.14\n(C1-P5) Exercise 1.4.1\n(C1-P6) Exercise 1.4.3\n\n\n\nWEEK 02\n\n(C1-P7) Exercise 1.4.6\n(C1-P8) Exercise 1.4.8\n(C1-P9) Computer Problem 1.4.7\n(C1-P10) Exercise 1.5.1\n(C1-P11)\n(C3-P1) Exercise 3.1.1a\n(C3-P1) Exercise 3.1.1c\n(C3-P2) Exercise 3.1.2a\n(C3-P2) Exercise 3.1.2c\n(C3-P3) Exercise 3.1.6\n\n\n\nWEEK 03\n\n(C0-P2)\n(C3-P4) Exercise 3.2.2\n(C3-P5) Exercise 3.2.5\n(C3-P6) Exercise 3.2.6\n(C3-P7) Computer Problem 3.1.3\n(C3-P8) Exercise 3.3.1ac\n(C3-P9) Exercise 3.3.2ac\n(C3-P10) Exercise 3.3.3\n\n\n\nWEEK 04\n\n(C5-P1) Exercise 5.2.1a\n(C5-P1) Exercise 5.2.1b\n(C5-P2) Exercise 5.2.2ab\n(C5-P3) Exercise 5.2.3ab\n(C5-P4) Exercise 5.2.10\n(C5-P5) Exercise 5.2.12\n(C5-P6) Computer Problem 5.2.1ac\n(C5-P7) Computer Problem 5.2.2de\n(C5-P8) Computer Problem 5.2.9bf\n\n\n\nWEEK 05\n\n(C5-P9) Exercise 5.5.1ab\n(C5-P10) Exercise 5.5.4cd\n(C5-P11) Computer Problem 5.4.1acd\n(C5-P12) Computer Problem 5.4.2\n(C5-P13) Computer Problem 5.4.3acd\n(C5-P14) Exercise 5.5.5cd\n(C5-P15) Exercise 5.5.7\n\n\n\nWEEK 06\n\n(C2-P1) Exercise 2.1.2a\n(C2-P1) Exercise 2.1.2c\n(C2-P2) Computer Problem 2.1.2ac\n(C2-P3) Exercise 2.2.1ab\n(C2-P4) Exercise 2.2.2ab\n\n\n\nWEEK 07\n\n(C2-P5) Exercise 2.2.4\n(C2-P6) Computer Problem 2.2.1ab\n(C2-P7) Exercise 2.4.1ab\n(C2-P8) Exercise 2.4.2ab\n(C2-P9) Exercise 2.4.4a\n(C2-P10) Exercise 2.4.6\n\n\n\nWEEK 08\n\n(C2-P11) Exercise 2.5.2ab\n(C2-P12) Computer Problem 2.5.2 (solve using both Jacobi and Gauss-Seidel, compare results)\n(C4-P1) Exercise 4.1.2a\n(C4-P1) Exercise 4.1.2b\n(C4-P2) Computer Problem 4.1.5 (also use a quadratic fit and compare)\n(C4-P3) Exercise 4.3.2\n\n\n\nWEEK 09\n\nExercise 4.3.1d\n(C4-P4) Exercise 4.3.4\n(C4-P5) Exercise 4.3.7 (you can use your QR factorizations from Exercise 4.3.2)\n(C4-P6) Computer Problem 4.3.4 Additional instructions: Write a classical Gram-Schmidt code only. Use the matrices in Exercise 4.3.2 to check your code. If you use the code I provided, you must comment it (explain what every line does).\n\n\n\nWEEK 10\n\n(C4-P7) Exercise 4.4.2\n(C4-P8) Exercise 4.4.3\n(C4-P9) Computer Problem 4.4.2 (find a preconditioned GMRES Python code and use it)\n\n\n\nWEEK 11\nNone\n\n\nWEEK 12\n\n(C10-P1) Exercise 10.1.1ad (also, find the inverse DFT of your result, compare to the original vector)\n(C10-P2) Exercise 10.1.8\n(C10-P3) Exercise 10.2.1ab\n(C10-P4) Exercise 10.2.3ab\n(C10-P5) Exercise 10.2.3 (plot data and function to show your interpolating function does interpolate the data)\n(C10-P6) Computer Problem 10.2.4\n(C10-P7) Exercise 10.3.2ab\n\n\n\nWEEK 13\n\n(C10-P8) Computer Problem 10.3.2cd\n(C10-P9) Exercise 10.3.5 (Complete the \\sum\\_{j=0}^{n-1} \\cos \\frac{2 \\pi j k}{n} \\cos \\frac{2 \\pi j l}{n} result only)\n(C10-P10) Exercise 10.1.6"
  },
  {
    "objectID": "homework/w02/exercise3-1-6.html",
    "href": "homework/w02/exercise3-1-6.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nWe will use Newton’s divided differences to first construct the degree 3 polynomial, and then extend it to a degree 5 polynomial.\n\n\n\nWe will use the given points (x_1, y_1) = (1, 1), (x_2, y_2) = (2, 3), (x_3, y_3) = (3, 3), and (x_4, y_4) = (4, 4) to compute the divided differences and build the polynomial.\n\n\n\n\n\n\n\n\n\n\nx\nf(x)\nFirst Difference f[x_i, x_{i+1}]\nSecond Difference f[x_i, x_{i+1}, x_{i+2}]\nThird Difference f[x_i, x_{i+1}, x_{i+2}, x_{i+3}]\n\n\n\n\n1\n1\n\n\n\n\n\n2\n3\n\\frac{3 - 1}{2 - 1} = 2\n\n\n\n\n3\n3\n\\frac{3 - 3}{3 - 2} = 0\n\\frac{0 - 2}{3 - 1} = -1\n\n\n\n4\n4\n\\frac{4 - 3}{4 - 3} = 1\n\\frac{1 - 0}{4 - 2} = \\frac{1}{2}\n\\frac{\\frac{1}{2} - (-1)}{4 - 1} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\nThe Newton’s divided difference form of the interpolating polynomial is:\n\nP_3(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + f[x_1, x_2, x_3, x_4](x - x_1)(x - x_2)(x - x_3)\n\nSubstituting the values from the divided difference table:\n\nP_3(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3)\n\n\n\n\nTo create a degree 5 polynomial, we add an extra term c(x - 1)(x - 2)(x - 3)(x - 4), which evaluates to zero at the given points:\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nSubstitute P_3(x):\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\n\nThe value of c does not affect the polynomial at the given points. The additional term evaluates to 0 at x = 1, 2, 3, 4, so no matter what c is, the polynomial will pass through the points (1, 1), (2, 3), (3, 3), (4, 4).\nHowever, changing c will affect the polynomial’s behavior outside the given points. For different values of c, the polynomial will look different outside the interpolation points.\n\n\n\nThus, the degree 5 polynomial is:\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThis polynomial passes through the points (1, 1), (2, 3), (3, 3), (4, 4), and c is an arbitrary constant that influences how the polynomial behaves outside of those points."
  },
  {
    "objectID": "homework/w02/exercise3-1-6.html#question",
    "href": "homework/w02/exercise3-1-6.html#question",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nWe will use Newton’s divided differences to first construct the degree 3 polynomial, and then extend it to a degree 5 polynomial.\n\n\n\nWe will use the given points (x_1, y_1) = (1, 1), (x_2, y_2) = (2, 3), (x_3, y_3) = (3, 3), and (x_4, y_4) = (4, 4) to compute the divided differences and build the polynomial.\n\n\n\n\n\n\n\n\n\n\nx\nf(x)\nFirst Difference f[x_i, x_{i+1}]\nSecond Difference f[x_i, x_{i+1}, x_{i+2}]\nThird Difference f[x_i, x_{i+1}, x_{i+2}, x_{i+3}]\n\n\n\n\n1\n1\n\n\n\n\n\n2\n3\n\\frac{3 - 1}{2 - 1} = 2\n\n\n\n\n3\n3\n\\frac{3 - 3}{3 - 2} = 0\n\\frac{0 - 2}{3 - 1} = -1\n\n\n\n4\n4\n\\frac{4 - 3}{4 - 3} = 1\n\\frac{1 - 0}{4 - 2} = \\frac{1}{2}\n\\frac{\\frac{1}{2} - (-1)}{4 - 1} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\nThe Newton’s divided difference form of the interpolating polynomial is:\n\nP_3(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + f[x_1, x_2, x_3, x_4](x - x_1)(x - x_2)(x - x_3)\n\nSubstituting the values from the divided difference table:\n\nP_3(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3)\n\n\n\n\nTo create a degree 5 polynomial, we add an extra term c(x - 1)(x - 2)(x - 3)(x - 4), which evaluates to zero at the given points:\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nSubstitute P_3(x):\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\n\nThe value of c does not affect the polynomial at the given points. The additional term evaluates to 0 at x = 1, 2, 3, 4, so no matter what c is, the polynomial will pass through the points (1, 1), (2, 3), (3, 3), (4, 4).\nHowever, changing c will affect the polynomial’s behavior outside the given points. For different values of c, the polynomial will look different outside the interpolation points.\n\n\n\nThus, the degree 5 polynomial is:\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThis polynomial passes through the points (1, 1), (2, 3), (3, 3), (4, 4), and c is an arbitrary constant that influences how the polynomial behaves outside of those points."
  },
  {
    "objectID": "homework/w03/exercise3-2-5.html",
    "href": "homework/w03/exercise3-2-5.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\n\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\n\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "homework/w03/exercise3-2-5.html#problem",
    "href": "homework/w03/exercise3-2-5.html#problem",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\n\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\n\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "homework/w03/exercise3-3-3.html",
    "href": "homework/w03/exercise3-3-3.html",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "",
    "text": "Assume that Chebyshev interpolation is used to find a fifth-degree interpolating polynomial Q_5(x) on the interval [-1, 1] for the function f(x) = e^x. Use the interpolation error formula to find a worst-case estimate for the error |e^x - Q_5(x)| that is valid for x throughout the interval [-1, 1]. How many digits after the decimal point will be correct when Q_5(x) is used to approximate e^x?"
  },
  {
    "objectID": "homework/w03/exercise3-3-3.html#problem",
    "href": "homework/w03/exercise3-3-3.html#problem",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "",
    "text": "Assume that Chebyshev interpolation is used to find a fifth-degree interpolating polynomial Q_5(x) on the interval [-1, 1] for the function f(x) = e^x. Use the interpolation error formula to find a worst-case estimate for the error |e^x - Q_5(x)| that is valid for x throughout the interval [-1, 1]. How many digits after the decimal point will be correct when Q_5(x) is used to approximate e^x?"
  },
  {
    "objectID": "homework/w03/exercise3-3-3.html#solution",
    "href": "homework/w03/exercise3-3-3.html#solution",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "Solution:",
    "text": "Solution:\nWe need to compute the worst-case error for the interpolation of f(x) = e^x using a Chebyshev interpolating polynomial Q_5(x). We will use the interpolation error formula:\n\n|f(x) - P(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\max_{x \\in [-1,1]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|\n\nWhere M is an upper bound on the 6th derivative of f(x) = e^x over [-1, 1], and x_1, x_2, \\dots, x_n are the Chebyshev nodes.\n\nSteps:\n\nChebyshev Node Bound: For Chebyshev interpolation on the interval [-1, 1], the product (x - x_1)(x - x_2) \\cdots (x - x_n) is bounded by \\frac{1}{2^n}. For n = 5, this becomes:\n\n\\frac{1}{2^5} = \\frac{1}{32}\n\nSixth Derivative of e^x: The 6th derivative of f(x) = e^x is f^{(6)}(x) = e^x, and the maximum value of this derivative on the interval [-1, 1] is at x = 1, where f^{(6)}(1) = e \\approx 2.718.\nFactorial Term: The term 6! = 720.\nError Bound Formula: Now, we plug these values into the error bound formula:\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{32}\n\nSubstituting e \\approx 2.718, we get:\n\n|e^x - Q_5(x)| \\leq \\frac{2.718}{720 \\times 32} \\approx \\frac{2.718}{23,040} \\approx 0.000118\n\nThis is the worst-case error estimate for the approximation of e^x on the interval [-1, 1].\nCorrect Decimal Places: Since the error bound is approximately 0.000118, this means we can expect approximately 3 correct digits after the decimal point. The error affects the fourth decimal place, but the first three digits are expected to be correct.\nTherefore, the approximation Q_5(x) will be accurate to 3 digits after the decimal point when approximating e^x on the interval [-1, 1]."
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html",
    "href": "homework/w07/exercise2-4-4a.html",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "",
    "text": "Solve the system by finding the PA = LU factorization and then carrying out the two-step back substitution:\n\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 4 & 4 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html#problem",
    "href": "homework/w07/exercise2-4-4a.html#problem",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "",
    "text": "Solve the system by finding the PA = LU factorization and then carrying out the two-step back substitution:\n\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 4 & 4 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html#solution",
    "href": "homework/w07/exercise2-4-4a.html#solution",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "Solution:",
    "text": "Solution:\n\nLU Factorization with Partial Pivoting\nStep 1: First Column\n\nPivot Selection: Both a_{11} = 4 and a_{21} = 4 are tied for the largest absolute value. Choose a_{11} as the pivot (no row swap needed).\nCompute Multipliers and Eliminate Below Pivot:\n\nRow 2:\n\nL_{21} = \\dfrac{a_{21}}{a_{11}} = \\dfrac{4}{4} = 1\n\n\n\\text{Row 2} \\rightarrow \\text{Row 2} - L_{21} \\times \\text{Row 1}\n\n\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix}\n\nRow 3: \nL_{31} = \\dfrac{a_{31}}{a_{11}} = \\dfrac{2}{4} = \\dfrac{1}{2}\n \n\\text{Row 3} \\rightarrow \\text{Row 3} - L_{31} \\times \\text{Row 1}\n \n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 0 & 1 & 3 \\end{pmatrix}\n\n\n\nStep 2: Second Column\n\nPivot Selection: U_{22} = 2 is the largest absolute value below the pivot (no row swap needed).\nCompute Multiplier and Eliminate Below Pivot:\n\nRow 3: \nL_{32} = \\dfrac{U_{32}}{U_{22}} = \\dfrac{1}{2}\n \n\\text{Row 3} \\rightarrow \\text{Row 3} - L_{32} \\times \\text{Row 2}\n \n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n\n\n\nResulting Matrices\n\nLower Triangular Matrix L:\n\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\dfrac{1}{2} & \\dfrac{1}{2} & 1\n\\end{pmatrix}\n\nUpper Triangular Matrix U:\n\nU = \\begin{pmatrix}\n4 & 2 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\nPermutation Matrix P: \nP = I \\quad (\\text{identity matrix, since no row swaps were performed})\n\n\n\n\nForward Substitution: Solve Ly = b\n\nL \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}\n\n\nEquation 1:\n\ny_1 = 2\n\nEquation 2:\n\ny_2 = b_2 - L_{21} y_1 = 4 - (1)(2) = 2\n\nEquation 3: \ny_3 = b_3 - L_{31} y_1 - L_{32} y_2 = 6 - \\left( \\dfrac{1}{2} \\times 2 \\right) - \\left( \\dfrac{1}{2} \\times 2 \\right) = 6 - 1 - 1 = 4\n\n\nSolution:\n\ny = \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix}"
  },
  {
    "objectID": "homework/w07/exercise2-4-4a.html#back-substitution-solve-ux-y",
    "href": "homework/w07/exercise2-4-4a.html#back-substitution-solve-ux-y",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "Back Substitution: Solve Ux = y",
    "text": "Back Substitution: Solve Ux = y\n\nU \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix}\n\n\nEquation 3:\n\n2 x_3 = y_3 \\implies x_3 = \\dfrac{y_3}{2} = \\dfrac{4}{2} = 2\n\nEquation 2:\n\n2 x_2 + 2 x_3 = y_2 \\implies x_2 = \\dfrac{y_2 - 2 x_3}{2} = \\dfrac{2 - (2 \\times 2)}{2} = \\dfrac{-2}{2} = -1\n\nEquation 1: \n4 x_1 + 2 x_2 = y_1 \\implies x_1 = \\dfrac{y_1 - 2 x_2}{4} = \\dfrac{2 - (2 \\times -1)}{4} = \\dfrac{2 + 2}{4} = \\dfrac{4}{4} = 1\n\n\nSolution:\n\nx = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\nFinal Answer\nThe solution to the system is:\n\nx_1 = 1, \\quad x_2 = -1, \\quad x_3 = 2"
  },
  {
    "objectID": "homework/w08/exercise4-1-2a.html",
    "href": "homework/w08/exercise4-1-2a.html",
    "title": "Exercise 4.1.2a (C4-P1)",
    "section": "",
    "text": "Problem:\n\n\n\n\n\n\n4.1.2a\n\n\n\nFind the least squares solution \\mathbf{\\hat{x}} and the RMSE of the following system:\n\n\\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n1 & 2 & 1 \\\\\n1 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix}\n\n\n\n\n\nSolution:\n\nCompute A^\\top A and A^\\top \\mathbf{b}\nThe transpose of A is:\n\nA^\\top =\n\\begin{bmatrix}\n1 & 0 & 1 & 1 \\\\\n1 & 1 & 2 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{bmatrix}\n\nCompute A^\\top A:\n\nA^\\top A =\n\\begin{bmatrix}\n3 & 3 & 2 \\\\\n3 & 6 & 3 \\\\\n2 & 3 & 3\n\\end{bmatrix}\n\nCompute A^\\top \\mathbf{b}:\n\nA^\\top \\mathbf{b} =\n\\begin{bmatrix}\n9 \\\\ 10 \\\\ 9\n\\end{bmatrix}\n\nThe normal equation is:\n\nA^\\top A \\mathbf{\\hat{x}} = A^\\top \\mathbf{b}\n\n\n\nSolve A^\\top A \\mathbf{\\hat{x}} = A^\\top \\mathbf{b} using Row Reduction\nThe augmented matrix for the system is:\n\n\\left[\n\\begin{array}{ccc|c}\n3 & 3 & 2 & 9 \\\\\n3 & 6 & 3 & 10 \\\\\n2 & 3 & 3 & 9\n\\end{array}\n\\right]\n\n\nNormalize Row 1\nDivide Row 1 by 3:\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & \\frac{2}{3} & 3 \\\\\n3 & 6 & 3 & 10 \\\\\n2 & 3 & 3 & 9\n\\end{array}\n\\right]\n\n\n\nEliminate the first column in Rows 2 and 3\n\nR_2 \\to R_2 - 3R_1\nR_3 \\to R_3 - 2R_1\n\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & \\frac{2}{3} & 3 \\\\\n0 & 3 & 1 & 1 \\\\\n0 & 1 & \\frac{5}{3} & 3\n\\end{array}\n\\right]\n\n\n\nNormalize Row 2\nDivide Row 2 by 3:\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & \\frac{2}{3} & 3 \\\\\n0 & 1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 1 & \\frac{5}{3} & 3\n\\end{array}\n\\right]\n\n\n\nEliminate the second column in Rows 1 and 3\n\nR_1 \\to R_1 - R_2\nR_3 \\to R_3 - R_2\n\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & \\frac{4}{9} & \\frac{8}{3} \\\\\n0 & 1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & \\frac{4}{3} & \\frac{8}{3}\n\\end{array}\n\\right]\n\n\n\nNormalize Row 3\nDivide Row 3 by \\frac{4}{3}:\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & \\frac{4}{9} & \\frac{8}{3} \\\\\n0 & 1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & 1 & 2\n\\end{array}\n\\right]\n\n\n\nEliminate the third column in Rows 1 and 2\n\nR_1 \\to R_1 - \\frac{4}{9}R_3\nR_2 \\to R_2 - \\frac{1}{3}R_3\n\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -\\frac{1}{3} \\\\\n0 & 0 & 1 & 2\n\\end{array}\n\\right]\n\nThus, the least squares solution is:\n\n\\mathbf{\\hat{x}} =\n\\begin{bmatrix}\n2 \\\\ -\\frac{1}{3} \\\\ 2\n\\end{bmatrix}\n\n\n\n\nCompute the Residual\nThe residual is:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{\\hat{x}}.\n\nSubstituting \\mathbf{\\hat{x}}:\n\n\\mathbf{r} =\n\\begin{bmatrix}\n2 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n1 + (-\\frac{1}{3}) \\\\ -\\frac{1}{3} + 2 \\\\ 2 + 2(-\\frac{1}{3}) + 2 \\\\ 2 + 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ -\\frac{1}{3} \\\\ 0\n\\end{bmatrix}\n\n\n\nCompute RMSE\nThe RMSE is given by:\n\n\\text{RMSE} = \\sqrt{\\frac{\\|\\mathbf{r}\\|^2}{n}}\n\nFirst, compute \\|\\mathbf{r}\\|^2:\n\n\\|\\mathbf{r}\\|^2 = \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 + 0^2 = \\frac{1}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{1}{3}\n\nSubstitute into the RMSE formula:\n\n\\text{RMSE} = \\sqrt{\\frac{\\frac{1}{3}}{4}} = \\sqrt{\\frac{1}{12}}\n\nThe approximate value is:\n\n\\text{RMSE} \\approx 0.2887\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\n\nLeast Squares Solution:\n\n\\hat{x}_1 = 2, \\quad \\hat{x}_2 = -\\frac{1}{3}, \\quad \\hat{x}_3 = 2\n\nResidual:\n\n\\mathbf{r} =\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ -\\frac{1}{3} \\\\ 0 \\end{bmatrix}\n\nRMSE: \n\\sqrt{\\frac{1}{12}} \\approx 0.2887"
  },
  {
    "objectID": "homework/w09/exercise4-3-4.html",
    "href": "homework/w09/exercise4-3-4.html",
    "title": "Exercise 4.3.4 (C4-P4)",
    "section": "",
    "text": "Problem\n\n\nSolution"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "NOTES",
    "section": "",
    "text": "WEEK 01\n\nBisection Method\nFixed-Point Iteration\nNewtons’s Method\nSecant Method\n\n\n\nWEEK 02\n\nLagrange Interpolation\nNewton’s Divided Differences\n\n\n\nWEEK 03\n\nInterpolation Error Formula\nRunge Phenomenon\nChebyshev Interpolation\n\n\n\nWEEK 04\n\nTrapezoidal Rule\n\n\n\nWEEK 05\n\n\nWEEK 06\n\nLinear Systems: A\\mathbf{x} = \\mathbf{b}\nSpectral Radius\n\n\n\nWEEK 07\n\nLU Factorization\nIterative Methods for Solving Linear Systems\nChapter 2 Section 3 Notes\nNorms\nLinear System Error Analysis\n\n\n\nWEEK 08\n\nLeast Squares Solution for Inconsistent Systems\nGram-Schmidt Orthogonalization\nModified Gram-Schmidt Orthogonalization\n\n\n\nWEEK 09\n\n\nWEEK 10\n\n\nWEEK 11\n\n\nWEEK 12"
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html",
    "href": "notes/w01/fixed-point-iteration.html",
    "title": "Fixed-Point Iteration",
    "section": "",
    "text": "Fixed-Point Iteration is a simple numerical method for solving equations of the form x = g(x). It is based on the idea of iteratively applying a function g(x) to approximate a fixed point x^*, where x^* = g(x^*). This method is commonly used in numerical root-finding and optimization."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#overview",
    "href": "notes/w01/fixed-point-iteration.html#overview",
    "title": "Fixed-Point Iteration",
    "section": "",
    "text": "Fixed-Point Iteration is a simple numerical method for solving equations of the form x = g(x). It is based on the idea of iteratively applying a function g(x) to approximate a fixed point x^*, where x^* = g(x^*). This method is commonly used in numerical root-finding and optimization."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#fixed-point-iteration-formula",
    "href": "notes/w01/fixed-point-iteration.html#fixed-point-iteration-formula",
    "title": "Fixed-Point Iteration",
    "section": "Fixed-Point Iteration Formula",
    "text": "Fixed-Point Iteration Formula\nThe fixed-point iteration method uses the recursive formula:\n\nx_{k+1} = g(x_k)\n\nwhere x_k is the k-th approximation of the solution. Starting from an initial guess x_0, the sequence of approximations is generated iteratively, and convergence is expected when |x_{k+1} - x_k| becomes sufficiently small."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#convergence-criteria",
    "href": "notes/w01/fixed-point-iteration.html#convergence-criteria",
    "title": "Fixed-Point Iteration",
    "section": "Convergence Criteria",
    "text": "Convergence Criteria\nFor the fixed-point iteration to converge, certain conditions must be met:\n\nThe function g(x) must be continuous.\nThe derivative g'(x) at the fixed point x^* must satisfy: \n|g'(x^*)| &lt; 1\n\n\nIf |g'(x^*)| \\geq 1, the method may fail to converge."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#step-by-step-procedure",
    "href": "notes/w01/fixed-point-iteration.html#step-by-step-procedure",
    "title": "Fixed-Point Iteration",
    "section": "Step-by-Step Procedure",
    "text": "Step-by-Step Procedure\n\nRewrite the given equation f(x) = 0 in the form x = g(x).\nChoose an initial guess x_0.\nApply the iteration formula: \nx_{k+1} = g(x_k)\n\nRepeat the iteration until |x_{k+1} - x_k| &lt; \\epsilon, where \\epsilon is the tolerance.\nThe final x_k is an approximate solution."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#example",
    "href": "notes/w01/fixed-point-iteration.html#example",
    "title": "Fixed-Point Iteration",
    "section": "Example",
    "text": "Example\nConsider the equation:\n\nx^2 - 2 = 0\n\nRewriting it as x = g(x):\n\ng(x) = \\frac{2}{x}\n\n\nIterative Steps\n\nInitial Guess: x_0 = 1.5.\nFirst Iteration: \nx_1 = g(x_0) = \\frac{2}{1.5} = 1.3333\n\nSecond Iteration: \nx_2 = g(x_1) = \\frac{2}{1.3333} \\approx 1.5\n\nThird Iteration: \nx_3 = g(x_2) = \\frac{2}{1.5} \\approx 1.3333\n\n\nThe values oscillate around the solution \\sqrt{2}. With more iterations and a smaller tolerance \\epsilon, the method converges to the actual solution."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#convergence-and-stability",
    "href": "notes/w01/fixed-point-iteration.html#convergence-and-stability",
    "title": "Fixed-Point Iteration",
    "section": "Convergence and Stability",
    "text": "Convergence and Stability\nFor fixed-point iteration to converge:\n\n|g'(x^*)| &lt; 1 ensures stability near the fixed point.\nPoorly chosen g(x) or initial guesses can lead to divergence or slow convergence.\n\n\nExample of Divergence\nIf g(x) is poorly chosen such that |g'(x^*)| &gt; 1, the method may fail to converge. For instance, using g(x) = x^2 for the same equation would cause the iteration to diverge."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#applications",
    "href": "notes/w01/fixed-point-iteration.html#applications",
    "title": "Fixed-Point Iteration",
    "section": "Applications",
    "text": "Applications\n\nRoot-Finding: Solve equations like f(x) = 0.\nDynamical Systems: Analyze equilibrium points.\nOptimization: Solve constraints arising in optimization problems."
  },
  {
    "objectID": "notes/w01/fixed-point-iteration.html#advantages-and-limitations",
    "href": "notes/w01/fixed-point-iteration.html#advantages-and-limitations",
    "title": "Fixed-Point Iteration",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages\n\nSimple Implementation: Requires minimal computation.\nVersatile: Applicable to a wide range of problems.\n\n\n\nLimitations\n\nConvergence Issues: Sensitive to g(x) and initial guess.\nSlow Convergence: May require many iterations for highly accurate solutions."
  },
  {
    "objectID": "notes/w01/secant-method.html",
    "href": "notes/w01/secant-method.html",
    "title": "Secant Method",
    "section": "",
    "text": "The Secant Method is a numerical method for finding roots of a nonlinear equation f(x) = 0. It is similar to Newton’s Method, but it does not require the computation of the derivative f'(x). Instead, the Secant Method approximates the derivative using a secant line through two points on the function."
  },
  {
    "objectID": "notes/w01/secant-method.html#the-secant-method-formula",
    "href": "notes/w01/secant-method.html#the-secant-method-formula",
    "title": "Secant Method",
    "section": "The Secant Method Formula",
    "text": "The Secant Method Formula\nGiven two initial approximations x_{k-1} and x_k, the next approximation x_{k+1} is computed using the secant line through these points. The formula is:\n\nx_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}\n\nThis equation is derived by approximating the derivative f'(x_k) using the difference quotient:\n\nf'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n\nThe main advantage of the Secant Method is that it avoids the need to compute the derivative f'(x), making it useful for functions where the derivative is difficult to compute or does not exist.\n\nAlgorithm\n\nInitial Guesses: Start with two initial approximations x_0 and x_1.\nIteration Formula: Compute successive approximations using the formula:\n\n\nx_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}\n\n\nRepeat: Continue iterating until the difference between successive approximations is less than a specified tolerance \\epsilon, or until |f(x_k)| &lt; \\epsilon.\n\n\n\nConvergence\nThe Secant Method typically converges faster than the Bisection Method but slower than Newton’s Method. It has a convergence rate of approximately 1.618, known as superlinear convergence. This is faster than the linear convergence of the Bisection Method, but slower than the quadratic convergence of Newton’s Method.\n\n\nExample\nLet’s solve the equation f(x) = x^2 - 4 = 0 using the Secant Method, which has roots at x = \\pm 2.\n\nInitial Guesses: Let x_0 = 3 and x_1 = 2.5.\nFirst Iteration:\n\nx_2 = x_1 - \\frac{f(x_1)(x_1 - x_0)}{f(x_1) - f(x_0)} = 2.5 - \\frac{(2.5^2 - 4)(2.5 - 3)}{(2.5^2 - 4) - (3^2 - 4)} = 2.05\n\nSecond Iteration:\n\nx_3 = x_2 - \\frac{f(x_2)(x_2 - x_1)}{f(x_2) - f(x_1)} = 2.05 - \\frac{(2.05^2 - 4)(2.05 - 2.5)}{(2.05^2 - 4) - (2.5^2 - 4)} \\approx 2.0006\n\nFurther Iterations: Continue until the difference between successive approximations is less than a specified tolerance (e.g., \\epsilon = 10^{-5}).\n\nIn this case, after just two iterations, we are already very close to the root x = 2.\n\n\nGeneral Properties of the Secant Method\n\nNo Derivatives Needed: Unlike Newton’s Method, the Secant Method does not require the computation of the derivative f'(x), making it useful for functions that are not differentiable or where computing the derivative is expensive.\nSuperlinear Convergence: The Secant Method converges faster than the Bisection Method but slower than Newton’s Method. Its convergence rate is superlinear with a rate of approximately 1.618.\nRequires Two Initial Guesses: The method requires two initial approximations, x_0 and x_1, unlike Newton’s Method, which only needs one initial guess.\n\n\n\nApplications of the Secant Method\n\nRoot Finding: The Secant Method is widely used to find roots of non-linear equations, especially in cases where the derivative is not available or is costly to compute.\nOptimization: It can be used in optimization problems where the objective is to minimize or maximize a function without requiring the calculation of the derivative.\n\n\n\nAdvantages of the Secant Method\n\nNo Derivatives: The method does not require the calculation of f'(x), making it easier to apply in situations where the derivative is not known.\nFaster than Bisection: The Secant Method generally converges more quickly than the Bisection Method, especially when the initial guesses are close to the root.\n\n\n\nLimitations of the Secant Method\n\nSlower than Newton’s Method: While it converges faster than the Bisection Method, the Secant Method typically converges more slowly than Newton’s Method, which has quadratic convergence.\nConvergence is Not Guaranteed: The Secant Method does not always converge, especially if the initial guesses are not close to the actual root. If f(x_k) = f(x_{k-1}), the method will fail due to division by zero.\nRequires Good Initial Guesses: Poor choices for the initial approximations x_0 and x_1 can result in slow convergence or failure to converge.\n\n\n\nConclusion\nThe Secant Method provides a good balance between speed and ease of use, especially when derivatives are difficult or costly to compute. It is faster than the Bisection Method but not as fast as Newton’s Method when derivatives are available. Careful selection of initial guesses is important for ensuring successful convergence."
  },
  {
    "objectID": "notes/w02/newtons-divided-differences.html",
    "href": "notes/w02/newtons-divided-differences.html",
    "title": "Newton’s Divided Differences",
    "section": "",
    "text": "Newton’s Divided Differences is an efficient method for computing an interpolating polynomial for a given set of data points. This method builds the polynomial iteratively and offers better efficiency for incremental data points compared to Lagrange interpolation."
  },
  {
    "objectID": "notes/w02/newtons-divided-differences.html#the-newton-divided-difference-formula",
    "href": "notes/w02/newtons-divided-differences.html#the-newton-divided-difference-formula",
    "title": "Newton’s Divided Differences",
    "section": "The Newton Divided Difference Formula",
    "text": "The Newton Divided Difference Formula\nGiven n data points (x_1, y_1), (x_2, y_2), ..., (x_n, y_n), the Newton divided difference interpolating polynomial P(x) can be expressed as:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + \\cdots + f[x_1, x_2, \\ldots, x_n](x - x_1)(x - x_2)\\cdots(x - x_{n-1})\n\nWhere f[x_i, x_j, ..., x_k] are the divided differences and are recursively defined as follows:\n\nf[x_i] = y_i\n\n\nf[x_i, x_j] = \\frac{f[x_j] - f[x_i]}{x_j - x_i}\n\n\nf[x_i, x_j, x_k] = \\frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}\n\nAnd so on for higher orders of divided differences.\n\nRecursive Formula for Divided Differences\nThe divided differences are computed recursively. For the first-order difference between two points, the formula is:\n\nf[x_i, x_{i+1}] = \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}\n\nFor the second-order difference between three points:\n\nf[x_i, x_{i+1}, x_{i+2}] = \\frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i}\n\nThis recursive approach continues for higher orders of differences.\n\n\nStep-by-Step Construction of the Newton Polynomial\n\nStart with the first point (x_1, y_1), where f[x_1] = y_1.\nFirst-order divided difference between (x_1, y_1) and (x_2, y_2) is:\n\n\nf[x_1, x_2] = \\frac{y_2 - y_1}{x_2 - x_1}\n\n\nSecond-order divided difference between (x_1, y_1), (x_2, y_2), and (x_3, y_3):\n\n\nf[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1}\n\n\nContinue this process for higher-order divided differences.\n\nHere’s the corrected example in the same format as your original:\n\n\nExample\nLet’s consider the data points (1, 1), (2, 4), (3, 9), and (4, 16). The goal is to find the interpolating polynomial using Newton’s divided differences.\n\nFirst point:\n\n\nf[1] = 1\n\n\nFirst-order divided differences:\n\n\nf[1, 2] = \\frac{4 - 1}{2 - 1} = 3\n\n\nf[2, 3] = \\frac{9 - 4}{3 - 2} = 5\n\n\nf[3, 4] = \\frac{16 - 9}{4 - 3} = 7\n\n\nSecond-order divided differences:\n\n\nf[1, 2, 3] = \\frac{f[2, 3] - f[1, 2]}{3 - 1} = \\frac{5 - 3}{2} = 1\n\n\nf[2, 3, 4] = \\frac{f[3, 4] - f[2, 3]}{4 - 2} = \\frac{7 - 5}{2} = 1\n\n\nThird-order divided difference:\n\n\nf[1, 2, 3, 4] = \\frac{f[2, 3, 4] - f[1, 2, 3]}{4 - 1} = \\frac{1 - 1}{3} = 0\n\nNow, the Newton polynomial can be written as:\n\nP(x) = 1 + 3(x - 1) + 1(x - 1)(x - 2) + 0(x - 1)(x - 2)(x - 3)\n\nSimplifying:\n\nP(x) = 1 + 3(x - 1) + (x - 1)(x - 2)\n\nExpanding the terms:\n\nP(x) = 1 + 3x - 3 + (x^2 - 3x + 2)\n\nSimplifying further:\n\nP(x) = x^2\n\n\n\nGeneral Properties of Newton’s Divided Differences\n\nEfficiency: Newton’s divided differences offer better computational efficiency when adding new points to the data set compared to Lagrange interpolation because earlier divided differences can be reused.\nUniqueness: The Newton polynomial is unique, meaning for a given set of n distinct points, there is exactly one polynomial of degree n-1 that interpolates the points.\nIterative Construction: The method allows for iterative construction, which is useful when dealing with real-time updates or adding new data points.\n\n\n\nApplications of Newton’s Divided Differences\n\nPolynomial Interpolation: Newton’s divided differences are commonly used to find an interpolating polynomial for a given set of data points.\nNumerical Differentiation: The method is used to approximate derivatives of functions when analytical differentiation is not feasible.\nCurve Fitting: It is used in applications requiring curve fitting, especially in scientific computing and data analysis.\n\n\n\nAdvantages of Newton’s Divided Differences\n\nEfficient for Incremental Data: If you need to add a new data point, you don’t have to recompute the entire polynomial. Only the new divided differences need to be computed.\nEasy to Implement: The recursive approach to finding divided differences makes this method easy to implement in code.\n\n\n\nLimitations of Newton’s Divided Differences\n\nNumerical Stability: Like other polynomial interpolation methods, Newton’s divided differences can suffer from numerical instability, especially with large datasets or unevenly spaced data.\nOscillations: High-degree interpolating polynomials may oscillate significantly between data points, especially if the data is not well-distributed (similar to Runge’s phenomenon).\n\n\n\nConclusion\nNewton’s Divided Differences is a powerful method for constructing interpolating polynomials, especially when efficiency and incremental data updates are needed. Its recursive nature allows for fast updates when new data points are added, making it useful in applications such as numerical analysis, interpolation, and curve fitting."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html",
    "href": "notes/w03/interpolation-error-formula.html",
    "title": "Interpolation Error Formula",
    "section": "",
    "text": "The interpolation error formula is an expression used to estimate the error between a true function f(x) and its interpolating polynomial P(x) at a specific point. This formula helps us understand how closely the interpolating polynomial approximates the function, especially based on the distribution of interpolation points and the smoothness of f(x).\nThe interpolation error formula is given by:\n\nf(x) - P(x) = \\frac{(x - x_1)(x - x_2) \\cdots (x - x_n)}{n!} f^{(n)}(c)\n\nwhere:\n\nn is the number of interpolated points.\nP(x) is the interpolating polynomial of degree n - 1 that fits the points (x_1, y_1), \\ldots, (x_n, y_n),\nf^{(n)}(c) is the n-th derivative of f(x) evaluated at some unknown point c in the interval [x_1, x_n]."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html#overview",
    "href": "notes/w03/interpolation-error-formula.html#overview",
    "title": "Interpolation Error Formula",
    "section": "",
    "text": "The interpolation error formula is an expression used to estimate the error between a true function f(x) and its interpolating polynomial P(x) at a specific point. This formula helps us understand how closely the interpolating polynomial approximates the function, especially based on the distribution of interpolation points and the smoothness of f(x).\nThe interpolation error formula is given by:\n\nf(x) - P(x) = \\frac{(x - x_1)(x - x_2) \\cdots (x - x_n)}{n!} f^{(n)}(c)\n\nwhere:\n\nn is the number of interpolated points.\nP(x) is the interpolating polynomial of degree n - 1 that fits the points (x_1, y_1), \\ldots, (x_n, y_n),\nf^{(n)}(c) is the n-th derivative of f(x) evaluated at some unknown point c in the interval [x_1, x_n]."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html#what-this-formula-shows",
    "href": "notes/w03/interpolation-error-formula.html#what-this-formula-shows",
    "title": "Interpolation Error Formula",
    "section": "What This Formula Shows",
    "text": "What This Formula Shows\nThis formula provides the error at a specific point x between the true function f(x) and the interpolating polynomial P(x). It does not directly give the maximum error across the entire interval, but rather the error at a particular x based on the distribution of interpolation points and the properties of f(x) at that point.\n\nKey Points\n\nPoint-Specific Error: This formula gives the interpolation error at a particular point x. It shows the difference f(x) - P(x) at that point rather than over the entire interval.\nDependence on Higher Derivatives: The error depends on the n-th derivative of f(x) evaluated at some unknown point c within [x_1, x_n]. This term reflects how “curved” f(x) is over the interval. A larger |f^{(n)}(c)| generally results in a larger error, as higher derivatives capture more variation in f(x).\nApproximate Error Size: Although c is unknown, we can approximate the error by assuming f^{(n)}(c) reaches its maximum absolute value over [x_1, x_n]. This allows us to estimate an upper bound on the error at any point in the interval, though it’s still approximate.\nEffect of Distance from Nodes: The term (x - x_1)(x - x_2) \\cdots (x - x_n) grows as x moves away from the interpolation nodes. This indicates that interpolation error typically increases the farther x is from the interpolation points, which is why interpolation tends to be most accurate near the nodes."
  },
  {
    "objectID": "notes/w03/interpolation-error-formula.html#finding-the-maximum-error-over-the-interval",
    "href": "notes/w03/interpolation-error-formula.html#finding-the-maximum-error-over-the-interval",
    "title": "Interpolation Error Formula",
    "section": "Finding the Maximum Error Over the Interval",
    "text": "Finding the Maximum Error Over the Interval\nTo find the maximum error over the entire interval [x_1, x_n], we can use the interpolation error formula to create an upper bound for the error on the interval:\n\nMaximize |f^{(n)}(x)| over the interval: Find the maximum of the n-th derivative of f(x), f^{(n)}(x), over the interval [x_1, x_n]. This value represents the largest possible influence of the function’s curvature on the error.\nMaximize |(x - x_1)(x - x_2) \\cdots (x - x_n)|: Determine the maximum value of the product |(x - x_1)(x - x_2) \\cdots (x - x_n)| over the interval [x_1, x_n]. This product is largest near the midpoint of the interval (between the nodes) and tends to be smaller near the endpoints.\nCombine the Results: Multiply these two maximum values and divide by n! to get an upper bound on the maximum error over the interval:\n\n\\max_{x \\in [x_1, x_n]} |f(x) - P(x)| \\approx \\frac{\\max_{x \\in [x_1, x_n]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|}{n!} \\cdot \\max_{x \\in [x_1, x_n]} |f^{(n)}(x)|\n\n\nThis approach provides an approximate maximum error over the interval. By using these maximum values, we can ensure that the error does not exceed this bound anywhere in [x_1, x_n].\nIn summary, the interpolation error formula gives insight into the local error at a specific point and can be used to estimate the maximum error on the interval by considering the maximum values of the components in the formula."
  },
  {
    "objectID": "notes/w06/spectral-radius.html",
    "href": "notes/w06/spectral-radius.html",
    "title": "Spectral Radius",
    "section": "",
    "text": "The spectral radius is a fundamental concept in linear algebra and matrix analysis, particularly in understanding the behavior of iterative methods like the Jacobi Method for solving linear systems."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#overview",
    "href": "notes/w06/spectral-radius.html#overview",
    "title": "Spectral Radius",
    "section": "",
    "text": "The spectral radius is a fundamental concept in linear algebra and matrix analysis, particularly in understanding the behavior of iterative methods like the Jacobi Method for solving linear systems."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#definition-of-spectral-radius",
    "href": "notes/w06/spectral-radius.html#definition-of-spectral-radius",
    "title": "Spectral Radius",
    "section": "Definition of Spectral Radius",
    "text": "Definition of Spectral Radius\nThe spectral radius of a square matrix A is defined as the largest absolute value of its eigenvalues:\n\n\\rho(A) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}\n\n\nInterpretation: It measures the “largest influence” of the eigenvalues of A, which governs the convergence properties of matrix-related iterative processes."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#properties-of-the-spectral-radius",
    "href": "notes/w06/spectral-radius.html#properties-of-the-spectral-radius",
    "title": "Spectral Radius",
    "section": "Properties of the Spectral Radius",
    "text": "Properties of the Spectral Radius\n\nNon-Negativity:\n\n\\rho(A) \\geq 0\n\nsince the spectral radius is the maximum of the absolute values of eigenvalues.\nBehavior Under Similarity Transformations: If B = P^{-1}AP, then:\n\n\\rho(B) = \\rho(A)\n\nbecause eigenvalues are invariant under similarity transformations.\nNorm Relationship: The spectral radius is related to matrix norms but generally satisfies: \n\\rho(A) \\leq \\|A\\|\n for certain matrix norms. Equality holds in some cases, such as the spectral norm for symmetric matrices."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#importance-in-iterative-methods",
    "href": "notes/w06/spectral-radius.html#importance-in-iterative-methods",
    "title": "Spectral Radius",
    "section": "Importance in Iterative Methods",
    "text": "Importance in Iterative Methods\nThe spectral radius plays a key role in determining the convergence of iterative methods for solving systems of linear equations.\n\nConvergence Criterion\nFor an iterative method defined by:\n\nx_{k+1} = Gx_k + c\n\nwhere G is the iteration matrix, the method converges if and only if:\n\n\\rho(G) &lt; 1\n\n\nExplanation: This ensures that successive iterations diminish in magnitude, eventually converging to the solution."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#example-computing-the-spectral-radius",
    "href": "notes/w06/spectral-radius.html#example-computing-the-spectral-radius",
    "title": "Spectral Radius",
    "section": "Example: Computing the Spectral Radius",
    "text": "Example: Computing the Spectral Radius\nConsider the matrix:\n\nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{bmatrix}\n\n\nStep 1: Find Eigenvalues\nSolve \\det(A - \\lambda I) = 0:\n\n\\det\\left(\\begin{bmatrix}\n2-\\lambda & 1 \\\\\n1 & 3-\\lambda\n\\end{bmatrix}\\right) = 0\n\nExpanding the determinant:\n\n(2-\\lambda)(3-\\lambda) - 1 = \\lambda^2 - 5\\lambda + 5 = 0\n\nThe eigenvalues are:\n\n\\lambda = \\frac{5 \\pm \\sqrt{5}}{2}\n\n\n\nStep 2: Compute Spectral Radius\nThe eigenvalues are approximately:\n\n\\lambda_1 \\approx 4.618, \\quad \\lambda_2 \\approx 0.382\n\nThus, the spectral radius is:\n\n\\rho(A) = \\max(|\\lambda_1|, |\\lambda_2|) = 4.618"
  },
  {
    "objectID": "notes/w06/spectral-radius.html#geometric-interpretation",
    "href": "notes/w06/spectral-radius.html#geometric-interpretation",
    "title": "Spectral Radius",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nThe spectral radius represents the largest “stretching factor” of a matrix when applied to a vector. For a square matrix A, eigenvectors corresponding to the eigenvalue with the largest magnitude indicate the direction in which A has the most influence."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#applications-of-spectral-radius",
    "href": "notes/w06/spectral-radius.html#applications-of-spectral-radius",
    "title": "Spectral Radius",
    "section": "Applications of Spectral Radius",
    "text": "Applications of Spectral Radius\n\nIterative Solvers:\n\nDetermines the convergence of methods like Jacobi and Gauss-Seidel.\n\nStability Analysis:\n\nUsed in analyzing the stability of dynamical systems where A represents a system’s state transition.\n\nNetwork Analysis:\n\nIn graph theory, the spectral radius of adjacency matrices provides insights into network properties."
  },
  {
    "objectID": "notes/w06/spectral-radius.html#conclusion",
    "href": "notes/w06/spectral-radius.html#conclusion",
    "title": "Spectral Radius",
    "section": "Conclusion",
    "text": "Conclusion\nThe spectral radius is a powerful tool for understanding the properties of matrices, particularly in the context of iterative methods and stability analysis. Mastering this concept is essential for applications ranging from solving linear systems to analyzing complex networks and dynamical systems."
  },
  {
    "objectID": "notes/w07/ax-b-iterative-methods/index.html",
    "href": "notes/w07/ax-b-iterative-methods/index.html",
    "title": "ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS",
    "section": "",
    "text": "Jacobi Method\nGauss-Seidel Method\nSuccessive Over-Relaxation (SOR)"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html",
    "href": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "",
    "text": "In numerical linear algebra, efficiently and accurately solving systems of equations is crucial. The Condition Number of a matrix is a fundamental concept that quantifies the sensitivity of the solution of a system of linear equations to errors in the input data. Understanding the condition number helps in assessing the reliability of numerical computations and in designing stable algorithms.\nThis note focuses on the Condition Number using the Infinity Norm, providing insights into its definition, interpretation, and practical computation."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#overview",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "",
    "text": "In numerical linear algebra, efficiently and accurately solving systems of equations is crucial. The Condition Number of a matrix is a fundamental concept that quantifies the sensitivity of the solution of a system of linear equations to errors in the input data. Understanding the condition number helps in assessing the reliability of numerical computations and in designing stable algorithms.\nThis note focuses on the Condition Number using the Infinity Norm, providing insights into its definition, interpretation, and practical computation."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#what-is-the-condition-number",
    "href": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#what-is-the-condition-number",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "What is the Condition Number?",
    "text": "What is the Condition Number?\nThe Condition Number of a matrix A, denoted as \\kappa_\\infty(A), measures how much the output value of a function can change for a small change in the input argument. In the context of linear systems, it indicates how sensitive the solution \\mathbf{x} of A\\mathbf{x} = \\mathbf{b} is to changes or errors in A or \\mathbf{b}.\nMathematically, the condition number using the Infinity Norm is defined as:\n\n\\kappa_\\infty(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty\n\nWhere:\n\n\\|A\\|_\\infty is the Infinity Norm of matrix A, defined as the maximum absolute row sum.\n\\|A^{-1}\\|_\\infty is the Infinity Norm of the inverse of matrix A.\n\n\nInfinity Norm (\\|\\cdot\\|_\\infty)\nThe Infinity Norm of a matrix A is calculated as:\n\n\\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{n} |a_{ij}|\n\nWhere a_{ij} are the elements of matrix A, and m and n are the number of rows and columns, respectively."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#interpreting-the-condition-number",
    "href": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#interpreting-the-condition-number",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "Interpreting the Condition Number",
    "text": "Interpreting the Condition Number\nThe condition number provides insight into the numerical stability of solving linear systems:\n\nWell-Conditioned Matrix: \\kappa_\\infty(A) is close to 1.\n\nSmall changes in A or \\mathbf{b} lead to small changes in \\mathbf{x}.\nSolutions are reliable and stable.\n\nIll-Conditioned Matrix: \\kappa_\\infty(A) is large (significantly greater than 1).\n\nSmall changes in A or \\mathbf{b} can cause large changes in \\mathbf{x}.\nSolutions are unreliable and sensitive to errors."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#why-the-condition-number-matters",
    "href": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#why-the-condition-number-matters",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "Why the Condition Number Matters",
    "text": "Why the Condition Number Matters\nUnderstanding the condition number is essential for several reasons:\n\nError Analysis: It helps predict how errors in data propagate to the solution.\nAlgorithm Selection: Guides the choice of numerical methods that are more stable for certain condition numbers.\nMatrix Inversion: Indicates the feasibility and accuracy of computing the inverse of a matrix.\nOptimization: Plays a role in optimization algorithms where matrix conditioning affects convergence rates."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#a-practical-example",
    "href": "notes/w07/errors-analysis-linear-systems/condition-number-matrix.html#a-practical-example",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "A Practical Example",
    "text": "A Practical Example\nLet’s explore a concrete example to illustrate the concept of the condition number using the Infinity Norm and its implications.\n\nThe Problem Setup\nConsider the system of equations:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} is the true solution.\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} is the approximate (computed) solution.\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} is the input data.\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} is the coefficient matrix.\n\n\n\nStep 1: Compute the Infinity Norms\nWe will compute the condition number using the Infinity Norm.\n\nCompute \\|A\\|_\\infty\n\n\\|A\\|_\\infty = \\max \\left\\{ |1| + |1|, \\ |3| + |-4| \\right\\} = \\max \\{ 2, 7 \\} = 7\n\n\n\nCompute \\|A^{-1}\\|_\\infty\nFirst, find the inverse of A:\n\nA^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} -4 & -1 \\\\ -3 & 1 \\end{bmatrix}\n\nCompute the determinant:\n\n\\det(A) = (1)(-4) - (1)(3) = -4 - 3 = -7\n\nThus,\n\nA^{-1} = \\frac{1}{-7} \\begin{bmatrix} -4 & -1 \\\\ -3 & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{7} & \\frac{1}{7} \\\\ \\frac{3}{7} & -\\frac{1}{7} \\end{bmatrix}\n\nNow, compute \\|A^{-1}\\|_\\infty:\n\n\\|A^{-1}\\|_\\infty = \\max \\left\\{ \\left| \\frac{4}{7} \\right| + \\left| \\frac{1}{7} \\right|, \\ \\left| \\frac{3}{7} \\right| + \\left| -\\frac{1}{7} \\right| \\right\\} = \\max \\left\\{ \\frac{5}{7}, \\frac{4}{7} \\right\\} = \\frac{5}{7} \\approx 0.7143\n\n\n\n\nStep 2: Compute the Condition Number \\kappa_\\infty(A)\n\n\\kappa_\\infty(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 7 \\cdot \\frac{5}{7} = 5\n\n\n\nStep 3: Interpretation\nA condition number \\kappa_\\infty(A) = 5 indicates that the matrix A is moderately well-conditioned. This means that the solution \\mathbf{x} is relatively stable with respect to small perturbations in A or \\mathbf{b}. While some error amplification is possible, it is not excessively large, and the solution can be considered reliable for practical purposes."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html",
    "title": "Relative Backward Error in Linear Systems",
    "section": "",
    "text": "The relative backward error is a normalized version of the backward error. It measures how large the residual vector \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is relative to the size of the right-hand side vector \\mathbf{b}. This normalization ensures the backward error is interpreted in the context of the magnitude of the original problem.\nThe relative backward error is defined as:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nwhere:\n\n\\|\\mathbf{r}\\|_\\infty: The infinity norm of the residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\mathbf{b}\\|_\\infty: The infinity norm of the right-hand side vector \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#overview",
    "title": "Relative Backward Error in Linear Systems",
    "section": "",
    "text": "The relative backward error is a normalized version of the backward error. It measures how large the residual vector \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is relative to the size of the right-hand side vector \\mathbf{b}. This normalization ensures the backward error is interpreted in the context of the magnitude of the original problem.\nThe relative backward error is defined as:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nwhere:\n\n\\|\\mathbf{r}\\|_\\infty: The infinity norm of the residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\mathbf{b}\\|_\\infty: The infinity norm of the right-hand side vector \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#what-relative-backward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#what-relative-backward-error-represents",
    "title": "Relative Backward Error in Linear Systems",
    "section": "What Relative Backward Error Represents",
    "text": "What Relative Backward Error Represents\n\nScale-Invariant Error:\n\nBy dividing the backward error by \\|\\mathbf{b}\\|_\\infty, the relative backward error accounts for the scale of \\mathbf{b}. This is useful when comparing systems with different magnitudes of \\mathbf{b}.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then the residual \\mathbf{r} = \\mathbf{0}, and:\n\n\\text{RBE} = 0\n\n\nError Normalization:\n\nA small relative backward error indicates that the residual is negligible compared to the size of \\mathbf{b}, suggesting a high-quality solution."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#why-relative-backward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#why-relative-backward-error-matters",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Why Relative Backward Error Matters",
    "text": "Why Relative Backward Error Matters\n\nAssessing Solution Quality:\n\nThe relative backward error is a scale-invariant metric, making it easier to compare errors across systems of different sizes.\n\nNumerical Stability:\n\nA small relative backward error ensures that the approximate solution \\mathbf{x_a} satisfies a nearby system with respect to the scale of \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#example",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\times 1 + 1 \\times 1 \\\\ 3 \\times 1 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 - 2 \\\\ 2 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Relative Backward Error\n\nCompute the infinity norms:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3,\n\\|\\mathbf{b}\\|_\\infty = \\max(|3|, |2|) = 3.\n\nCalculate the relative backward error:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1\n\n\n\n\nStep 4: Interpretation\n\nA relative backward error of 1 indicates that the residual \\mathbf{r} is as large as the largest component of \\mathbf{b}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/relative-backward-error.html#conclusion",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe relative backward error provides a normalized measure of how much the right-hand side \\mathbf{b} must be perturbed for \\mathbf{x_a} to satisfy the system exactly.\nBy comparing the size of \\mathbf{r} to \\mathbf{b}, the relative backward error allows for consistent error analysis across problems of different scales."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html",
    "title": "Residual in Linear Systems",
    "section": "",
    "text": "The residual is a fundamental concept in numerical linear algebra, used to quantify how far an approximate solution \\mathbf{x_a} to a linear system A\\mathbf{x} = \\mathbf{b} is from satisfying the system. It provides a direct measure of the “error” in the system when the computed solution \\mathbf{x_a} is substituted back into the equation.\nThe residual is defined as:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nwhere:\n\n\\mathbf{b}: The right-hand side vector of the system.\nA\\mathbf{x_a}: The result of substituting the approximate solution \\mathbf{x_a} into the system.\n\nIf \\mathbf{r} = \\mathbf{0}, the approximate solution is exact; otherwise, \\mathbf{r} quantifies the degree of error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#overview",
    "title": "Residual in Linear Systems",
    "section": "",
    "text": "The residual is a fundamental concept in numerical linear algebra, used to quantify how far an approximate solution \\mathbf{x_a} to a linear system A\\mathbf{x} = \\mathbf{b} is from satisfying the system. It provides a direct measure of the “error” in the system when the computed solution \\mathbf{x_a} is substituted back into the equation.\nThe residual is defined as:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nwhere:\n\n\\mathbf{b}: The right-hand side vector of the system.\nA\\mathbf{x_a}: The result of substituting the approximate solution \\mathbf{x_a} into the system.\n\nIf \\mathbf{r} = \\mathbf{0}, the approximate solution is exact; otherwise, \\mathbf{r} quantifies the degree of error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#what-the-residual-represents",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#what-the-residual-represents",
    "title": "Residual in Linear Systems",
    "section": "What the Residual Represents",
    "text": "What the Residual Represents\nThe residual measures how far \\mathbf{x_a} is from satisfying the system A\\mathbf{x} = \\mathbf{b}. Each component of \\mathbf{r} indicates the mismatch for the corresponding equation in the system.\n\nKey Points\n\nResidual as a Vector:\n\nThe residual \\mathbf{r} is a vector with the same dimensions as \\mathbf{b}.\nEach entry r_i measures the difference between b_i and the corresponding value of (A\\mathbf{x_a})_i.\n\nExact Solution:\n\nIf \\mathbf{x_a} = \\mathbf{x} (the exact solution), then: \n\\mathbf{r} = \\mathbf{0}\n\n\nApproximate Solution:\n\nFor an approximate solution \\mathbf{x_a}, the residual \\mathbf{r} \\neq \\mathbf{0}."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#norm-of-the-residual",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#norm-of-the-residual",
    "title": "Residual in Linear Systems",
    "section": "Norm of the Residual",
    "text": "Norm of the Residual\nThe size of the residual can be measured using norms, such as the infinity norm:\n\n\\|\\mathbf{r}\\|_\\infty = \\max_{i} |r_i|\n\nThis provides a scalar measure of the largest discrepancy in the system. A smaller residual norm indicates that \\mathbf{x_a} is closer to satisfying the system."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#example",
    "title": "Residual in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nThe residual is:\n\n\\mathbf{r} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Residual Norm\nThe infinity norm of \\mathbf{r} is:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3\n\nThis indicates that the largest mismatch in the system is 3."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#applications-of-the-residual",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#applications-of-the-residual",
    "title": "Residual in Linear Systems",
    "section": "Applications of the Residual",
    "text": "Applications of the Residual\n\nError Analysis:\n\nThe residual is used to assess the accuracy of an approximate solution.\n\nIterative Methods:\n\nResiduals are central to iterative solvers, such as the Jacobi and Gauss-Seidel methods, to track convergence.\n\nNumerical Stability:\n\nA large residual often indicates instability or poor conditioning in the matrix A.\n\nRefining Solutions:\n\nResidual-based refinement techniques iteratively adjust \\mathbf{x_a} to minimize \\|\\mathbf{r}\\|."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/residual-linear-systems.html#conclusion",
    "title": "Residual in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThe residual \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is a key tool in numerical linear algebra for evaluating the accuracy of an approximate solution. By analyzing the residual and its norm, we can diagnose errors, refine solutions, and ensure stability in solving linear systems."
  },
  {
    "objectID": "notes/w07/lu-factorization.html",
    "href": "notes/w07/lu-factorization.html",
    "title": "LU Factorization",
    "section": "",
    "text": "LU Factorization (or LU Decomposition) is a powerful technique in linear algebra for breaking down a matrix A into the product of a lower triangular matrix L and an upper triangular matrix U. This factorization is commonly used for solving linear systems, computing determinants, and inverting matrices. This note explores LU Factorization’s definition, properties, computation, and applications."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#overview",
    "href": "notes/w07/lu-factorization.html#overview",
    "title": "LU Factorization",
    "section": "",
    "text": "LU Factorization (or LU Decomposition) is a powerful technique in linear algebra for breaking down a matrix A into the product of a lower triangular matrix L and an upper triangular matrix U. This factorization is commonly used for solving linear systems, computing determinants, and inverting matrices. This note explores LU Factorization’s definition, properties, computation, and applications."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#definition-and-decomposition",
    "href": "notes/w07/lu-factorization.html#definition-and-decomposition",
    "title": "LU Factorization",
    "section": "Definition and Decomposition",
    "text": "Definition and Decomposition\nFor a square matrix A, LU Factorization is given by:\n\nA = LU\n\nwhere:\n\nL is a lower triangular matrix with ones on the diagonal.\nU is an upper triangular matrix.\n\nIf A cannot be decomposed directly, partial pivoting may be applied, resulting in:\n\nPA = LU\n\nwhere P is a permutation matrix that records row exchanges."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#conditions-for-lu-factorization",
    "href": "notes/w07/lu-factorization.html#conditions-for-lu-factorization",
    "title": "LU Factorization",
    "section": "Conditions for LU Factorization",
    "text": "Conditions for LU Factorization\nLU Factorization is valid when:\n\nMatrix is Square: A must be a square matrix.\nNon-Singular Leading Submatrices: Each leading principal submatrix (upper-left submatrix) of A must be non-singular.\n\nWhen these conditions are not met, row pivoting enables decomposition."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#factorization-process",
    "href": "notes/w07/lu-factorization.html#factorization-process",
    "title": "LU Factorization",
    "section": "Factorization Process",
    "text": "Factorization Process\nTo factorize A into L and U:\n\nEliminate Elements: Perform row operations to create zeros below the main diagonal of U.\nStore Multipliers: Record the multipliers in L.\n\n\nExample\nGiven a 3 \\times 3 matrix:\n\nA = \\begin{pmatrix} 2 & 3 & 1 \\\\ 4 & 7 & -1 \\\\ -2 & 3 & 5 \\end{pmatrix}\n\n\nTransform A into U using row operations.\nRecord elimination factors in L.\nThe result satisfies A = LU."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#solving-systems-with-lu-factorization",
    "href": "notes/w07/lu-factorization.html#solving-systems-with-lu-factorization",
    "title": "LU Factorization",
    "section": "Solving Systems with LU Factorization",
    "text": "Solving Systems with LU Factorization\nLU Factorization allows us to solve Ax = b by breaking it down into two simpler systems:\n\nSolve Ly = b: Use forward substitution, as L is lower triangular.\nSolve Ux = y: Use back substitution with U as an upper triangular matrix.\n\n\nEfficiency\nThis approach reduces computation time, especially when solving multiple systems with the same A but different b vectors, as the factorization needs to be computed only once."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#pivoting-and-permutation",
    "href": "notes/w07/lu-factorization.html#pivoting-and-permutation",
    "title": "LU Factorization",
    "section": "Pivoting and Permutation",
    "text": "Pivoting and Permutation\nIn cases where A has zeros or small values on the diagonal, partial pivoting improves numerical stability by reordering rows to place a larger element on the diagonal:\n\nPA = LU\n\nwhere P is a permutation matrix that tracks row exchanges."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#advantages-of-lu-factorization",
    "href": "notes/w07/lu-factorization.html#advantages-of-lu-factorization",
    "title": "LU Factorization",
    "section": "Advantages of LU Factorization",
    "text": "Advantages of LU Factorization\n\nEfficient Linear System Solving: Faster than Gaussian elimination for repeated systems.\nDeterminant Computation: The determinant of A is the product of the diagonal elements of U.\nMatrix Inversion: LU Factorization simplifies inversion by inverting L and U separately."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#applications-of-lu-factorization",
    "href": "notes/w07/lu-factorization.html#applications-of-lu-factorization",
    "title": "LU Factorization",
    "section": "Applications of LU Factorization",
    "text": "Applications of LU Factorization\nLU Factorization is widely used across various fields due to its computational efficiency:\n\nNumerical Linear Algebra: Fundamental for solving linear systems.\nOptimization: Integral in algorithms that rely on matrix decompositions.\nComputer Graphics: Enables transformations and projections.\nScientific Computing: Common in simulations and solving differential equations."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#example-problem",
    "href": "notes/w07/lu-factorization.html#example-problem",
    "title": "LU Factorization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: For the matrix\n\nA = \\begin{pmatrix} 3 & -7 & -2 \\\\ -3 & 5 & 1 \\\\ 6 & -4 & 0 \\end{pmatrix}\n\n\nFactorize A into L and U.\nSolve Ax = b for b = \\begin{pmatrix} 5 \\\\ -1 \\\\ 3 \\end{pmatrix}.\n\n\nSolution Steps\n\nPerform row operations to decompose A into L and U.\nSolve Ly = b via forward substitution.\nSolve Ux = y via back substitution."
  },
  {
    "objectID": "notes/w07/lu-factorization.html#conclusion",
    "href": "notes/w07/lu-factorization.html#conclusion",
    "title": "LU Factorization",
    "section": "Conclusion",
    "text": "Conclusion\nLU Factorization is an essential tool in linear algebra, providing a simplified method to solve linear systems efficiently. By breaking matrices into triangular forms, it reduces computational complexity and lays the groundwork for more advanced numerical techniques."
  },
  {
    "objectID": "notes/w07/norms/index.html",
    "href": "notes/w07/norms/index.html",
    "title": "NORMS",
    "section": "",
    "text": "Euclidean Vector Norm\nTaxicab Vector Norm\nInfinity Vector Norm\nInfinity Norm for Matrices"
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html",
    "href": "notes/w07/norms/infinity-vector-norm.html",
    "title": "Infinity Vector Norm",
    "section": "",
    "text": "The infinity norm (also called the maximum norm or \\ell_\\infty-norm) measures the size of a vector by taking the maximum absolute value of its components. Unlike the Euclidean or Taxicab norms, which involve summing components, the infinity norm focuses on the “largest step” in any single direction."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#overview",
    "href": "notes/w07/norms/infinity-vector-norm.html#overview",
    "title": "Infinity Vector Norm",
    "section": "",
    "text": "The infinity norm (also called the maximum norm or \\ell_\\infty-norm) measures the size of a vector by taking the maximum absolute value of its components. Unlike the Euclidean or Taxicab norms, which involve summing components, the infinity norm focuses on the “largest step” in any single direction."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#definition",
    "href": "notes/w07/norms/infinity-vector-norm.html#definition",
    "title": "Infinity Vector Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the infinity norm is defined as:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|\n\nThis norm is often used in settings where the largest component dominates or in grid-based systems where movement is limited by a single axis."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#properties",
    "href": "notes/w07/norms/infinity-vector-norm.html#properties",
    "title": "Infinity Vector Norm",
    "section": "Properties",
    "text": "Properties\nThe infinity norm satisfies the following properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_\\infty \\geq 0, and \\|\\mathbf{v}\\|_\\infty = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_\\infty = |c| \\|\\mathbf{v}\\|_\\infty.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_\\infty \\leq \\|\\mathbf{u}\\|_\\infty + \\|\\mathbf{v}\\|_\\infty."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#examples",
    "href": "notes/w07/norms/infinity-vector-norm.html#examples",
    "title": "Infinity Vector Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Example\nFor \\mathbf{v} = [3, -4]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|3|, |-4|) = \\max(3, 4) = 4\n\n\n\n2. 3D Example\nFor \\mathbf{v} = [1, -2, 3]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|1|, |-2|, |3|) = \\max(1, 2, 3) = 3\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\dots, |v_n|)"
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#applications",
    "href": "notes/w07/norms/infinity-vector-norm.html#applications",
    "title": "Infinity Vector Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Error Analysis\nThe infinity norm is used to measure the largest error in numerical solutions, ensuring that no individual error component dominates the result.\n\n\n2. Optimization\nIn optimization problems, the infinity norm simplifies constraints by focusing on the largest deviation in variables.\n\n\n3. Machine Learning\nThe infinity norm is used in regularization techniques and as a metric in certain classification problems.\n\n\n4. Computational Efficiency\nSince the infinity norm involves only a maximum operation, it is computationally inexpensive compared to other norms."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#visualization",
    "href": "notes/w07/norms/infinity-vector-norm.html#visualization",
    "title": "Infinity Vector Norm",
    "section": "Visualization",
    "text": "Visualization\nIn 2D, the set of points at a fixed infinity norm distance from the origin forms a square aligned with the coordinate axes. For example, all points satisfying \\|\\mathbf{v}\\|_\\infty = 3 in \\mathbb{R}^2 would form the square:\n\n\\max(|x|, |y|) = 3\n\nor equivalently:\n\n-3 \\leq x \\leq 3, \\quad -3 \\leq y \\leq 3"
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#example-problem",
    "href": "notes/w07/norms/infinity-vector-norm.html#example-problem",
    "title": "Infinity Vector Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the infinity norm of \\mathbf{v} = [-3, 4, -5].\n\nSolution:\n\nTake the absolute values of the components: |-3| = 3, |4| = 4, |-5| = 5.\nFind the maximum: \\|\\mathbf{v}\\|_\\infty = \\max(3, 4, 5) = 5."
  },
  {
    "objectID": "notes/w07/norms/infinity-vector-norm.html#conclusion",
    "href": "notes/w07/norms/infinity-vector-norm.html#conclusion",
    "title": "Infinity Vector Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe infinity norm provides a simple and efficient way to measure vector size by focusing on the largest component. It is particularly useful in applications like error analysis, optimization, and machine learning where the largest deviation or influence is of primary interest."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html",
    "href": "notes/w08/gram-schmidt-orthogonalization.html",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Gram-Schmidt Orthogonalization process is a fundamental technique in linear algebra for transforming a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This method is widely used in applications such as QR factorization, solving least squares problems, and numerical linear algebra. This note explores the definition, properties, computation, and applications of the Gram-Schmidt process."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#overview",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#overview",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Gram-Schmidt Orthogonalization process is a fundamental technique in linear algebra for transforming a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This method is widely used in applications such as QR factorization, solving least squares problems, and numerical linear algebra. This note explores the definition, properties, computation, and applications of the Gram-Schmidt process."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#definition-and-process",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#definition-and-process",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Definition and Process",
    "text": "Definition and Process\nGiven a set of linearly independent vectors A_1, A_2, \\dots, A_n, the Gram-Schmidt process produces an orthogonal (or orthonormal) set of vectors q_1, q_2, \\dots, q_n such that:\n\nEach vector q_i is orthogonal to the previous vectors q_1, q_2, \\dots, q_{i-1}.\nThe span of q_1, q_2, \\dots, q_n is the same as the span of A_1, A_2, \\dots, A_n.\n\nThis orthogonal set can also be normalized to create an orthonormal basis."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#steps-of-the-gram-schmidt-process",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#steps-of-the-gram-schmidt-process",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Steps of the Gram-Schmidt Process",
    "text": "Steps of the Gram-Schmidt Process\nThe Gram-Schmidt process involves the following steps:\n\nInitialize with the First Vector: Set q_1 as the normalized version of A_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\nCompute Subsequent Vectors: For each vector A_i, construct a new vector u_i by subtracting components that align with previously computed q-vectors. Normalize u_i to obtain q_i:\n\nDefine the non-normalized vector u_i: \nu_i = A_i - \\sum_{j=1}^{i-1} (q_j \\cdot A_i) \\, q_j\n\nNormalize u_i to obtain q_i: \nq_i = \\frac{u_i}{\\|u_i\\|}\n\n\n\nEach vector q_i is thus orthogonal to the preceding q-vectors and has unit length if normalized."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#example",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#example",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Example",
    "text": "Example\nGiven vectors A_1 and A_2, the Gram-Schmidt process works as follows:\n\nCalculate q_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\nCalculate q_2:\n\nFirst, remove the component of A_2 in the direction of q_1 to obtain u_2: \nu_2 = A_2 - (q_1 \\cdot A_2) q_1\n\nThen, normalize u_2 to get q_2: \nq_2 = \\frac{u_2}{\\|u_2\\|}"
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#properties-of-gram-schmidt-orthogonalization",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#properties-of-gram-schmidt-orthogonalization",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Properties of Gram-Schmidt Orthogonalization",
    "text": "Properties of Gram-Schmidt Orthogonalization\n\nOrthogonality: Each vector q_i is orthogonal to all previously generated vectors q_1, \\dots, q_{i-1}.\nSpan Preservation: The set \\{q_1, q_2, \\dots, q_n\\} spans the same subspace as the original set \\{A_1, A_2, \\dots, A_n\\}.\nOrthonormal Basis: By normalizing each u_i to get q_i, the process yields an orthonormal basis for the subspace."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#applications-of-gram-schmidt-orthogonalization",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#applications-of-gram-schmidt-orthogonalization",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Applications of Gram-Schmidt Orthogonalization",
    "text": "Applications of Gram-Schmidt Orthogonalization\nGram-Schmidt orthogonalization is widely applied in various fields due to its utility in creating orthogonal bases:\n\nQR Factorization: Used to decompose a matrix into an orthogonal matrix Q and an upper triangular matrix R.\nLeast Squares Problems: Assists in minimizing the error in fitting data to a model by creating orthogonal projections.\nSignal Processing and Data Compression: Forms the foundation for methods that reduce redundancy by representing data in orthogonal bases.\nMachine Learning and Statistics: Simplifies computations by projecting data onto orthogonal components."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#example-problem",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#example-problem",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Given the vectors\n\nA_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}\n\n\nUse Gram-Schmidt to find orthogonal vectors q_1 and q_2.\nNormalize q_1 and q_2 to form an orthonormal basis.\n\n\nSolution Steps\n\nCompute q_1 by normalizing A_1.\nCalculate u_2 by removing the component of A_2 in the direction of q_1.\nNormalize u_2 to obtain q_2."
  },
  {
    "objectID": "notes/w08/gram-schmidt-orthogonalization.html#conclusion",
    "href": "notes/w08/gram-schmidt-orthogonalization.html#conclusion",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Conclusion",
    "text": "Conclusion\nThe Gram-Schmidt process is a valuable tool in linear algebra for constructing orthogonal (or orthonormal) bases. By transforming a set of linearly independent vectors, it simplifies many matrix operations and lays the groundwork for QR factorization, data projections, and error minimization in least squares problems."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Modified Gram-Schmidt Orthogonalization (MGS) is a variation of the classical Gram-Schmidt process that improves numerical stability during computations. While it produces the same mathematical results as the classical process, MGS incrementally updates the working vector, avoiding the accumulation of rounding errors. This is especially beneficial when working with floating-point arithmetic in machine computations."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#overview",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#overview",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Modified Gram-Schmidt Orthogonalization (MGS) is a variation of the classical Gram-Schmidt process that improves numerical stability during computations. While it produces the same mathematical results as the classical process, MGS incrementally updates the working vector, avoiding the accumulation of rounding errors. This is especially beneficial when working with floating-point arithmetic in machine computations."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#definition-and-process",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#definition-and-process",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Definition and Process",
    "text": "Definition and Process\nGiven a set of linearly independent vectors A_1, A_2, \\dots, A_n, the Modified Gram-Schmidt process produces an orthogonal (or orthonormal) set of vectors q_1, q_2, \\dots, q_n such that:\n\nEach vector q_i is orthogonal to the previous vectors q_1, q_2, \\dots, q_{i-1}.\nThe span of q_1, q_2, \\dots, q_n is the same as the span of A_1, A_2, \\dots, A_n.\n\nThe primary distinction of MGS is its incremental update of the working vector y, ensuring orthogonality is preserved at every step."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#steps-of-the-modified-gram-schmidt-process",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#steps-of-the-modified-gram-schmidt-process",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Steps of the Modified Gram-Schmidt Process",
    "text": "Steps of the Modified Gram-Schmidt Process\n\nInitialize with the First Vector: Normalize A_1 to obtain q_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\nCompute Subsequent Vectors Incrementally:\n\nStart with y = A_i (the current vector being processed).\nSubtract the projections of y onto all previously computed q_j vectors, updating y step-by-step: \ny \\gets y - (q_j^\\top y) q_j \\quad \\text{for } j = 1, 2, \\dots, i-1\n\nNormalize the updated y to obtain q_i: \nq_i = \\frac{y}{\\|y\\|}\n\n\nRepeat this process for all vectors A_i."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#differences-between-classical-and-modified-gram-schmidt",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#differences-between-classical-and-modified-gram-schmidt",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Differences Between Classical and Modified Gram-Schmidt",
    "text": "Differences Between Classical and Modified Gram-Schmidt\n\n\n\n\nAspect\n\n\nClassical Gram-Schmidt\n\n\nModified Gram-Schmidt\n\n\n\n\nProjection Calculation\n\n\nSubtracts all projections at once.\n\n\nSubtracts projections incrementally, one at a time.\n\n\n\n\nNumerical Stability\n\n\nProne to rounding errors, especially for small or nearly parallel vectors.\n\n\nLess prone to rounding errors due to incremental updates.\n\n\n\n\nIntermediate Vector Updates\n\n\nProjections are computed using the original vector, so errors accumulate.\n\n\nProjections are computed step-by-step using updated vectors, reducing error propagation.\n\n\n\n\nOrthogonality of Output\n\n\nOrthogonality may degrade due to numerical issues.\n\n\nBetter orthogonality preservation in finite-precision arithmetic.\n\n\n\n\nUse Case\n\n\nUseful for theoretical computations and small-scale problems.\n\n\nPreferred for numerical applications and large-scale computations."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#properties-of-modified-gram-schmidt-orthogonalization",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#properties-of-modified-gram-schmidt-orthogonalization",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Properties of Modified Gram-Schmidt Orthogonalization",
    "text": "Properties of Modified Gram-Schmidt Orthogonalization\n\nOrthogonality: Each vector q_i is orthogonal to all previously generated vectors q_1, \\dots, q_{i-1}.\nSpan Preservation: The set \\{q_1, q_2, \\dots, q_n\\} spans the same subspace as \\{A_1, A_2, \\dots, A_n\\}.\nNumerical Stability: Incremental updates reduce the impact of rounding errors, making it more robust in practice."
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#example-problem",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#example-problem",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Given the matrix\n\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n\\delta & 0 & 0 \\\\\n0 & \\delta & 0 \\\\\n0 & 0 & \\delta\n\\end{bmatrix}, \\quad \\delta = 10^{-10}\n\nfind the orthonormal basis Q = [q_1, q_2, q_3] using the Modified Gram-Schmidt process.\n\nSolution Steps\n\nCompute q_1: Normalize A_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|} =\n\\begin{bmatrix}\n1 \\\\ \\delta \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\nCompute q_2:\n\nStart with y = A_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\delta \\\\ 0 \\end{bmatrix}.\nSubtract the projection onto q_1:\n\n\\text{proj}_{q_1}(y) = (q_1^\\top y) q_1, \\quad q_1^\\top y = 1\n\n\ny \\gets y - \\text{proj}_{q_1}(y) =\n\\begin{bmatrix}\n1 \\\\ 0 \\\\ \\delta \\\\ 0\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n1 \\\\ \\delta \\\\ 0 \\\\ 0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ -\\delta \\\\ \\delta \\\\ 0\n\\end{bmatrix}\n\nNormalize y: \nq_2 = \\frac{y}{\\|y\\|} = \\frac{1}{\\delta \\sqrt{2}}\n\\begin{bmatrix}\n0 \\\\ -\\delta \\\\ \\delta \\\\ 0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0\n\\end{bmatrix}\n\n\nCompute q_3:\n\nStart with y = A_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\delta \\end{bmatrix}.\nSubtract the projection onto q_1: \ny \\gets y - \\text{proj}_{q_1}(y), \\quad \\text{proj}_{q_1}(y) =\n\\begin{bmatrix}\n1 \\\\ \\delta \\\\ 0 \\\\ 0\n\\end{bmatrix}\n \ny = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\delta \\end{bmatrix} -\n\\begin{bmatrix} 1 \\\\ \\delta \\\\ 0 \\\\ 0 \\end{bmatrix} =\n\\begin{bmatrix} 0 \\\\ -\\delta \\\\ 0 \\\\ \\delta \\end{bmatrix}\n\nSubtract the projection onto q_2: \ny \\gets y - \\text{proj}_{q_2}(y), \\quad q_2^\\top y = \\frac{\\delta}{\\sqrt{2}}\n \ny = \\begin{bmatrix} 0 \\\\ -\\delta \\\\ 0 \\\\ \\delta \\end{bmatrix} -\n\\begin{bmatrix} 0 \\\\ -\\frac{\\delta}{2} \\\\ \\frac{\\delta}{2} \\\\ 0 \\end{bmatrix} =\n\\begin{bmatrix} 0 \\\\ -\\frac{\\delta}{2} \\\\ -\\frac{\\delta}{2} \\\\ \\delta \\end{bmatrix}\n\nNormalize y: \nq_3 = \\frac{1}{\\|y\\|} \\begin{bmatrix} 0 \\\\ -\\frac{\\delta}{2} \\\\ -\\frac{\\delta}{2} \\\\ \\delta \\end{bmatrix} =\n\\begin{bmatrix}\n0 \\\\ -\\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}}\n\\end{bmatrix}"
  },
  {
    "objectID": "notes/w08/modified-gram-schmidt-orthogonalization.html#conclusion",
    "href": "notes/w08/modified-gram-schmidt-orthogonalization.html#conclusion",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Conclusion",
    "text": "Conclusion\nModified Gram-Schmidt is a more stable alternative to the classical process, particularly when dealing with small or near-parallel vectors. By incrementally updating the working vector y, MGS ensures numerical stability and maintains orthogonality even in the presence of rounding errors. It is especially useful in applications like QR factorization and solving least squares problems in computational settings."
  },
  {
    "objectID": "worksheets/index.html",
    "href": "worksheets/index.html",
    "title": "WORKSHEETS",
    "section": "",
    "text": "Jacobi Method - Convergence Proof"
  },
  {
    "objectID": "notes/w03/runge-phenomenon.html",
    "href": "notes/w03/runge-phenomenon.html",
    "title": "Runge Phenomenon",
    "section": "",
    "text": "The Runge Phenomenon refers to the oscillatory behavior that occurs when using high-degree polynomial interpolation on evenly spaced data points, particularly near the endpoints of an interval. The phenomenon is named after Carl Runge, who discovered this issue while studying interpolation of functions with large oscillations near the boundaries of an interval.\n\n\nWhen interpolating a smooth function using polynomials of high degree, the interpolation can become highly oscillatory near the edges of the interval, even if the function being interpolated is smooth and well-behaved. This is particularly problematic with equally spaced points, as the polynomial tries to fit too closely to the data points near the boundaries, leading to large errors.\n\n\n\nThe most famous example illustrating the Runge phenomenon is based on the Runge function:\n\nf(x) = \\frac{1}{1 + 25x^2}\n\nRunge showed that using high-degree polynomial interpolation on this function with evenly spaced points over the interval [-5, 5] leads to significant oscillations near the edges. The interpolation error grows as the degree of the polynomial increases, especially near the endpoints of the interval.\n\n\n\nThe Runge phenomenon arises because high-degree polynomials tend to oscillate more as their degree increases, especially when they are forced to pass through many points. With equally spaced points, the interpolation error is concentrated near the edges of the interval, causing large deviations from the true function in those regions.\nMathematically, for a function f(x) interpolated at n evenly spaced points using a polynomial P_n(x) of degree n-1, the interpolation error is given by:\n\n|f(x) - P_n(x)| = \\frac{|f^{(n)}(c)|}{n!} \\prod_{i=1}^{n}(x - x_i)\n\nAs the number of interpolation points n increases, the product term \\prod_{i=1}^{n}(x - x_i) becomes very large near the endpoints of the interval, causing the interpolation error to increase dramatically.\n\n\n\nThe following figure illustrates the Runge phenomenon. For the Runge function f(x) = \\frac{1}{1 + 25x^2}, the polynomial interpolation for large n shows excessive oscillations near the interval boundaries:\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\nx_values = np.linspace(-5, 5, 1000)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, runge_function(x_values), label=\"Runge Function\", color=\"blue\")\n\nplt.title(\"Runge Phenomenon\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Runge phenomenon can be mitigated by using Chebyshev nodes instead of equally spaced points. Chebyshev nodes distribute the points more densely near the edges of the interval, where the oscillations are most likely to occur. This results in more accurate interpolation with less oscillation near the boundaries.\nChebyshev nodes x_i for n interpolation points on the interval [-1, 1] are given by:\n\nx_i = \\cos\\left(\\frac{(2i - 1)\\pi}{2n}\\right)\n\nUsing Chebyshev interpolation reduces the risk of large oscillations near the endpoints, providing a more stable approximation.\n\n\n\nThe Runge phenomenon highlights the dangers of using high-degree polynomials for interpolation with evenly spaced points. To avoid this issue, it is recommended to use Chebyshev interpolation or lower-degree polynomial interpolation in smaller intervals (piecewise interpolation)."
  },
  {
    "objectID": "notes/w03/runge-phenomenon.html#introduction",
    "href": "notes/w03/runge-phenomenon.html#introduction",
    "title": "Runge Phenomenon",
    "section": "",
    "text": "The Runge Phenomenon refers to the oscillatory behavior that occurs when using high-degree polynomial interpolation on evenly spaced data points, particularly near the endpoints of an interval. The phenomenon is named after Carl Runge, who discovered this issue while studying interpolation of functions with large oscillations near the boundaries of an interval.\n\n\nWhen interpolating a smooth function using polynomials of high degree, the interpolation can become highly oscillatory near the edges of the interval, even if the function being interpolated is smooth and well-behaved. This is particularly problematic with equally spaced points, as the polynomial tries to fit too closely to the data points near the boundaries, leading to large errors.\n\n\n\nThe most famous example illustrating the Runge phenomenon is based on the Runge function:\n\nf(x) = \\frac{1}{1 + 25x^2}\n\nRunge showed that using high-degree polynomial interpolation on this function with evenly spaced points over the interval [-5, 5] leads to significant oscillations near the edges. The interpolation error grows as the degree of the polynomial increases, especially near the endpoints of the interval.\n\n\n\nThe Runge phenomenon arises because high-degree polynomials tend to oscillate more as their degree increases, especially when they are forced to pass through many points. With equally spaced points, the interpolation error is concentrated near the edges of the interval, causing large deviations from the true function in those regions.\nMathematically, for a function f(x) interpolated at n evenly spaced points using a polynomial P_n(x) of degree n-1, the interpolation error is given by:\n\n|f(x) - P_n(x)| = \\frac{|f^{(n)}(c)|}{n!} \\prod_{i=1}^{n}(x - x_i)\n\nAs the number of interpolation points n increases, the product term \\prod_{i=1}^{n}(x - x_i) becomes very large near the endpoints of the interval, causing the interpolation error to increase dramatically.\n\n\n\nThe following figure illustrates the Runge phenomenon. For the Runge function f(x) = \\frac{1}{1 + 25x^2}, the polynomial interpolation for large n shows excessive oscillations near the interval boundaries:\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\nx_values = np.linspace(-5, 5, 1000)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, runge_function(x_values), label=\"Runge Function\", color=\"blue\")\n\nplt.title(\"Runge Phenomenon\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Runge phenomenon can be mitigated by using Chebyshev nodes instead of equally spaced points. Chebyshev nodes distribute the points more densely near the edges of the interval, where the oscillations are most likely to occur. This results in more accurate interpolation with less oscillation near the boundaries.\nChebyshev nodes x_i for n interpolation points on the interval [-1, 1] are given by:\n\nx_i = \\cos\\left(\\frac{(2i - 1)\\pi}{2n}\\right)\n\nUsing Chebyshev interpolation reduces the risk of large oscillations near the endpoints, providing a more stable approximation.\n\n\n\nThe Runge phenomenon highlights the dangers of using high-degree polynomials for interpolation with evenly spaced points. To avoid this issue, it is recommended to use Chebyshev interpolation or lower-degree polynomial interpolation in smaller intervals (piecewise interpolation)."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html",
    "title": "Backward Error in Linear Systems",
    "section": "",
    "text": "The backward error is a crucial concept in numerical linear algebra, measuring the smallest perturbation in the right-hand side \\mathbf{b} that would make the approximate solution \\mathbf{x_a} satisfy the linear system A\\mathbf{x} = \\mathbf{b} exactly. In other words, it quantifies how much we need to adjust \\mathbf{b} so that \\mathbf{x_a} becomes an exact solution to a slightly modified system. This concept helps us understand the sensitivity of solutions and the reliability of numerical methods.\nThe backward error is defined as:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{r}: The residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the residual."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#overview",
    "title": "Backward Error in Linear Systems",
    "section": "",
    "text": "The backward error is a crucial concept in numerical linear algebra, measuring the smallest perturbation in the right-hand side \\mathbf{b} that would make the approximate solution \\mathbf{x_a} satisfy the linear system A\\mathbf{x} = \\mathbf{b} exactly. In other words, it quantifies how much we need to adjust \\mathbf{b} so that \\mathbf{x_a} becomes an exact solution to a slightly modified system. This concept helps us understand the sensitivity of solutions and the reliability of numerical methods.\nThe backward error is defined as:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{r}: The residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the residual."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#what-backward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#what-backward-error-represents",
    "title": "Backward Error in Linear Systems",
    "section": "What Backward Error Represents",
    "text": "What Backward Error Represents\n\nResidual Perspective:\n\nThe backward error reflects the size of the residual in the infinity norm. A smaller residual indicates that \\mathbf{x_a} nearly satisfies the system A\\mathbf{x} = \\mathbf{b}.\n\nAdjustment to \\mathbf{b}:\n\nIt provides an estimate of the minimal adjustment needed in \\mathbf{b} to make \\mathbf{x_a} an exact solution. Essentially, it tells us how much we need to perturb \\mathbf{b} so that A\\mathbf{x_a} = \\mathbf{b}' holds exactly for some \\mathbf{b}' close to \\mathbf{b}.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then the residual \\mathbf{r} = \\mathbf{0}, and hence:\n\n\\text{BE} = 0"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#why-backward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#why-backward-error-matters",
    "title": "Backward Error in Linear Systems",
    "section": "Why Backward Error Matters",
    "text": "Why Backward Error Matters\n\nAssessing Solution Quality:\n\nThe backward error helps evaluate how good the approximate solution \\mathbf{x_a} is by measuring its exactness for a nearby system.\n\nNumerical Stability:\n\nAlgorithms with small backward errors are considered numerically stable because they produce solutions that are accurate for slightly perturbed inputs.\n\nError Analysis:\n\nUnderstanding the backward error allows us to relate it to the forward error (the difference between \\mathbf{x_a} and the true solution \\mathbf{x}) and to analyze the overall accuracy of numerical methods."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#example",
    "title": "Backward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\times 1 + 1 \\times 1 \\\\ 3 \\times 1 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 - 2 \\\\ 2 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Backward Error\nThe backward error is the infinity norm of the residual:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3\n\nThis means the largest adjustment needed in \\mathbf{b} is 3, making \\mathbf{x_a} an exact solution for the perturbed system A\\mathbf{x_a} = \\mathbf{b}', where \\mathbf{b}' = A\\mathbf{x_a}.\n\n\nStep 4: Interpretation\n\nResidual Components:\n\nThe residual vector \\mathbf{r} has components:\n\n\\mathbf{r} = \\begin{bmatrix} r_1 \\\\ r_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nThe infinity norm is the maximum absolute value among these components:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|r_1|, |r_2|) = \\max(1, 3) = 3\n\nThis highlights that the backward error is dominated by the change in the y-component (r_2 = 3)."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/backward-error.html#visualization-of-backward-error",
    "href": "notes/w07/errors-analysis-linear-systems/backward-error.html#visualization-of-backward-error",
    "title": "Backward Error in Linear Systems",
    "section": "Visualization of Backward Error",
    "text": "Visualization of Backward Error\nThe graph below illustrates the backward error:\n\nBlue Vector (\\mathbf{b}): The target vector in the system.\nGreen Vector (A\\mathbf{x_a}): The vector computed by substituting the approximate solution \\mathbf{x_a}.\nRed Vector (\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}): The residual vector, representing the discrepancy.\nResidual Components: Projections of \\mathbf{r} onto the x and y axes, showing r_1 and r_2.\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nA = np.array([[1, 1], [3, -4]])\nb = np.array([3, 2])\nx_a = np.array([1, 1])\n\nAx_a = A @ x_a\nr = b - Ax_a\n\nplt.figure(figsize=(10, 8))\norigin = np.zeros(2)\n\nplt.quiver(*origin, *b, color='blue', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{b}$')\nplt.quiver(*origin, *Ax_a, color='green', angles='xy', scale_units='xy', scale=1, label=r'$A\\mathbf{x}_a$')\nplt.quiver(*Ax_a, *r, color='red', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_a$')\n\n\nplt.annotate(r'$A\\mathbf{x}_a$', (Ax_a[0], Ax_a[1]), textcoords=\"offset points\", xytext=(-60,10), ha='center', color='green', fontsize=12)\nplt.annotate(r'$\\mathbf{b}$', (b[0], b[1]), textcoords=\"offset points\", xytext=(-20,15), ha='center', color='blue', fontsize=12)\nplt.annotate(r'$\\mathbf{r}$', (Ax_a[0] + r[0]/2, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='red', fontsize=12)\n\nplt.plot([Ax_a[0], b[0]], [Ax_a[1], b[1]], 'r--', linewidth=1)\n\nplt.quiver(*Ax_a, r[0], 0, color='orange', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r_1}$')\nplt.quiver(Ax_a[0] + r[0], Ax_a[1], 0, r[1], color='purple', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r_2}$')\n\nplt.annotate(r'$\\mathbf{r_1}$', (Ax_a[0] + r[0]/2, Ax_a[1] - 0.2), textcoords=\"offset points\", xytext=(0,-10), ha='center', color='orange', fontsize=12)\nplt.annotate(r'$\\mathbf{r_2}$', (Ax_a[0] + r[0] + 0.1, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='purple', fontsize=12)\n\nplt.text(Ax_a[0] + r[0] + 0.1, Ax_a[1] + r[1] -0.3, r'$\\max(|\\mathbf{r_1}|, |\\mathbf{r_2}|) = |\\mathbf{r_2}| = 3$', color='purple', fontsize=10)\n\nplt.xlim(-1, 5)\nplt.ylim(-2, 5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Backward Error with Residual Components', fontsize=16)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html",
    "title": "Forward Error in Linear Systems",
    "section": "",
    "text": "The forward error is a critical concept in numerical linear algebra, measuring the difference between the approximate solution \\mathbf{x_a} and the true solution \\mathbf{x} of a linear system A\\mathbf{x} = \\mathbf{b}. It quantifies how far the computed solution is from the exact solution, providing insight into the accuracy of numerical methods.\nThe forward error is defined as:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the vector."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#overview",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#overview",
    "title": "Forward Error in Linear Systems",
    "section": "",
    "text": "The forward error is a critical concept in numerical linear algebra, measuring the difference between the approximate solution \\mathbf{x_a} and the true solution \\mathbf{x} of a linear system A\\mathbf{x} = \\mathbf{b}. It quantifies how far the computed solution is from the exact solution, providing insight into the accuracy of numerical methods.\nThe forward error is defined as:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the vector."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#what-forward-error-represents",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#what-forward-error-represents",
    "title": "Forward Error in Linear Systems",
    "section": "What Forward Error Represents",
    "text": "What Forward Error Represents\n\nSolution Accuracy:\n\nThe forward error reflects the maximum difference between the components of the true solution and the approximate solution.\n\nComponent-wise Deviation:\n\nIt indicates the largest deviation in any component of the solution vector.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then:\n\n\\text{FE} = 0"
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#why-forward-error-matters",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#why-forward-error-matters",
    "title": "Forward Error in Linear Systems",
    "section": "Why Forward Error Matters",
    "text": "Why Forward Error Matters\n\nAssessing Solution Quality:\n\nThe forward error directly measures the accuracy of the approximate solution, helping determine how close it is to the true solution.\n\nError Propagation:\n\nUnderstanding the forward error aids in analyzing how errors in computations propagate through the solution process.\n\nNumerical Stability:\n\nA small forward error indicates that the numerical method is producing reliable results."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#example",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#example",
    "title": "Forward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nTo enhance the visualization, we’ll adjust the approximate solution \\mathbf{x_a} so that the differences in both components are whole numbers but not equal, providing a clearer depiction of forward error in multiple dimensions.\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\nStep 1: Verify the True Solution\nCheck that \\mathbf{x} satisfies A\\mathbf{x} = \\mathbf{b}:\n\nA\\mathbf{x} = \\begin{bmatrix} 1 \\times 2 + 1 \\times 1 \\\\ 3 \\times 2 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\mathbf{b}\n\n\n\nStep 2: Compute the Forward Error\nCompute the difference between \\mathbf{x} and \\mathbf{x_a}:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 - 1 \\\\ 1 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nThe forward error is the infinity norm of \\mathbf{e}:\n\n\\text{FE} = \\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\n\n\nStep 3: Interpretation\n\nThe maximum deviation between the true and approximate solutions is 2, occurring in the second component.\nThe approximate solution \\mathbf{x_a} differs from \\mathbf{x} by 1 in the x-component and 2 in the y-component."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#visualization-of-forward-error",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#visualization-of-forward-error",
    "title": "Forward Error in Linear Systems",
    "section": "Visualization of Forward Error",
    "text": "Visualization of Forward Error\nTo illustrate the forward error, we will visualize the true solution \\mathbf{x} and the approximate solution \\mathbf{x_a}, along with the error vector \\mathbf{e} = \\mathbf{x} - \\mathbf{x_a}. By plotting these vectors, we can see how the approximate solution deviates from the true solution in both components.\n\nBlack Vector (\\mathbf{x}): The true solution vector.\nGray Vector (\\mathbf{x_a}): The approximate solution vector.\nOrange Vector (\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a}): The error vector.\nError Components: Projections of \\mathbf{e} onto the x and y axes.\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_true = np.array([2, 1])\nx_a = np.array([1, -1])\ne = x_true - x_a\n\nplt.figure(figsize=(12, 8))\norigin = np.zeros(2)\n\nplt.quiver(*origin, *x_true, color='black', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{x}$ (True Solution)', width=0.01, zorder=5)\nplt.quiver(*origin, *x_a, color='gray', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{x}_a$ (Approximate Solution)', width=0.01, zorder=5)\nplt.quiver(*x_a, *e, color='orange', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e} = \\mathbf{x} - \\mathbf{x}_a$', width=0.01, zorder=5)\nplt.quiver(*x_a, e[0], 0, color='red', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e_1}$', width=0.01, zorder=5)\nplt.quiver(x_a[0] + e[0], x_a[1], 0, e[1], color='purple', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e_2}$', width=0.01, zorder=5)\n\nplt.annotate(r'$\\mathbf{x}_a$', (x_a[0], x_a[1]), textcoords=\"offset points\", xytext=(-30, -15), ha='center', color='gray', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{x}$', (x_true[0], x_true[1]), textcoords=\"offset points\", xytext=(0, 10), ha='center', color='black', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{e}$', (x_a[0] + e[0]/2, x_a[1] + e[1]/2), textcoords=\"offset points\", xytext=(15, -15), ha='center', color='orange', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{e_1}$', (x_a[0] + e[0]/2, x_a[1] - 0.2), textcoords=\"offset points\", xytext=(0, -20), ha='center', color='red', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{e_2}$', (x_a[0] + e[0] + 0.1, x_a[1] + e[1]/2), textcoords=\"offset points\", xytext=(20, 0), ha='center', color='purple', fontsize=14, zorder=6)\n\nplt.text(x_a[0] + e[0] + 0.3, x_a[1] + e[1] + 0.3, r'$\\max(|\\mathbf{e_1}|, |\\mathbf{e_2}|) = |\\mathbf{e_2}| = 2$', color='purple', fontsize=14, zorder=6)\n\nplt.plot([x_a[0], x_true[0]], [x_a[1], x_true[1]], color='orange', linewidth=1.5, zorder=4)  # Keep the connecting line below vectors\nplt.xlim(-1, 5)\nplt.ylim(-2, 3)\nplt.axhline(0, color='black', linewidth=0.8, zorder=1)\nplt.axvline(0, color='black', linewidth=0.8, zorder=1)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.7, zorder=0)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Forward Error with Error Components', fontsize=18)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nVector Addition:\n\nThe error vector \\mathbf{e} is drawn starting from \\mathbf{x_a} and pointing towards \\mathbf{x}, demonstrating that:\n\n\\mathbf{x_a} + \\mathbf{e} = \\mathbf{x}\n\n\nError Components:\n\nThe error vector \\mathbf{e} is decomposed into its x-component e_1 (red dashed arrow) and y-component e_2 (purple dashed arrow).\nThe components are:\n\ne_1 = 1, \\quad e_2 = 2\n\n\nInfinity Norm Highlighted:\n\nThe maximum absolute component of the error is |e_2| = 2, which is the forward error \\text{FE} = \\|\\mathbf{e}\\|_\\infty.\nThis is highlighted in the graph with a text annotation.\n\nUnderstanding the Forward Error:\n\nBy visualizing the error components, we see that the deviation occurs in both the x and y components.\nThe largest error is in the y-component, which determines the forward error."
  },
  {
    "objectID": "notes/w07/errors-analysis-linear-systems/forward-error.html#conclusion",
    "href": "notes/w07/errors-analysis-linear-systems/forward-error.html#conclusion",
    "title": "Forward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nForward Error Significance:\n\nThe forward error provides a direct measure of the accuracy of the approximate solution \\mathbf{x_a} in relation to the true solution \\mathbf{x}.\n\nVisualization Enhancements:\n\nBy adjusting \\mathbf{x_a} to differ by whole numbers in both components, the visualization effectively demonstrates how errors in multiple dimensions contribute to the overall forward error.\nDecomposing the error vector into its components and highlighting the infinity norm offers a clearer understanding of how the forward error is calculated.\n\nPractical Implications:\n\nIn numerical computations, minimizing the forward error is crucial for obtaining accurate solutions."
  },
  {
    "objectID": "homework/w02/exercise3-1-1c.html",
    "href": "homework/w02/exercise3-1-1c.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, -2), (2, 1), (4, 4).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, -2)):\n\n\n-2 \\cdot \\frac{(x - 2)(x - 4)}{(0 - 2)(0 - 4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{(-2)(-4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{8} = \\frac{-1}{4}(x - 2)(x - 4)\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 1))\n\n\n1 \\cdot \\frac{(x - 0)(x - 4)}{(2 - 0)(2 - 4)} = 1 \\cdot \\frac{x(x - 4)}{(2)(-2)} = \\frac{x(x - 4)}{-4}\n\n\nThird term (corresponding to (x_3, y_3) = (4, 4)):\n\n\n4 \\cdot \\frac{(x - 0)(x - 2)}{(4 - 0)(4 - 2)} = 4 \\cdot \\frac{x(x - 2)}{(4)(2)} = 4 \\cdot \\frac{x(x - 2)}{8} = \\frac{x(x - 2)}{2}\n\n\n\n\n\nP(x) = \\frac{-1}{4}(x - 2)(x - 4) + \\frac{x(x - 4)}{-4} + \\frac{x(x - 2)}{2}\n\n\n\n\nFirst term:\n\n\\frac{-1}{4}(x - 2)(x - 4) = \\frac{-x^2 + 6x - 8}{4}\n\nSecond term:\n\n\\frac{-x(x - 4)}{4} = \\frac{-x^2 + 4x}{4}\n\nThird term:\n\n\\frac{x(x - 2)}{2} = \\frac{x^2 - 2x}{2}\n\nCombine the terms:\n\nP(x) = \\frac{-x^2 + 6x - 8}{4} + \\frac{-x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nTo combine, first rewrite everything with a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nConvert the second term to have a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{2x^2 - 4x}{4}\n\nNow simplify:\n\nP(x) = \\frac{-2x^2 + 10x - 8}{4} + \\frac{2x^2 - 4x}{4}\n\n\nP(x) = \\frac{6x - 8}{4} = \\frac{3x - 4}{2}\n\nFinal polynomial:\n\nP(x) = \\frac{3x - 4}{2}\n\nThis is the interpolating polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (3 * x - 4) / 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (3x - 4) / 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-1c.html#question",
    "href": "homework/w02/exercise3-1-1c.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, -2), (2, 1), (4, 4).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, -2)):\n\n\n-2 \\cdot \\frac{(x - 2)(x - 4)}{(0 - 2)(0 - 4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{(-2)(-4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{8} = \\frac{-1}{4}(x - 2)(x - 4)\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 1))\n\n\n1 \\cdot \\frac{(x - 0)(x - 4)}{(2 - 0)(2 - 4)} = 1 \\cdot \\frac{x(x - 4)}{(2)(-2)} = \\frac{x(x - 4)}{-4}\n\n\nThird term (corresponding to (x_3, y_3) = (4, 4)):\n\n\n4 \\cdot \\frac{(x - 0)(x - 2)}{(4 - 0)(4 - 2)} = 4 \\cdot \\frac{x(x - 2)}{(4)(2)} = 4 \\cdot \\frac{x(x - 2)}{8} = \\frac{x(x - 2)}{2}\n\n\n\n\n\nP(x) = \\frac{-1}{4}(x - 2)(x - 4) + \\frac{x(x - 4)}{-4} + \\frac{x(x - 2)}{2}\n\n\n\n\nFirst term:\n\n\\frac{-1}{4}(x - 2)(x - 4) = \\frac{-x^2 + 6x - 8}{4}\n\nSecond term:\n\n\\frac{-x(x - 4)}{4} = \\frac{-x^2 + 4x}{4}\n\nThird term:\n\n\\frac{x(x - 2)}{2} = \\frac{x^2 - 2x}{2}\n\nCombine the terms:\n\nP(x) = \\frac{-x^2 + 6x - 8}{4} + \\frac{-x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nTo combine, first rewrite everything with a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nConvert the second term to have a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{2x^2 - 4x}{4}\n\nNow simplify:\n\nP(x) = \\frac{-2x^2 + 10x - 8}{4} + \\frac{2x^2 - 4x}{4}\n\n\nP(x) = \\frac{6x - 8}{4} = \\frac{3x - 4}{2}\n\nFinal polynomial:\n\nP(x) = \\frac{3x - 4}{2}\n\nThis is the interpolating polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (3 * x - 4) / 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (3x - 4) / 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-2c.html",
    "href": "homework/w02/exercise3-1-2c.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n-2\n\n\n\n\n2\n1\n1.5\n\n\n\n4\n4\n1.5\n0\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = -2, \\quad f[x_2] = y_2 = 1, \\quad f[x_3] = y_3 = 4 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-2)}{2 - 0} = \\frac{3}{2} = 1.5 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{4 - 1}{4 - 2} = \\frac{3}{2} = 1.5 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{1.5 - 1.5}{4 - 0} = 0 \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = -2 + 1.5(x - 0) + 0(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = -2 + 1.5x\n\nSo the final polynomial is:\n\nP(x) = 1.5x - 2\n\nThis is the Newton interpolating polynomial for the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return 1.5 * x - 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = 1.5x - 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "homework/w02/exercise3-1-2c.html#question",
    "href": "homework/w02/exercise3-1-2c.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n-2\n\n\n\n\n2\n1\n1.5\n\n\n\n4\n4\n1.5\n0\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = -2, \\quad f[x_2] = y_2 = 1, \\quad f[x_3] = y_3 = 4 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-2)}{2 - 0} = \\frac{3}{2} = 1.5 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{4 - 1}{4 - 2} = \\frac{3}{2} = 1.5 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{1.5 - 1.5}{4 - 0} = 0 \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = -2 + 1.5(x - 0) + 0(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = -2 + 1.5x\n\nSo the final polynomial is:\n\nP(x) = 1.5x - 2\n\nThis is the Newton interpolating polynomial for the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return 1.5 * x - 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = 1.5x - 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "reality-checks/rc05/rc05.html",
    "href": "reality-checks/rc05/rc05.html",
    "title": "REALITY CHECK 05",
    "section": "",
    "text": "The use of Adaptive Quadrature is essential for maintaining constant speed along a specific path. This is a requirement in fields like computer-aided manufacturing, robotics and animation. Smooth and controlled movement is crucial for accuracy, but achieving a constant speed along a curved or complex path is challenging. Dividing a path into equal time intervals does not ensure equal-distance segments because the path’s shape influences the distance covered.\nTo address this, numerical methods are employed to divide the path into equal arc-length segments, ensuring consistent movement. The process involves several key steps:\n\nArc Length Measurement: The total length of the path is calculated using parametric equations, accounting for all curves and directional changes. This measurement provides the foundation for precise segmentation.\nMapping Path Position: To locate a point at a given distance s along the path, the corresponding parameter t is determined using numerical methods like Bisection or Newton’s Method. This ensures precise mapping of arc-length positions to their parametric coordinates.\nSegmenting the Path: The path is divided into segments of equal arc length, a process called equipartitioning. This segmentation ensures uniformity in the spacing of points along the path, regardless of its complexity or curvature.\nSmooth Traversal: Animations or simulations often demonstrate the practical effects of this approach. By comparing movement at constant parameter speed with movement along equal arc-length segments, the benefits of consistent, controlled traversal become clear, showcasing smoother and more predictable motion."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#path-equipartitioning-by-arc-length",
    "href": "reality-checks/rc05/rc05.html#path-equipartitioning-by-arc-length",
    "title": "REALITY CHECK 05",
    "section": "Path Equipartitioning by Arc Length",
    "text": "Path Equipartitioning by Arc Length\n\nProblem Statement\nEquipartition the path of Figure 5.6 into n subpaths of equal length, for n = 4 and n = 20. Plot analogues of Figure 5.6, showing the equipartitions.\n\n\n\n\n\n\n\n\n\n\n\nObjective and Approach\nThe objective is to partition the path, defined by parametric equations x(t) and y(t), into segments of equal arc length for a specified n. This method is valuable in fields requiring consistent movement along a path, such as animation or robotics.\nThe approach includes the following steps:\n\nCalculate Total Arc Length: Compute the total arc length from t = 0 to t = 1 using numerical integration, enabling calculation of the length of each segment.\n\n\\text{Segment length} = \\frac{\\text{Total arc length}}{n}\n\nLocate Partition Points Using the Bisection Method: For each segment i, use the Bisection Method to locate the parameter t_i so that the arc length from t = 0 to t = t_i equals i \\times \\text{Segment length}. This ensures equal arc lengths for each segment.\nPlot the Equipartitioned Path: Calculate x(t_i) and y(t_i) at each partition point and plot for both n = 4 and n = 20, visualizing uniform segmentation.\n\n\n\nSolution Code\n\n# Bisection method to find t for a target arc length fraction\ndef bisection_find_t(target_length, tol=1e-8):\n    a, b = 0, 1\n    while (b - a) / 2 &gt; tol:\n        midpoint = (a + b) / 2\n        if compute_arc_length(midpoint) == target_length:\n            return midpoint\n        elif compute_arc_length(midpoint) &lt; target_length:\n            a = midpoint\n        else:\n            b = midpoint\n    return (a + b) / 2\n\n# Equipartition function\ndef equipartition(n):\n    partition_points = [0]\n    total_length = compute_arc_length(1)\n    segment_length = total_length / n\n    for i in range(1, n):\n        target_length = i * segment_length\n        t_i = bisection_find_t(target_length)\n        partition_points.append(t_i)\n    partition_points.append(1)\n    return partition_points\n\n# Plot function for equipartitioned curve\ndef plot_styled_curve(n):\n    plt.figure(figsize=(8, 8), facecolor='white')\n\n    t_vals = np.linspace(0, 1, 500)\n    x_vals = x(t_vals)\n    y_vals = y(t_vals)\n\n    key_points_t = equipartition(n)\n    key_points_x = [x(t) for t in key_points_t]\n    key_points_y = [y(t) for t in key_points_t]\n\n    # Plot the curve with enhanced styling\n    plt.plot(x_vals, y_vals, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    plt.scatter(key_points_x, key_points_y, color=\"#1565C0\", s=40, zorder=4)\n\n    # Add grid with softer appearance\n    plt.grid(True, linestyle='-', alpha=0.2, color='gray')\n    plt.xticks(np.arange(-1, 1.5, 0.5))\n    plt.yticks(np.arange(0, 2.5, 0.5))\n\n\n    # Enhanced axis lines\n    ax = plt.gca()\n    ax.set_xticklabels(['' if x == 0 else str(x) for x in ax.get_xticks()])\n    ax.set_yticklabels(['' if y == 0 else str(y) for y in ax.get_yticks()])\n\n    ax.spines['left'].set_position('zero')\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['left'].set_linewidth(1.5)\n    ax.spines['bottom'].set_linewidth(1.5)\n\n    # Enhance tick appearance\n    plt.tick_params(axis='both', which='major', length=6, width=1, colors='black', direction='out')\n    plt.tick_params(axis='both', which='minor', length=3, width=1, colors='black', direction='out')\n\n    # Label positioning and styling\n    ax.set_ylabel('y', rotation=0, labelpad=15, y=1.02, fontsize=12)\n    ax.set_xlabel('x', x=1.02, fontsize=12)\n\n    plt.xlim(-1.5, 1.5)\n    plt.ylim(-0.5, 2)\n    plt.gca().set_aspect('equal')\n\n    plt.show()\n\n\nEquipartitioned Curve with n = 4\n\n\n\n\n\n\n\n\n\n\n\nEquipartitioned Curve with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of Solution Components\n\nArc Length Calculation: The function compute_arc_length(s) uses numerical integration to compute the arc length from t = 0 to a given t = s.\nBisection Method for Partitioning: bisection_find_t(target_length) finds the parameter t corresponding to a specific arc length, ensuring accurate partition points.\nEquipartition Function: equipartition(n) calculates t-values for partitioning the path into n equal arc-length segments.\nVisualization: plot_styled_curve(n) generates a plot showing the path with points marking each partition.\n\n\n\nResults and Observations\nThe plots for n = 4 and n = 20 illustrate the uniform segmentation of the path into equal-length segments, confirming the effectiveness of the equipartitioning process. This approach achieves constant distances along the path, despite non-uniform parameter spacing.\n\n\nConclusion\nThe solution effectively partitions the path into equal-length segments using numerical integration and the Bisection Method. This method can be further enhanced by employing Newton’s Method for faster convergence or adapting it to three-dimensional paths."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#path-equipartitioning-using-newtons-method",
    "href": "reality-checks/rc05/rc05.html#path-equipartitioning-using-newtons-method",
    "title": "REALITY CHECK 05",
    "section": "Path Equipartitioning Using Newton’s Method",
    "text": "Path Equipartitioning Using Newton’s Method\n\nProblem Statement\nReplace the Bisection Method in Step 2 with Newton’s Method, and repeat Steps 2 and 3. What is the derivative needed? What is a good choice for the initial guess? Is computation time decreased by this replacement?\n\n\nObjective and Approach\n\nObjective: Use Newton’s Method to locate each partition point t_i along the path, ensuring equal arc-length segments for a specified number of partitions n. Newton’s Method is expected to offer faster convergence than the Bisection Method, especially when starting with a good initial guess.\nRequired Derivative: Newton’s Method requires the derivative of the arc length function with respect to t, which is simply the arc length integrand evaluated at t:\n\nf'(t) = \\sqrt{\\left( \\frac{dx}{dt} \\right)^2 + \\left( \\frac{dy}{dt} \\right)^2}\n\nInitial Guess: A reasonable initial guess for each t_i is t_i = \\frac{i}{n}, which provides a uniformly spaced initial estimate along t, aiding the convergence of Newton’s Method.\nPerformance Comparison: To evaluate if Newton’s Method reduces computation time, we will measure the time taken by both the Bisection and Newton’s methods to achieve the same accuracy.\n\n\nWhy t_i = \\frac{i}{n} is a Good Initial Guess\n\nUniform Parameter Distribution: The parameter t varies between 0 and 1 (or the specified range of t), and \\frac{i}{n} provides evenly spaced points within this interval. This ensures that the initial guess is distributed consistently across the parameter space.\nProximity to the True Solution: For smooth and “well-behaved” curves, the true t_i values for equal arc-length segments are often near \\frac{i}{n}. This proximity ensures that Newton’s Method starts “in the ballpark” of the correct value.\nSimplicity and Efficiency: Computing \\frac{i}{n} is computationally trivial and requires no extra effort. This simplicity makes it a practical choice compared to complex initialization schemes.\nImproved Convergence: Starting close to the actual solution allows Newton’s Method to converge quadratically, reducing the number of iterations needed to achieve the desired accuracy.\n\n\n\n\nSolution Code\nThe following Python code implements Newton’s Method to find partition points and compares its performance with the Bisection Method.\n\nimport time\n\n# Newton's Method to find t for a target arc length\ndef newton_find_t(target_length, initial_guess, tol=1e-8, max_iter=100):\n    t = initial_guess\n    for _ in range(max_iter):\n        f_t = compute_arc_length(t) - target_length\n        f_prime_t = integrand(t)\n        if abs(f_t) &lt; tol:\n            return t\n        t -= f_t / f_prime_t  # Update t\n    return t\n\n# Compare performance of Bisection and Newton's methods\ndef compare_performance(target_length):\n    start_time_bisection = time.time()\n    bisection_result = bisection_find_t(target_length)\n    bisection_time = time.time() - start_time_bisection\n\n    start_time_newton = time.time()\n    newton_result = newton_find_t(target_length, initial_guess=0.5)\n    newton_time = time.time() - start_time_newton\n\n    print(f\"Bisection Method Result: {bisection_result:.9f} Time: {bisection_time:.9f} seconds\")\n    print(f\"Newton's Method Result: {newton_result:.9f} Time: {newton_time:.9f} seconds\")\n\n# Example target length (e.g., half the arc length)\ntotal_length = compute_arc_length(1)\ncompare_performance(total_length / 2)\n\nBisection Method Result: 0.800593771 Time: 0.007006168 seconds\nNewton's Method Result: 0.800593767 Time: 0.001001596 seconds\n\n\n\n\nExplanation of Solution Components\n\nNewton’s Method Implementation: The newton_find_t function applies Newton’s Method to locate the parameter t for a given arc length. It iteratively refines t by calculating f(t) and f'(t), adjusting t based on the result.\nPerformance Comparison: The compare_performance function compares the time taken by Bisection and Newton’s methods to find the target t-value. This illustrates the efficiency difference between the two methods.\n\n\n\nResults and Observations\n\nPerformance Gain: Newton’s Method generally converges faster than the Bisection Method due to its quadratic convergence rate.\nAccuracy: With an appropriately chosen initial guess, Newton’s Method efficiently reaches an accurate solution within fewer iterations.\n\n\n\nConclusion\nNewton’s Method provides a more efficient approach for finding the partition points, particularly when an initial guess is available. This reduction in computation time makes it suitable for tasks requiring high precision and quick convergence, such as real-time applications in path traversal and equipartitioning. Future explorations could involve further optimizations by dynamically refining initial guesses based on prior calculations."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#path-animation-at-original-and-constant-speed",
    "href": "reality-checks/rc05/rc05.html#path-animation-at-original-and-constant-speed",
    "title": "REALITY CHECK 05",
    "section": "Path Animation at Original and Constant Speed",
    "text": "Path Animation at Original and Constant Speed\n\nProblem Statement\nUse Python animation commands to demonstrate traveling along the path in two ways:\n\nAt the original speed, based on parameter t for 0 \\leq t \\leq 1, which results in non-uniform speed along the path.\nAt a constant speed using t^*(s) for 0 \\leq s \\leq 1, where the path is re-parameterized to maintain equal arc-length segments.\n\n\n\nObjective and Approach\n\nObjective: To visualize the difference between non-uniform and constant-speed traversal along a path.\n\nOriginal Speed: Animate movement along the path based on evenly spaced t-values, resulting in variable speed.\nConstant Speed: Animate movement along the path with equal arc-length segments by using equipartition points t^*(s).\n\nApproach:\n\nOriginal Speed Animation: Use uniformly spaced t-values from t = 0 to t = 1 to display the natural parameter-based speed.\nConstant Speed Animation: Use the previously calculated equipartition points t^*(s) to animate movement along equal arc-length segments, ensuring a uniform speed.\n\n\n\n\nSolution Code\nThe following Python code generates both animations, showing the path traversal at original and constant speeds.\nimport matplotlib.animation as animation\n\ndef animate_path():\n    fig = plt.figure(figsize=(16, 8), facecolor='white')\n\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n\n    # Generate data for original and constant speed\n    t_values_original_speed = np.linspace(0, 1, 25)\n    x_vals_original_speed = x(t_values_original_speed)\n    y_vals_original_speed = y(t_values_original_speed)\n\n    t_values_constant_speed = equipartition(25)\n    x_vals_constant_speed = [x(t) for t in t_values_constant_speed]\n    y_vals_constant_speed = [y(t) for t in t_values_constant_speed]\n\n    # Enhanced styling function for subplots\n    def style_subplot(ax, title):\n        ax.grid(True, linestyle='-', alpha=0.2, color='gray')\n        ax.set_xlim(-1.5, 1.5)\n        ax.set_ylim(-0.5, 2)\n        ax.set_xticks(np.arange(-1, 1.5, 0.5))\n        ax.set_yticks(np.arange(0, 2.5, 0.5))\n        ax.set_aspect('equal')\n        ax.set_xticklabels(['' if x == 0 else str(x) for x in ax.get_xticks()])\n        ax.set_yticklabels(['' if y == 0 else str(y) for y in ax.get_yticks()])\n\n\n        # Enhanced axis lines\n        ax.spines['left'].set_position('zero')\n        ax.spines['bottom'].set_position('zero')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.spines['left'].set_linewidth(1.5)\n        ax.spines['bottom'].set_linewidth(1.5)\n\n        # Enhanced ticks\n        ax.tick_params(axis='both', which='major', length=6, width=1, colors='black', direction='out')\n        ax.tick_params(axis='both', which='minor', length=3, width=1, colors='black', direction='out')\n\n        ax.set_title(title, pad=20, fontsize=12, fontweight='bold')\n\n    # Configure first subplot\n    ax1.plot(x_vals_original_speed, y_vals_original_speed, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    original_point, = ax1.plot([], [], 'o', color=\"#1565C0\", markersize=8, zorder=4)\n    style_subplot(ax1, \"Original Speed\")\n\n    # Configure second subplot\n    ax2.plot(x_vals_constant_speed, y_vals_constant_speed, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    constant_point, = ax2.plot([], [], 'go',  markersize=8, zorder=4)\n    style_subplot(ax2, \"Constant Speed\")\n\n    plt.tight_layout()\n\n    def update_original(fnum):\n        original_point.set_data(x_vals_original_speed[:fnum], y_vals_original_speed[:fnum])\n        return original_point,\n\n    def update_constant(fnum):\n        constant_point.set_data(x_vals_constant_speed[:fnum], y_vals_constant_speed[:fnum])\n        return constant_point,\n\n    num_frames = len(x_vals_original_speed)\n    ani = animation.FuncAnimation(fig, lambda fnum: update_original(fnum) + update_constant(fnum),\n                                frames=num_frames, interval=200, blit=True)\n    ani.save('combined_animation.mp4', writer='ffmpeg')\n\n  Your browser does not support the video tag. \n\n\n\nExplanation of Solution Components\n\nAnimation Setup:\n\nOriginal Speed: Uses evenly spaced t-values from 0 to 1, resulting in non-uniform movement along the path.\nConstant Speed: Uses equipartition points t^*(s), calculated to ensure each segment has the same arc length, resulting in uniform movement.\n\nAnimation Update Functions: Each animation frame updates the moving point on the respective path for both original and constant speeds.\n\n\n\nResults and Observations\nThe two animations effectively demonstrate the difference between moving at a variable speed (based on t) and moving at a constant speed along equal arc-length segments. By using equipartition points, the constant-speed animation shows smooth, uniform movement, which can be advantageous for applications requiring consistent traversal rates."
  },
  {
    "objectID": "reality-checks/rc05/rc05.html#experimenting-with-equipartitioning-on-a-custom-path",
    "href": "reality-checks/rc05/rc05.html#experimenting-with-equipartitioning-on-a-custom-path",
    "title": "REALITY CHECK 05",
    "section": "Experimenting with Equipartitioning on a Custom Path",
    "text": "Experimenting with Equipartitioning on a Custom Path\n\nProblem Statement\nExperiment with equipartitioning a path of your choice. Choose a path defined by parametric equations, partition it into equal arc-length segments, and animate the traversal as demonstrated in Problem 5.\n\nChosen Equation:\n\nx(t) = 0.4 \\sin(3t + \\frac{\\pi}{2}) + 0.5\n\n\ny(t) = 0.3 \\sin(4t) + 0.5\n\n\n\n\nObjective and Approach\n\nObjective: To apply equipartitioning to the specified path, dividing it into segments of equal arc length and visualizing the traversal at constant speed.\nApproach:\n\nPath Definition: Define x(t) and y(t) based on the given equations.\nEquipartitioning: Use numerical integration and Newton’s Method to divide the path into equal-length segments.\nAnimation: Animate the traversal of the path at a constant speed along the equal arc-length segments and compare it with traversal at the original, parameter-based speed.\n\n\n\n\nSolution Code\nThe Python code below calculates the equipartitioned segments and animates traversal along the path:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom scipy.integrate import quad\n\n# Parameters for the curve\nA = 0.4\na = 3\nf = np.pi / 2\nc = 0.5\nB = 0.3\nb = 4\nD = 0.5\n\n# Maximum value of t for one full loop\nt_max = 2 * np.pi\n\n# Define the functions for x(t) and y(t)\ndef x(t):\n    return A * np.sin(a * t + f) + c\n\ndef y(t):\n    return B * np.sin(b * t) + D\n\n# Derivatives of x(t) and y(t) for arc length calculation\ndef dx_dt(t):\n    return A * a * np.cos(a * t + f)\n\ndef dy_dt(t):\n    return B * b * np.cos(b * t)\n\n# Integrand for arc length calculation\ndef integrand(t):\n    return np.sqrt(dx_dt(t)**2 + dy_dt(t)**2)\n\n# Compute arc length using numerical integration\ndef compute_arc_length(s):\n    arc_length, _ = quad(integrand, 0, s)\n    return arc_length\n\n# Equipartition function to divide path into equal arc-length segments\ndef equipartition(n):\n    total_length = compute_arc_length(2 * np.pi)\n    segment_length = total_length / n\n    partition_points = [0]\n    for i in range(1, n):\n        target_length = i * segment_length\n        partition_points.append(find_t_for_length(target_length, partition_points[-1]))\n    partition_points.append(2 * np.pi)\n    return partition_points\n\n# Find parameter t for a given arc length using Newton's Method\ndef find_t_for_length(target_length, initial_guess=0, tol=1e-8, max_iter=100):\n    t = initial_guess\n    for _ in range(max_iter):\n        f_t = compute_arc_length(t) - target_length\n        f_prime_t = integrand(t)\n        if abs(f_t) &lt; tol:\n            return t\n        t -= f_t / f_prime_t\n        t = max(0, min(2 * np.pi, t))\n    return t\n\n\n# Data for animations\nn_points = 200\nt_values_original = np.linspace(0, 2 * np.pi, n_points)\nx_original = x(t_values_original)\ny_original = y(t_values_original)\n\nt_values_constant = equipartition(n_points)\nx_constant = [x(t) for t in t_values_constant]\ny_constant = [y(t) for t in t_values_constant]\n\n# Set up the figure for side-by-side animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Original speed plot\nax1.plot(x_original, y_original, color=\"#2196F3\", linewidth=2)\npoint1, = ax1.plot([], [], 'o', color=\"#1565C0\")\nax1.set_title(\"Original Speed\")\nax1.set_xlim(0, 1)\nax1.set_ylim(0, 1)\nax1.set_aspect('equal')\nax1.grid(True, linestyle='-', alpha=0.2, color='gray')\n\n# Constant speed plot\nax2.plot(x_constant, y_constant, color=\"#2196F3\", linewidth=2)\npoint2, = ax2.plot([], [], 'go')\nax2.set_title(\"Constant Speed\")\nax2.set_xlim(0, 1)\nax2.set_ylim(0, 1)\nax2.set_aspect('equal')\nax2.grid(True, linestyle='-', alpha=0.2, color='gray')\n\n# Update functions for each animation\ndef update_original(frame):\n    point1.set_data(x_original[:frame], y_original[:frame])\n    return point1,\n\ndef update_constant(frame):\n    point2.set_data(x_constant[:frame], y_constant[:frame])\n    return point2,\n\n# Combine animations into one\nnum_frames = len(x_original)\nani = animation.FuncAnimation(\n    fig,\n    lambda frame: update_original(frame) + update_constant(frame),\n    frames=num_frames,\n    interval=100,\n    blit=True\n)\n\n# Save animation as MP4\nani.save(\"custom_path_animation.mp4\", writer=\"ffmpeg\")\n\n  Your browser does not support the video tag. \n\n\n\nExplanation of Solution Components\n\nPath Definition: The parametric equations for x(t) = 0.4 \\sin(3t + \\frac{\\pi}{2}) + 0.5 and y(t) = 0.3 \\sin(4t) + 0.5 define a periodic curve with sinusoidal behavior, creating a visually interesting pattern with symmetric, tight curves.\nArc Length Calculation: The function compute_arc_length integrates the instantaneous speed along the curve (using derivatives dx/dt and dy/dt) over the interval [0, s] to determine the total distance traveled up to a given s.\nEquipartitioning: The equipartition function divides the path into n segments of equal arc length by calculating the target length of each segment and using Newton’s Method to determine t values corresponding to each target segment length. This ensures the segments are evenly spaced along the curve.\nAnimation: The animate_path function generates side-by-side animations of path traversal at original speed (based on parameter t) and constant speed (based on equal arc-length segments).\n\n\n\nResults and Observations\nIn the animation:\n\nOriginal Speed: The left animation shows traversal based on equally spaced t values, resulting in variable speed along the curve. The point moves faster along straighter sections and slows down significantly in tighter curves.\nConstant Speed: The right animation demonstrates traversal at a constant speed along equal arc-length segments. This movement is smoother and consistent, highlighting how equipartitioning ensures a steady traversal rate even along complex paths.\n\n\n\nConclusion\nThis exercise illustrates the benefits of equipartitioning a path into equal arc-length segments for applications that require consistent speed. By reparameterizing the curve to maintain constant speed, we can avoid the variable movement speed that results from a simple, evenly spaced parameter t. This method has potential applications in animation, robotics, and automated manufacturing, where uniform movement along a path with varying curvature is essential."
  }
]